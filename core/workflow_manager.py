# core/workflow_manager.py
import json
import time
import os
import re
import uuid
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional, Union

from core.workflow_io import load_workflow, save_workflow
from core.changes import apply_global_style, replace_entity_reference
from core.runner import run_pipeline, run_stylize, run_video_generate
from core.utils import get_ffmpeg_path

# Film IR é›†æˆ
from core.film_ir_schema import create_empty_film_ir
from core.film_ir_io import save_film_ir, load_film_ir, film_ir_exists
from core.film_ir_manager import FilmIRManager

# å¼•å…¥æ‹†è§£æ‰€éœ€çš„åº“å’Œé€»è¾‘
from google import genai
from analyze_video import DIRECTOR_METAPROMPT, wait_until_file_active, extract_json_array
from extract_frames import to_seconds

class WorkflowManager:
    def __init__(self, job_id: Optional[str] = None, project_root: Optional[Path] = None):
        self.project_dir = project_root or Path(__file__).parent.parent
        self.job_id = job_id
        self.workflow: Dict[str, Any] = {}
        
        if job_id:
            self.job_dir = self.project_dir / "jobs" / job_id
            if (self.job_dir / "workflow.json").exists():
                self.load()

    def initialize_from_file(self, temp_video_path: Path) -> str:
        """å…¨è‡ªåŠ¨åˆå§‹åŒ–ç®¡çº¿ï¼šå®Œæˆæ‹†è§£ä¸åŸå§‹ç´ ææå–"""
        new_id = f"job_{uuid.uuid4().hex[:8]}"
        self.job_id = new_id
        self.job_dir = self.project_dir / "jobs" / new_id
        
        self.job_dir.mkdir(parents=True, exist_ok=True)
        (self.job_dir / "frames").mkdir(exist_ok=True)
        (self.job_dir / "videos").mkdir(exist_ok=True)
        (self.job_dir / "source_segments").mkdir(exist_ok=True)
        (self.job_dir / "stylized_frames").mkdir(exist_ok=True)
        
        final_video_path = self.job_dir / "input.mp4"
        shutil.move(str(temp_video_path), str(final_video_path))
        
        print(f"ğŸš€ [Phase 1] æ­£åœ¨é€šè¿‡ Gemini æ‹†è§£è§†é¢‘: {new_id}...")
        storyboard = self._run_gemini_analysis(final_video_path)
        
        print(f"ğŸš€ [Phase 2] æ­£åœ¨æå–å…³é”®å¸§ä¸åŸå§‹åˆ†é•œçŸ­ç‰‡...")
        self._run_ffmpeg_extraction(final_video_path, storyboard)
        
        shots = []
        for s in storyboard:
            shot_num = int(s.get("shot_number", 1))
            sid = f"shot_{shot_num:02d}"

            # ğŸ“‹ Semantic Split: Narrative Layer (plot) + Technical Layer (metadata tags)
            # Narrative Layer - Pure visual/plot description (no camera technical terms)
            narrative_desc = s.get("frame_description") or s.get("content_analysis") or ""

            # ğŸ¬ Cinematography Fidelity Parameters - Hard-coded constraints from source analysis
            shot_scale = s.get("shot_scale", "")
            subject_frame_position = s.get("subject_frame_position", "")
            subject_orientation = s.get("subject_orientation", "")
            gaze_direction = s.get("gaze_direction", "")
            motion_vector = s.get("motion_vector", "")
            camera_type = s.get("camera_type") or s.get("camera_movement", "")

            # Build structured description with HARD-CODED cinematography constraints
            desc_lines = [narrative_desc]

            # ğŸ¯ CRITICAL: These are non-negotiable constraints that MUST be preserved
            if shot_scale:
                desc_lines.append(f"[SCALE: {shot_scale}]")
            if subject_frame_position:
                desc_lines.append(f"[POSITION: {subject_frame_position}]")
            if subject_orientation:
                desc_lines.append(f"[ORIENTATION: {subject_orientation}]")
            if gaze_direction:
                desc_lines.append(f"[GAZE: {gaze_direction}]")
            if motion_vector:
                desc_lines.append(f"[MOTION: {motion_vector}]")
            if camera_type:
                desc_lines.append(f"[CAMERA: {camera_type}]")

            # Join with newlines for clear separation
            full_description = "\n".join(desc_lines)

            # ğŸ’¾ Store raw cinematography data for downstream enforcement
            cinematography_data = {
                "shot_scale": shot_scale,
                "subject_frame_position": subject_frame_position,
                "subject_orientation": subject_orientation,
                "gaze_direction": gaze_direction,
                "motion_vector": motion_vector,
                "camera_type": camera_type
            }

            # ğŸ¬ SocialSaver æ‰©å±•å­—æ®µï¼šè§†å¬å±‚ä¿¡æ¯
            lighting = s.get("lighting", "")
            music_mood = s.get("music_mood", "")
            dialogue_voiceover = s.get("dialogue_voiceover", "")
            content_analysis = s.get("content_analysis", narrative_desc)

            shots.append({
                "shot_id": sid,
                "start_time": s.get("start_time"),
                "end_time": s.get("end_time"),
                "description": full_description,
                "content_analysis": content_analysis,  # ğŸ¬ åœºæ™¯å†…å®¹åˆ†æ
                "lighting": lighting,  # ğŸ¬ SocialSaver: å…‰çº¿æè¿°
                "music_mood": music_mood,  # ğŸ¬ SocialSaver: éŸ³ä¹æ°›å›´
                "dialogue_voiceover": dialogue_voiceover,  # ğŸ¬ SocialSaver: å¯¹ç™½/æ—ç™½
                "cinematography": cinematography_data,  # ğŸ¬ Hard-coded source cinematography for fidelity enforcement
                "entities": [],
                "assets": {
                    "first_frame": f"frames/{sid}.png",
                    "source_video_segment": f"source_segments/{sid}.mp4",
                    "stylized_frame": None, # ğŸ’¡ PMé€»è¾‘ï¼šåˆå§‹åŒ–ä¸ºç©ºï¼Œå¼ºåˆ¶è§¦å‘ AI ç”Ÿå›¾æµç¨‹
                    "video": None
                },
                "status": {
                    "stylize": "NOT_STARTED",
                    "video_generate": "NOT_STARTED"
                }
            })
            
        self.workflow = {
            "job_id": new_id,
            "source_video": "input.mp4",
            "film_ir_path": "film_ir.json",  # ğŸ¬ Film IR å…³è”
            "global": {"style_prompt": "Cinematic Realistic", "video_model": "veo"},
            "global_stages": {
                "analyze": "SUCCESS", "extract": "SUCCESS",
                "stylize": "NOT_STARTED", "video_gen": "NOT_STARTED", "merge": "NOT_STARTED"
            },
            "shots": shots,
            "meta": {"attempts": 0, "updated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
        }

        self.save()

        # ğŸ¬ åˆå§‹åŒ– Film IR (ç”µå½±é€»è¾‘ä¸­é—´å±‚)
        self._initialize_film_ir(new_id, storyboard)

        print(f"âœ… [Done] è§†é¢‘æ‹†è§£ä¸åˆ‡ç‰‡å®Œæˆï¼ŒJob ID: {new_id}")
        return new_id

    def _initialize_film_ir(self, job_id: str, storyboard: List[Dict]) -> None:
        """
        åˆå§‹åŒ– Film IR ç»“æ„
        å°†åŸå§‹åˆ†ææ•°æ®å¡«å……åˆ°æ”¯æŸ± III (Shot Recipe) çš„ concrete å±‚
        """
        ir = create_empty_film_ir(job_id, "input.mp4")

        # æ„å»º Shot Recipe concrete æ•°æ®
        shots_data = []
        for s in storyboard:
            shot_num = int(s.get("shot_number", 1))
            sid = f"shot_{shot_num:02d}"

            # æå–åˆ†é•œæ•°æ®
            narrative_desc = s.get("frame_description") or s.get("content_analysis") or ""

            shot_item = {
                "shotId": sid,
                "beatTag": self._infer_beat_tag(shot_num, len(storyboard)),
                "startTime": s.get("start_time", "0:00"),
                "endTime": s.get("end_time", "0:00"),
                "durationSeconds": to_seconds(s.get("end_time", "0")) - to_seconds(s.get("start_time", "0")),

                # 8 æ ¸å¿ƒå­—æ®µ
                "subject": narrative_desc,
                "scene": s.get("content_analysis", ""),
                "camera": {
                    "shotSize": s.get("shot_scale", ""),
                    "cameraAngle": s.get("subject_orientation", ""),
                    "cameraMovement": s.get("camera_movement") or s.get("camera_type", ""),
                    "focalLengthDepth": s.get("camera_type", "")
                },
                "lighting": s.get("lighting", "Natural lighting"),
                "dynamics": s.get("motion_vector", ""),
                "audio": {
                    "soundDesign": "",
                    "music": s.get("music_mood", ""),
                    "dialogue": s.get("dialogue_voiceover", "")
                },
                "style": "",
                "negative": "",

                # èµ„äº§è·¯å¾„
                "assets": {
                    "firstFrame": f"frames/{sid}.png",
                    "sourceSegment": f"source_segments/{sid}.mp4",
                    "stylizedFrame": None,
                    "video": None
                }
            }
            shots_data.append(shot_item)

        # å¡«å……æ”¯æŸ± III concrete
        ir["pillars"]["III_shotRecipe"]["concrete"] = {
            "globalVisualLanguage": {
                "visualStyle": "",
                "colorPalette": "",
                "lightingDesign": "",
                "cameraPhilosophy": ""
            },
            "globalSoundDesign": {
                "musicStyle": "",
                "soundAtmosphere": "",
                "rhythmPattern": ""
            },
            "symbolism": {
                "repeatingImagery": "",
                "symbolicMeaning": ""
            },
            "shots": shots_data
        }

        # ä¿å­˜ Film IR
        save_film_ir(self.job_dir, ir)
        print(f"ğŸ¬ [Film IR] Initialized with {len(shots_data)} shots")

        # ğŸ¬ è§¦å‘ Stage 1: Specific Analysis (Story Theme)
        try:
            ir_manager = FilmIRManager(job_id, self.project_dir)
            result = ir_manager.run_stage("specificAnalysis")
            if result.get("status") == "success":
                print(f"âœ… [Film IR] Story Theme analysis completed")
                # ğŸ¯ å…³é”®ï¼šFilm IR åˆ†æå®Œæˆåï¼Œæ ¹æ® representativeTimestamp é‡æ–°æå–å¸§
                self._reextract_frames_from_film_ir(final_video_path)
            else:
                print(f"âš ï¸ [Film IR] Story Theme analysis: {result.get('reason', 'unknown error')}")
        except Exception as e:
            print(f"âš ï¸ [Film IR] Story Theme analysis failed: {e}")
            # ä¸é˜»å¡ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ

    def _reextract_frames_from_film_ir(self, video_path: Path):
        """
        ğŸ¯ æ ¹æ® Film IR çš„ representativeTimestamp é‡æ–°æå–å¸§
        è¿™æ˜¯åœ¨ Film IR åˆ†æå®Œæˆåè°ƒç”¨çš„ï¼Œç”¨äºä¿®æ­£åˆå§‹å¸§æå–çš„æ—¶é—´åå·®

        ç‰©ç†å¯¹ä½æ³•ï¼š
        1. çŸ­é•œå¤´è§„åˆ™ï¼ˆduration < 2sï¼‰ï¼šå¼ºåˆ¶ä½¿ç”¨ endTime - 0.2
        2. åå·®ä¿æŠ¤ï¼šsafe_ts = max(rep_ts, startTime + 1.2)
        3. è¾¹ç•Œä¿æŠ¤ï¼šmin(safe_ts, endTime - 0.1)
        """
        try:
            ir = load_film_ir(self.job_dir)
            if not ir:
                print(f"âš ï¸ [Frame Re-extract] Film IR not found, skipping")
                return

            # è·å– concrete shots
            shots = ir.get("pillars", {}).get("III_shotRecipe", {}).get("concrete", {}).get("shots", [])
            if not shots:
                print(f"âš ï¸ [Frame Re-extract] No shots in Film IR, skipping")
                return

            ffmpeg_path = get_ffmpeg_path()
            frames_dir = self.job_dir / "frames"
            reextracted_count = 0

            for shot in shots:
                shot_id = shot.get("shotId", "shot_01")
                rep_ts = shot.get("representativeTimestamp")
                start_time = shot.get("startTime", "00:00:00.000")
                end_time = shot.get("endTime", "00:00:00.000")

                # è½¬æ¢æ—¶é—´æ ¼å¼
                start_sec = to_seconds(start_time) or 0
                end_sec = to_seconds(end_time) or start_sec + 1
                duration = end_sec - start_sec

                extract_ts = None

                # ğŸ¯ è§„åˆ™ 1ï¼šçŸ­é•œå¤´å¼ºåˆ¶è§„åˆ™ï¼ˆduration < 2sï¼‰
                # å¯¹äºå¿«åˆ‡è§†é¢‘ï¼Œç›´æ¥ä½¿ç”¨ endTime - 0.2ï¼Œè¿™æ˜¯ç¡®ä¿åœºæ™¯è½¬æ¢å®Œæˆçš„å”¯ä¸€æ–¹æ³•
                if duration < 2.0:
                    extract_ts = end_sec - 0.2
                    print(f"âš¡ [Re-extract] {shot_id}: çŸ­é•œå¤´è§„åˆ™ {extract_ts:.2f}s (endTime - 0.2, duration={duration:.2f}s)")

                # ğŸ¯ è§„åˆ™ 2ï¼šæ­£å¸¸é•œå¤´ - AI é”šç‚¹ + åå·®ä¿æŠ¤
                elif rep_ts is not None:
                    # åå·®ä¿æŠ¤ï¼šç¡®ä¿è‡³å°‘åœ¨ startTime + 1.2s ä¹‹å
                    safe_ts = max(rep_ts, start_sec + 1.2)
                    # è¾¹ç•Œä¿æŠ¤ï¼šç¡®ä¿ä¸è¶…å‡ºé•œå¤´èŒƒå›´
                    extract_ts = min(safe_ts, end_sec - 0.1)
                    if safe_ts != rep_ts:
                        print(f"ğŸ›¡ï¸ [Re-extract] {shot_id}: åå·®ä¿æŠ¤ {extract_ts:.2f}s (AIç»™{rep_ts:.2f}s, ä¿®æ­£åˆ°startTime+1.2)")
                    else:
                        print(f"ğŸ¯ [Re-extract] {shot_id}: AI è¯­ä¹‰é”šç‚¹ {extract_ts:.2f}s")

                # ğŸ¯ è§„åˆ™ 3ï¼šä¿åº•é€»è¾‘
                if extract_ts is None:
                    extract_ts = start_sec + (duration * 0.8)
                    print(f"ğŸ“ [Re-extract] {shot_id}: æ•°å­¦ä¿åº• {extract_ts:.2f}s (80% ä½ç½®)")

                # é‡æ–°æå–å¸§
                frame_path = frames_dir / f"{shot_id}.png"
                subprocess.run([
                    ffmpeg_path, "-y",
                    "-i", str(video_path),
                    "-ss", str(extract_ts),
                    "-frames:v", "1",
                    "-q:v", "2",
                    str(frame_path)
                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                reextracted_count += 1

            print(f"âœ… [Frame Re-extract] å·²æ ¹æ®ç‰©ç†å¯¹ä½æ³•é‡æ–°æå– {reextracted_count} å¸§")

        except Exception as e:
            print(f"âš ï¸ [Frame Re-extract] Failed: {e}")

    def _infer_beat_tag(self, shot_num: int, total_shots: int) -> str:
        """æ¨æ–­åˆ†é•œçš„èŠ‚æ‹æ ‡ç­¾"""
        ratio = shot_num / total_shots
        if ratio <= 0.15:
            return "HOOK"
        elif ratio <= 0.4:
            return "SETUP"
        elif ratio <= 0.75:
            return "TURN"
        else:
            return "CTA"

    def _run_gemini_analysis(self, video_path: Path):
        from google.genai import types
        api_key = os.getenv("GEMINI_API_KEY")
        client = genai.Client(api_key=api_key)
        uploaded = client.files.upload(file=str(video_path))
        video_file = wait_until_file_active(client, uploaded)
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=[DIRECTOR_METAPROMPT, video_file],
            config=types.GenerateContentConfig(
                response_mime_type="application/json"
            )
        )
        raw_shots = extract_json_array(response.text)

        # è¯­ä¹‰åŒ–åˆå¹¶ï¼šå‡å°‘è¿‡åº¦åˆ†é•œ
        merged_shots = self._merge_semantic_shots(raw_shots, client)
        return merged_shots

    def _merge_semantic_shots(self, shots: List[Dict], client) -> List[Dict]:
        """
        è¯­ä¹‰åŒ–åˆå¹¶ï¼šå°†è¿ç»­çš„ã€èƒŒæ™¯/è§’åº¦/ä¸»ä½“ç›¸ä¼¼çš„åˆ†é•œåˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´åˆ†é•œã€‚
        ä½¿ç”¨ AI åˆ¤æ–­å“ªäº›è¿ç»­åˆ†é•œåº”è¯¥åˆå¹¶ã€‚
        """
        if len(shots) <= 1:
            return shots

        # æ„å»ºåˆå¹¶åˆ¤æ–­æç¤º
        shots_summary = []
        for i, s in enumerate(shots):
            shots_summary.append({
                "index": i,
                "start_time": s.get("start_time"),
                "end_time": s.get("end_time"),
                "description": s.get("frame_description") or s.get("content_analysis"),
                "shot_type": s.get("shot_type"),
                "camera_angle": s.get("camera_angle"),
                "camera_movement": s.get("camera_movement")
            })

        merge_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å½±è§†å‰ªè¾‘å¸ˆã€‚è¯·åˆ†æä»¥ä¸‹åˆ†é•œåˆ—è¡¨ï¼Œåˆ¤æ–­å“ªäº›**è¿ç»­çš„**åˆ†é•œåº”è¯¥åˆå¹¶ã€‚

åˆå¹¶æ¡ä»¶ï¼ˆå¿…é¡»åŒæ—¶æ»¡è¶³ï¼‰ï¼š
1. åˆ†é•œæ˜¯**è¿ç»­çš„**ï¼ˆindex ç›¸é‚»ï¼‰
2. åœºæ™¯/èƒŒæ™¯æ²¡æœ‰æ˜¾è‘—å˜åŒ–
3. ä¸»ä½“/è§’è‰²æ²¡æœ‰åˆ‡æ¢
4. æœºä½è§’åº¦æ²¡æœ‰æ˜æ˜¾å˜åŒ–
5. å±äºåŒä¸€ä¸ªå®Œæ•´åŠ¨ä½œæˆ–äº‹ä»¶

åˆ†é•œåˆ—è¡¨ï¼š
{json.dumps(shots_summary, ensure_ascii=False, indent=2)}

è¯·è¾“å‡ºéœ€è¦åˆå¹¶çš„åˆ†é•œç»„ï¼Œæ ¼å¼ä¸ºçº¯JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªéœ€è¦åˆå¹¶çš„indexæ•°ç»„ã€‚
ä¾‹å¦‚ï¼š[[0,1,2], [5,6]] è¡¨ç¤ºå°†0-1-2åˆå¹¶ä¸ºä¸€ä¸ªåˆ†é•œï¼Œ5-6åˆå¹¶ä¸ºä¸€ä¸ªåˆ†é•œã€‚
å¦‚æœæ²¡æœ‰éœ€è¦åˆå¹¶çš„ï¼Œè¾“å‡ºç©ºæ•°ç»„ []ã€‚
ä»…è¾“å‡ºçº¯JSONï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚"""

        try:
            merge_response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=[merge_prompt],
            )
            merge_text = merge_response.text.strip()

            # æå–JSONæ•°ç»„
            if merge_text.startswith("["):
                merge_groups = json.loads(merge_text)
            else:
                l = merge_text.find("[")
                r = merge_text.rfind("]")
                if l != -1 and r != -1:
                    merge_groups = json.loads(merge_text[l:r+1])
                else:
                    merge_groups = []

            if not merge_groups:
                print(f"ğŸ“Š è¯­ä¹‰åˆ†æï¼šæ— éœ€åˆå¹¶ï¼Œä¿ç•™ {len(shots)} ä¸ªåˆ†é•œ")
                return shots

            # ğŸ›¡ï¸ é˜²æ­¢è¿‡åº¦åˆå¹¶ï¼šæ£€æŸ¥åˆå¹¶åæ˜¯å¦ä¼šå°‘äº2ä¸ªé•œå¤´
            total_merged = sum(len(g) - 1 for g in merge_groups if isinstance(g, list) and len(g) > 1)
            result_count = len(shots) - total_merged
            if result_count < 2:
                print(f"âš ï¸ åˆå¹¶ååªå‰© {result_count} ä¸ªåˆ†é•œï¼Œå–æ¶ˆåˆå¹¶ä»¥ä¿ç•™åŸå§‹åˆ†é•œç»“æ„")
                return shots

            # ğŸ›¡ï¸ é˜²æ­¢å•ç»„è¿‡åº¦åˆå¹¶ï¼šå¦‚æœå•ä¸ªåˆå¹¶ç»„è¶…è¿‡3ä¸ªé•œå¤´ï¼Œæ‹†åˆ†æˆ–è·³è¿‡
            safe_merge_groups = []
            for group in merge_groups:
                if isinstance(group, list) and len(group) > 1:
                    if len(group) > 3:
                        print(f"âš ï¸ åˆå¹¶ç»„ {group} è¿‡å¤§ï¼ˆ{len(group)}ä¸ªé•œå¤´ï¼‰ï¼Œè·³è¿‡æ­¤åˆå¹¶")
                        continue
                    safe_merge_groups.append(group)
            merge_groups = safe_merge_groups

            if not merge_groups:
                print(f"ğŸ“Š æ‰€æœ‰åˆå¹¶ç»„è¢«è¿‡æ»¤ï¼Œä¿ç•™åŸå§‹ {len(shots)} ä¸ªåˆ†é•œ")
                return shots

            # æ‰§è¡Œåˆå¹¶
            merged_indices = set()
            for group in merge_groups:
                if isinstance(group, list) and len(group) > 1:
                    merged_indices.update(group[1:])  # é™¤äº†ç¬¬ä¸€ä¸ªï¼Œå…¶ä½™æ ‡è®°ä¸ºè¢«åˆå¹¶

            result = []
            i = 0
            new_shot_num = 1
            while i < len(shots):
                shot = shots[i].copy()

                # æ£€æŸ¥æ˜¯å¦æ˜¯åˆå¹¶ç»„çš„èµ·å§‹
                merge_group = None
                for group in merge_groups:
                    if isinstance(group, list) and len(group) > 1 and group[0] == i:
                        merge_group = group
                        break

                if merge_group:
                    # åˆå¹¶è¯¥ç»„çš„æ‰€æœ‰åˆ†é•œ
                    last_idx = merge_group[-1]
                    shot["end_time"] = shots[last_idx].get("end_time")

                    # åˆå¹¶æè¿°
                    descriptions = []
                    for idx in merge_group:
                        if idx < len(shots):
                            desc = shots[idx].get("frame_description") or shots[idx].get("content_analysis")
                            if desc and desc not in descriptions:
                                descriptions.append(desc)
                    shot["frame_description"] = " â†’ ".join(descriptions[:3])  # æœ€å¤šä¿ç•™3æ®µæè¿°
                    shot["content_analysis"] = shot["frame_description"]

                    print(f"ğŸ”— åˆå¹¶åˆ†é•œ {[s+1 for s in merge_group]} -> shot_{new_shot_num:02d}")
                    i = last_idx + 1
                else:
                    if i not in merged_indices:
                        i += 1
                    else:
                        i += 1
                        continue

                shot["shot_number"] = new_shot_num
                result.append(shot)
                new_shot_num += 1

            print(f"ğŸ“Š è¯­ä¹‰åˆå¹¶å®Œæˆï¼š{len(shots)} ä¸ªåˆ†é•œ -> {len(result)} ä¸ªåˆ†é•œ")
            return result

        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰åˆå¹¶åˆ†æå¤±è´¥ ({e})ï¼Œä¿ç•™åŸå§‹åˆ†é•œ")
            return shots

    def _run_ffmpeg_extraction(self, video_path: Path, storyboard: List):
        """
        æ¯«ç§’çº§ç²¾å‡†æå–ï¼š
        - å…³é”®å¸§æå–ï¼šä¼˜å…ˆä½¿ç”¨ AI è¯­ä¹‰é”šç‚¹ (representativeTimestamp)ï¼Œä¿åº•ä½¿ç”¨æ•°å­¦é€»è¾‘
        - è§†é¢‘ç‰‡æ®µï¼šä½¿ç”¨ç²¾å‡†åˆ‡å‰²æ¨¡å¼
        """
        ffmpeg_path = get_ffmpeg_path()
        for s in storyboard:
            ts = to_seconds(s.get("start_time"))
            end_ts = to_seconds(s.get("end_time"))
            duration = end_ts - ts
            sid = f"shot_{int(s['shot_number']):02d}"

            # ğŸ¯ å…³é”®å¸§æå–ï¼šAI è¯­ä¹‰é”šç‚¹ + æ•°å­¦ä¿åº•
            # ä¼˜å…ˆçº§ï¼šrepresentativeTimestamp > startTime + duration * 0.8
            extract_ts = None
            extraction_method = "fallback"

            # é¦–é€‰ï¼šAI æä¾›çš„ä»£è¡¨å¸§æ—¶é—´æˆ³
            rep_ts = s.get("representativeTimestamp") or s.get("representative_timestamp")
            if rep_ts is not None:
                rep_ts = to_seconds(rep_ts)
                # æ ¡éªŒï¼šæ—¶é—´æˆ³å¿…é¡»åœ¨ [startTime + 0.1, endTime - 0.1] èŒƒå›´å†…
                min_valid = ts + 0.1
                max_valid = end_ts - 0.1
                if rep_ts is not None and min_valid <= rep_ts <= max_valid:
                    extract_ts = rep_ts
                    extraction_method = "ai_anchor"
                    print(f"ğŸ¯ {sid}: ä½¿ç”¨ AI è¯­ä¹‰é”šç‚¹ {extract_ts:.2f}s")

            # ä¿åº•ï¼šæ•°å­¦ä¿®æ­£ (startTime + duration * 0.8)
            if extract_ts is None:
                extract_ts = ts + (duration * 0.8)
                print(f"ğŸ“ {sid}: ä½¿ç”¨æ•°å­¦ä¿åº• {extract_ts:.2f}s (80% ä½ç½®)")

            img_out = self.job_dir / "frames" / f"{sid}.png"
            subprocess.run([
                ffmpeg_path, "-y",
                "-i", str(video_path),
                "-ss", str(extract_ts),
                "-frames:v", "1",
                "-q:v", "2",
                str(img_out)
            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

            # ğŸ¯ ç²¾å‡†è§†é¢‘ç‰‡æ®µåˆ‡å‰²
            video_segment_out = self.job_dir / "source_segments" / f"{sid}.mp4"
            subprocess.run([
                ffmpeg_path, "-y",
                "-i", str(video_path),
                "-ss", str(ts),           # è§†é¢‘ç‰‡æ®µä»èµ·å§‹ç‚¹å¼€å§‹
                "-t", str(duration),
                "-c:v", "libx264",        # é‡æ–°ç¼–ç ä»¥ç¡®ä¿ç²¾å‡†åˆ‡å‰²
                "-c:a", "aac",
                "-avoid_negative_ts", "make_zero",
                str(video_segment_out)
            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    def load(self):
        """åŠ è½½çŠ¶æ€å¹¶å¯¹é½ç‰©ç†æ–‡ä»¶çŠ¶æ€"""
        self.workflow = load_workflow(self.job_dir)
        if "global_stages" not in self.workflow:
            self.workflow["global_stages"] = {"analyze": "SUCCESS", "extract": "SUCCESS", "stylize": "NOT_STARTED", "video_gen": "NOT_STARTED", "merge": "NOT_STARTED"}

        updated = False
        shots = self.workflow.get("shots", [])
        for shot in shots:
            sid = shot.get("shot_id")
            status_node = shot.get("status", {})
            
            # 1. é£æ ¼åŒ–å‚è€ƒå›¾ç‰©ç†å¯¹é½
            stylized_path = self.job_dir / "stylized_frames" / f"{sid}.png"
            if stylized_path.exists() and status_node.get("stylize") != "SUCCESS":
                status_node["stylize"] = "SUCCESS"
                shot["assets"]["stylized_frame"] = f"stylized_frames/{sid}.png"
                updated = True

            # 2. è§†é¢‘äº§ç‰©ç‰©ç†å¯¹é½
            video_output_path = self.job_dir / "videos" / f"{sid}.mp4"
            current_video_status = status_node.get("video_generate")
            if video_output_path.exists() and current_video_status != "SUCCESS":
                status_node["video_generate"] = "SUCCESS"
                shot.setdefault("assets", {})["video"] = f"videos/{sid}.mp4"
                updated = True
            elif not video_output_path.exists() and current_video_status == "SUCCESS":
                status_node["video_generate"] = "NOT_STARTED"
                shot.setdefault("assets", {})["video"] = None
                updated = True
        
        # ğŸ’¡ æ ¸å¿ƒæ–°å¢ï¼šè®¡ç®—åˆå¹¶å°±ç»ªçŠ¶æ€ç»Ÿè®¡
        failed_count = sum(1 for s in shots if s["status"].get("video_generate") == "FAILED")
        pending_count = sum(1 for s in shots if s["status"].get("video_generate") in ["NOT_STARTED", "RUNNING"])
        
        self.workflow["merge_info"] = {
            "can_merge": failed_count == 0 and pending_count == 0 and len(shots) > 0,
            "failed_count": failed_count,
            "pending_count": pending_count,
            "message": ""
        }
        
        if failed_count > 0:
            self.workflow["merge_info"]["message"] = f"âš ï¸ {failed_count} shots failed and cannot be assembled."
        elif pending_count > 0:
            self.workflow["merge_info"]["message"] = "â³ Waiting for the shot list to be generated..."
        elif len(shots) > 0:
            self.workflow["merge_info"]["message"] = "âœ… All shots are ready and can be assembled into the final film."
        
        if updated: self.save()
        return self.workflow

    def save(self):
        self.workflow.setdefault("meta", {})["updated_at"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        save_workflow(self.job_dir, self.workflow)

    def apply_agent_action(self, action: Union[Dict, List]) -> Dict[str, Any]:
        """å¤„ç†ä¿®æ”¹æ„å›¾ï¼šå¼ºåˆ¶é‡ç½®åç»­æ‰€æœ‰ä¾èµ–èŠ‚ç‚¹"""
        actions = action if isinstance(action, list) else [action]
        total_affected = 0
        for act in actions:
            op = act.get("op")
            
            if op == "set_global_style":
                affected = apply_global_style(self.workflow, act.get("value"), cascade=True)
                if affected > 0:
                    for s in self.workflow.get("shots", []):
                        v_path = self.job_dir / "videos" / f"{s['shot_id']}.mp4"
                        if v_path.exists(): os.remove(v_path)
                        i_path = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                        if i_path.exists(): os.remove(i_path)
                        s["status"]["stylize"] = "NOT_STARTED"
                        s["status"]["video_generate"] = "NOT_STARTED"
                        s["assets"]["video"] = None
                        s["assets"]["stylized_frame"] = None
                total_affected += affected
                
            elif op == "global_subject_swap":
                old_subject = act.get("old_subject", "").lower()
                new_subject = act.get("new_subject", "").lower()
                if old_subject and new_subject:
                    for s in self.workflow.get("shots", []):
                        # ğŸï¸ Intelligent Scene Detection: Skip scenery/landscape shots
                        if self._is_scenery_shot(s["description"]):
                            print(f"ğŸï¸ Scenery shot skipped (no character injection): {s['shot_id']}")
                            continue

                        if old_subject in s["description"].lower():
                            desc = s["description"]

                            # ğŸ§¹ STEP 1: STRICT ATTRIBUTE PURGING for gender conflicts
                            # For simple swaps, purge gender-conflicting attributes
                            purged_desc = self._purge_conflicting_attributes(desc, old_subject, {})
                            print(f"ğŸ§¹ Purged conflicting attributes from {s['shot_id']}")

                            # ğŸ” STEP 2: Separate narrative layer from technical tags
                            tag_pattern = r'\[([A-Z]+): ([^\]]+)\]'
                            tags = re.findall(tag_pattern, purged_desc)
                            narrative_part = re.sub(tag_pattern, '', purged_desc).strip()

                            # ğŸ”„ STEP 3: Replace SUBJECT_PLACEHOLDER with new subject
                            if 'SUBJECT_PLACEHOLDER' in narrative_part:
                                # Replace ONLY the first placeholder with the subject
                                new_narrative = narrative_part.replace('SUBJECT_PLACEHOLDER', new_subject, 1)
                                # Replace remaining placeholders with "the [subject]"
                                new_narrative = new_narrative.replace('SUBJECT_PLACEHOLDER', f'the {new_subject}')
                            else:
                                # Fallback: direct replacement
                                new_narrative = re.sub(
                                    rf'\b{re.escape(old_subject)}\b',
                                    new_subject,
                                    narrative_part,
                                    flags=re.IGNORECASE
                                )

                            # ğŸ§¹ STEP 4: Semantic Sanitization for pronouns
                            new_narrative = self._semantic_sanitize_gender(new_narrative, old_subject, new_subject)

                            # ğŸ§¹ STEP 5: Clean up duplicates and grammar
                            new_narrative = re.sub(r',\s*,', ',', new_narrative)
                            new_narrative = re.sub(r'\s{2,}', ' ', new_narrative)
                            new_narrative = new_narrative.strip()
                            if new_narrative:
                                new_narrative = new_narrative[0].upper() + new_narrative[1:]

                            # Reconstruct description: narrative + preserved tags
                            tag_lines = [f"[{tag}: {value}]" for tag, value in tags]
                            s["description"] = new_narrative + ("\n" + "\n".join(tag_lines) if tag_lines else "")

                            s["status"]["stylize"] = "NOT_STARTED"
                            s["status"]["video_generate"] = "NOT_STARTED"
                            v_path = self.job_dir / "videos" / f"{s['shot_id']}.mp4"
                            if v_path.exists(): os.remove(v_path)
                            i_path = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                            if i_path.exists(): os.remove(i_path)
                            s["assets"]["video"] = None
                            s["assets"]["stylized_frame"] = None
                            total_affected += 1
                            print(f"ğŸ§¹ Clean swap applied: {s['shot_id']}")

            elif op == "detailed_subject_swap":
                # ğŸ¨ Fine-Grained Attribute Propagation: Detailed character replacement with visual attributes
                # ğŸ†” Global Identity Anchor: Store and propagate consistent character identity
                old_subject = act.get("old_subject", "").lower()
                new_subject = act.get("new_subject", "").lower()
                attributes = act.get("attributes", {})

                if old_subject and new_subject:
                    # Build the detailed character description from attributes
                    attr_parts = []

                    # Order attributes for natural reading: age, body, hair, eyes, skin, clothing, accessories, other
                    if attributes.get("age_descriptor"):
                        attr_parts.append(attributes["age_descriptor"])
                    if attributes.get("body_type"):
                        attr_parts.append(attributes["body_type"])

                    # Hair description (combine style and color)
                    hair_parts = []
                    if attributes.get("hair_color"):
                        hair_parts.append(attributes["hair_color"])
                    if attributes.get("hair_style"):
                        hair_parts.append(attributes["hair_style"])
                    if hair_parts:
                        attr_parts.append(" ".join(hair_parts) + " hair")

                    if attributes.get("eye_color"):
                        attr_parts.append(f"{attributes['eye_color']} eyes")
                    if attributes.get("skin_tone"):
                        attr_parts.append(f"{attributes['skin_tone']} skin")
                    if attributes.get("facial_features"):
                        attr_parts.append(f"with {attributes['facial_features']}")
                    if attributes.get("clothing"):
                        attr_parts.append(f"wearing {attributes['clothing']}")
                    if attributes.get("accessories"):
                        attr_parts.append(f"with {attributes['accessories']}")
                    if attributes.get("other_visual"):
                        attr_parts.append(attributes["other_visual"])

                    # Construct full character description
                    if attr_parts:
                        # Format: "a [attributes] [subject]" e.g., "a young golden short hair woman wearing red attire"
                        full_character_desc = f"a {' '.join(attr_parts)} {new_subject}"
                    else:
                        full_character_desc = new_subject

                    # ğŸ†” Store Global Identity Anchor in workflow for consistency tracking
                    self.workflow.setdefault("global", {})["identity_anchor"] = {
                        "base_subject": new_subject,
                        "full_description": full_character_desc,
                        "attributes": attributes,
                        "replaced_from": old_subject
                    }
                    print(f"ğŸ†” Global Identity Anchor set: '{full_character_desc}'")

                    shots_modified = 0
                    shots_skipped = 0

                    for s in self.workflow.get("shots", []):
                        # ğŸï¸ Intelligent Scene Detection: Skip scenery/landscape shots
                        if self._is_scenery_shot(s["description"]):
                            print(f"ğŸï¸ Scenery shot preserved (identity not injected): {s['shot_id']}")
                            shots_skipped += 1
                            continue

                        if old_subject in s["description"].lower():
                            desc = s["description"]

                            # ğŸ§¹ STEP 1: STRICT ATTRIBUTE PURGING
                            # Completely remove ALL conflicting descriptors before applying new Visual DNA
                            purged_desc = self._purge_conflicting_attributes(desc, old_subject, attributes)
                            print(f"ğŸ§¹ Purged conflicting attributes from {s['shot_id']}")

                            # ğŸ” STEP 2: Separate narrative layer from technical tags (tags preserved by purge)
                            tag_pattern = r'\[([A-Z]+): ([^\]]+)\]'
                            tags = re.findall(tag_pattern, purged_desc)
                            narrative_part = re.sub(tag_pattern, '', purged_desc).strip()

                            # ğŸ†” STEP 3: Replace SUBJECT_PLACEHOLDER with new identity
                            # The purge method leaves SUBJECT_PLACEHOLDER where the old subject was
                            if 'SUBJECT_PLACEHOLDER' in narrative_part:
                                # Replace ONLY the first placeholder with full description
                                new_narrative = narrative_part.replace('SUBJECT_PLACEHOLDER', full_character_desc, 1)
                                # Replace remaining placeholders with simple pronoun or "the [subject]"
                                new_narrative = new_narrative.replace('SUBJECT_PLACEHOLDER', f'the {new_subject}')
                            else:
                                # Fallback: append identity if placeholder not found
                                new_narrative = f"{full_character_desc}, {narrative_part}" if narrative_part else full_character_desc

                            # ğŸ§¹ STEP 4: Semantic Sanitization for pronouns
                            new_narrative = self._semantic_sanitize_gender(new_narrative, old_subject, new_subject)

                            # ğŸ§¹ STEP 5: Clean up duplicates and grammar
                            # Remove duplicate "a [subject]" patterns that may have been created
                            new_narrative = re.sub(rf'\b(a\s+{re.escape(new_subject)})\s*,\s*a\s+{re.escape(new_subject)}\b', r'\1', new_narrative, flags=re.IGNORECASE)
                            # Remove duplicate consecutive words
                            new_narrative = re.sub(r'\b(\w+)\s+\1\b', r'\1', new_narrative, flags=re.IGNORECASE)
                            # Clean up multiple commas/spaces
                            new_narrative = re.sub(r',\s*,', ',', new_narrative)
                            new_narrative = re.sub(r'\s{2,}', ' ', new_narrative)
                            # Capitalize first letter of sentence
                            new_narrative = new_narrative.strip()
                            if new_narrative:
                                new_narrative = new_narrative[0].upper() + new_narrative[1:]

                            # Reconstruct description: narrative + preserved tags
                            tag_lines = [f"[{tag}: {value}]" for tag, value in tags]
                            s["description"] = new_narrative + ("\n" + "\n".join(tag_lines) if tag_lines else "")

                            # Reset generation status
                            s["status"]["stylize"] = "NOT_STARTED"
                            s["status"]["video_generate"] = "NOT_STARTED"
                            v_path = self.job_dir / "videos" / f"{s['shot_id']}.mp4"
                            if v_path.exists(): os.remove(v_path)
                            i_path = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                            if i_path.exists(): os.remove(i_path)
                            s["assets"]["video"] = None
                            s["assets"]["stylized_frame"] = None
                            shots_modified += 1
                            print(f"ğŸ†” Clean identity applied: {s['shot_id']}")

                    total_affected += shots_modified
                    print(f"ğŸ¨ Identity Anchoring complete: {shots_modified} protagonist shots updated, {shots_skipped} scenery shots preserved")
                            
            elif op == "update_shot_params":
                sid = act.get("shot_id")
                for s in self.workflow.get("shots", []):
                    if s["shot_id"] == sid:
                        if "description" in act: s["description"] = act["description"]
                        s["status"]["stylize"] = "NOT_STARTED"
                        s["status"]["video_generate"] = "NOT_STARTED"
                        v_path = self.job_dir / "videos" / f"{sid}.mp4"
                        if v_path.exists(): os.remove(v_path)
                        i_path = self.job_dir / "stylized_frames" / f"{sid}.png"
                        if i_path.exists(): os.remove(i_path)
                        s["assets"]["video"] = None
                        s["assets"]["stylized_frame"] = None
                        total_affected += 1
                        break

            elif op == "enhance_shot_description":
                # ğŸ“ ç©ºé—´æ„ŸçŸ¥ + ğŸ¬ é£æ ¼å¼ºåŒ–ï¼šå¢å¼ºåˆ†é•œæè¿°
                sid = act.get("shot_id")
                spatial_info = act.get("spatial_info", "")
                style_boost = act.get("style_boost", "")
                for s in self.workflow.get("shots", []):
                    if s["shot_id"] == sid:
                        original_desc = s.get("description", "")
                        enhanced_parts = [original_desc]
                        if spatial_info:
                            enhanced_parts.append(f"[Spatial: {spatial_info}]")
                        if style_boost:
                            enhanced_parts.append(f"[Style: {style_boost}]")
                        s["description"] = " ".join(enhanced_parts)
                        s["status"]["stylize"] = "NOT_STARTED"
                        s["status"]["video_generate"] = "NOT_STARTED"
                        v_path = self.job_dir / "videos" / f"{sid}.mp4"
                        if v_path.exists(): os.remove(v_path)
                        i_path = self.job_dir / "stylized_frames" / f"{sid}.png"
                        if i_path.exists(): os.remove(i_path)
                        s["assets"]["video"] = None
                        s["assets"]["stylized_frame"] = None
                        total_affected += 1
                        print(f"ğŸ“ å¢å¼ºåˆ†é•œæè¿°: {sid} -> {s['description'][:80]}...")
                        break

            elif op == "update_cinematography":
                # ğŸ¬ æ‘„å½±å‚æ•°ä¿®æ”¹ï¼ˆä»…å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚æ—¶ï¼‰
                sid = act.get("shot_id")
                param = act.get("param", "")
                new_value = act.get("value", "")
                valid_params = ["shot_scale", "subject_frame_position", "subject_orientation", "gaze_direction", "motion_vector"]
                if param in valid_params and new_value:
                    for s in self.workflow.get("shots", []):
                        if s["shot_id"] == sid:
                            # Update the cinematography dict
                            s.setdefault("cinematography", {})[param] = new_value

                            # Update the description tags to match
                            tag_map = {
                                "shot_scale": "SCALE",
                                "subject_frame_position": "POSITION",
                                "subject_orientation": "ORIENTATION",
                                "gaze_direction": "GAZE",
                                "motion_vector": "MOTION"
                            }
                            tag_name = tag_map.get(param, param.upper())
                            desc = s.get("description", "")

                            # Replace existing tag or append new one
                            tag_pattern = rf'\[{tag_name}: [^\]]+\]'
                            new_tag = f"[{tag_name}: {new_value}]"
                            if re.search(tag_pattern, desc):
                                desc = re.sub(tag_pattern, new_tag, desc)
                            else:
                                desc = desc + f"\n{new_tag}"
                            s["description"] = desc

                            # Reset generation status
                            s["status"]["stylize"] = "NOT_STARTED"
                            s["status"]["video_generate"] = "NOT_STARTED"
                            v_path = self.job_dir / "videos" / f"{sid}.mp4"
                            if v_path.exists(): os.remove(v_path)
                            i_path = self.job_dir / "stylized_frames" / f"{sid}.png"
                            if i_path.exists(): os.remove(i_path)
                            s["assets"]["video"] = None
                            s["assets"]["stylized_frame"] = None
                            total_affected += 1
                            print(f"ğŸ¬ æ‘„å½±å‚æ•°æ›´æ–°: {sid} [{param}] -> {new_value}")
                            break

        if total_affected > 0: self.save()
        return {"status": "success", "affected_shots": total_affected}

    def run_node(self, node_type: str, shot_id: Optional[str] = None):
        """é€»è¾‘ç¼–æ’å¼•æ“ã€‚ç¡®ä¿â€˜å…ˆæœ‰å›¾ï¼Œåæœ‰è§†é¢‘â€™ä¸”æ— æ­»é”"""
        self.workflow.setdefault("meta", {}).setdefault("attempts", 0)
        self.workflow["meta"]["attempts"] += 1
        
        target_shots = [s for s in self.workflow.get("shots", []) if not shot_id or s["shot_id"] == shot_id]

        if node_type == "video_generate":
            for s in target_shots:
                # ç¡®ä¿ status å­—æ®µå­˜åœ¨
                if "status" not in s:
                    s["status"] = {"stylize": "NOT_STARTED", "video_generate": "NOT_STARTED"}
                if s["status"].get("stylize") != "SUCCESS":
                    print(f"ğŸ”— [Dependency] åˆ†é•œ {s['shot_id']} ç¼ºå°‘å®šå¦†å›¾ï¼Œæ­£åœ¨å‰ç½®ç”Ÿæˆ...")
                    run_stylize(self.job_dir, self.workflow, target_shot=s["shot_id"])
                    i_file = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                    if i_file.exists(): 
                        s["status"]["stylize"] = "SUCCESS"
                        s["assets"]["stylized_frame"] = f"stylized_frames/{s['shot_id']}.png"

        stage_key = "video_gen" if node_type == "video_generate" else "stylize"
        # ç¡®ä¿ global_stages å­˜åœ¨
        if "global_stages" not in self.workflow:
            self.workflow["global_stages"] = {
                "analyze": "SUCCESS",
                "extract": "SUCCESS",
                "stylize": "NOT_STARTED",
                "video_gen": "NOT_STARTED",
                "merge": "NOT_STARTED"
            }
        self.workflow["global_stages"][stage_key] = "RUNNING"

        for s in target_shots:
            if node_type == "video_generate":
                v_file = self.job_dir / "videos" / f"{s['shot_id']}.mp4"
                if v_file.exists(): os.remove(v_file)
                s["status"]["video_generate"] = "NOT_STARTED" 
                s["assets"]["video"] = None
            elif node_type == "stylize":
                i_file = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                if i_file.exists(): os.remove(i_file)
                s["status"]["stylize"] = "NOT_STARTED" 
                s["assets"]["stylized_frame"] = None

        self.save()

        if node_type == "stylize": 
            run_stylize(self.job_dir, self.workflow, target_shot=shot_id)
        elif node_type == "video_generate": 
            run_video_generate(self.job_dir, self.workflow, target_shot=shot_id)
        
        self.load() 

    def _is_scenery_shot(self, description: str) -> bool:
        """
        ğŸï¸ Intelligent Scene Detection: Determine if a shot is a scenery/landscape shot with no human protagonist.
        Returns True if the shot should be SKIPPED during character replacement.
        """
        desc_lower = description.lower()

        # Human subject indicators - if ANY of these are present, it's NOT a scenery shot
        human_indicators = [
            # Generic human terms
            'man', 'woman', 'person', 'people', 'human', 'figure', 'silhouette',
            'boy', 'girl', 'child', 'kid', 'baby', 'infant', 'toddler',
            'teenager', 'adult', 'elder', 'elderly', 'senior',
            # Relationship terms
            'father', 'mother', 'dad', 'mom', 'parent', 'son', 'daughter',
            'brother', 'sister', 'husband', 'wife', 'couple',
            'friend', 'stranger', 'visitor', 'guest',
            # Professional/role terms
            'worker', 'employee', 'boss', 'doctor', 'nurse', 'teacher', 'student',
            'driver', 'passenger', 'pedestrian', 'commuter',
            'actor', 'actress', 'performer', 'singer', 'dancer',
            'protagonist', 'character', 'hero', 'heroine',
            # Body parts (indicating human presence)
            'face', 'hand', 'hands', 'arm', 'arms', 'leg', 'legs',
            'head', 'body', 'shoulder', 'back', 'chest',
            'eye', 'eyes', 'hair', 'lips', 'mouth', 'nose',
            # Actions that imply human
            'walking', 'running', 'sitting', 'standing', 'talking', 'speaking',
            'looking', 'watching', 'holding', 'carrying', 'wearing',
            # Pronouns
            'he ', 'she ', 'his ', 'her ', 'him ', 'they ', 'their ',
        ]

        # Check for human presence
        has_human = any(indicator in desc_lower for indicator in human_indicators)

        if has_human:
            return False  # Not a scenery shot - has human protagonist

        # Scenery/landscape indicators - if these dominate without humans, it's scenery
        scenery_indicators = [
            # Nature scenes
            'landscape', 'scenery', 'vista', 'panorama', 'horizon',
            'mountain', 'valley', 'forest', 'woods', 'jungle', 'desert',
            'ocean', 'sea', 'lake', 'river', 'waterfall', 'beach', 'coast',
            'sky', 'clouds', 'sunset', 'sunrise', 'dawn', 'dusk', 'night sky',
            'field', 'meadow', 'prairie', 'grassland', 'garden', 'park',
            # Urban scenes without people
            'cityscape', 'skyline', 'building', 'architecture', 'street view',
            'empty room', 'interior', 'exterior', 'establishing shot',
            'aerial view', 'drone shot', 'bird\'s eye',
            # Object focus
            'close-up of object', 'food', 'vehicle', 'car ', 'train ', 'plane ',
            'furniture', 'decoration', 'artifact',
            # Transition/ambient
            'transition', 'time lapse', 'ambient', 'atmosphere', 'mood shot',
        ]

        # Check for scenery dominance
        has_scenery = any(indicator in desc_lower for indicator in scenery_indicators)

        return has_scenery  # Is scenery if indicators present and no humans

    def _semantic_sanitize_gender(self, description: str, old_subject: str, new_subject: str) -> str:
        """
        ğŸ§¹ Semantic Sanitization: Clean gender-conflicting attributes when swapping subjects.
        - Strips incompatible physical attributes (beard, mustache, Adam's apple, etc.)
        - Remaps gendered pronouns (he/him/his â†’ she/her/hers and vice versa)
        """
        # Define gender categories - expanded to catch variations
        male_keywords = {'man', 'men', 'boy', 'boys', 'male', 'males', 'gentleman', 'gentlemen',
                        'guy', 'guys', 'father', 'dad', 'husband', 'brother', 'son', 'uncle',
                        'grandfather', 'protagonist', 'protagonists', 'hero', 'heroes', 'actor'}
        female_keywords = {'woman', 'women', 'girl', 'girls', 'female', 'females', 'lady', 'ladies',
                         'mother', 'mom', 'wife', 'sister', 'daughter', 'aunt', 'grandmother',
                         'heroine', 'heroines', 'actress'}

        # Check if any male/female keyword appears in the subject string
        old_lower = old_subject.lower()
        new_lower = new_subject.lower()

        old_is_male = any(kw in old_lower for kw in male_keywords)
        old_is_female = any(kw in old_lower for kw in female_keywords)
        new_is_male = any(kw in new_lower for kw in male_keywords)
        new_is_female = any(kw in new_lower for kw in female_keywords)

        # Only sanitize if there's a gender change
        if (old_is_male and new_is_female) or (old_is_female and new_is_male):
            if old_is_male and new_is_female:
                # Male â†’ Female: Remove male-specific attributes
                male_attributes = [
                    # Mustache - ALL variations (most comprehensive)
                    r'\bwith\s+(?:a\s+)?(?:\w+\s+)*mustache\b',  # "with a thick mustache", "with a handlebar mustache"
                    r'\bwith\s+(?:a\s+)?(?:\w+\s+)*moustache\b', # British spelling
                    r'\bhas\s+(?:a\s+)?(?:\w+\s+)*mustache\b',   # "has a mustache"
                    r'\bhas\s+(?:a\s+)?(?:\w+\s+)*moustache\b',
                    r'\bhaving\s+(?:a\s+)?(?:\w+\s+)*mustache\b', # "having a mustache"
                    r'\bsporting\s+(?:a\s+)?(?:\w+\s+)*mustache\b', # "sporting a mustache"
                    r'\b(?:his|the|a)\s+(?:\w+\s+)*mustache\b',  # "his thick mustache", "the mustache"
                    r'\b(?:his|the|a)\s+(?:\w+\s+)*moustache\b',
                    r'\b\w+\s+mustache\b',  # "thick mustache", "handlebar mustache"
                    r'\b\w+\s+moustache\b',
                    r'\bmustached\b', r'\bmoustached\b',
                    r'\bmustache\b', r'\bmoustache\b',  # standalone as last resort
                    # Beard - ALL variations
                    r'\bwith\s+(?:a\s+)?(?:\w+\s+)*beard\b',
                    r'\bhas\s+(?:a\s+)?(?:\w+\s+)*beard\b',
                    r'\b(?:his|the|a)\s+(?:\w+\s+)*beard\b',
                    r'\b\w+\s+beard\b',  # "thick beard", "full beard"
                    r'\bbearded\b', r'\bbeard\b',
                    # Goatee
                    r'\bwith\s+(?:a\s+)?(?:\w+\s+)*goatee\b',
                    r'\b(?:his|the|a)\s+(?:\w+\s+)*goatee\b',
                    r'\bgoatee\b',
                    # Stubble
                    r'\bwith\s+(?:\w+\s+)*stubble\b',
                    r'\b\w+\s+stubble\b',  # "five o'clock stubble"
                    r'\bstubbled\b', r'\bstubble\b',
                    # Facial hair general
                    r'\bfacial\s+hair\b', r'\bwith\s+facial\s+hair\b',
                    # Other male-specific
                    r'\bAdam\'s\s+apple\b',
                    r'\bbald\b', r'\bbalding\b',
                    r'\bmuscular\b', r'\bbuff\b',
                    r'\bbroad[- ]shouldered\b',
                    r'\bchest\s+hair\b', r'\bwith\s+chest\s+hair\b',
                ]
                for attr in male_attributes:
                    description = re.sub(attr, '', description, flags=re.IGNORECASE)

                # Remap pronouns: he/him/his â†’ she/her/hers
                description = re.sub(r'\bhe\b', 'she', description, flags=re.IGNORECASE)
                description = re.sub(r'\bhim\b', 'her', description, flags=re.IGNORECASE)
                description = re.sub(r'\bhis\b', 'her', description, flags=re.IGNORECASE)
                description = re.sub(r'\bhimself\b', 'herself', description, flags=re.IGNORECASE)

            elif old_is_female and new_is_male:
                # Female â†’ Male: Remove female-specific attributes
                female_attributes = [
                    r'\b(lipstick)\b', r'\b(makeup|make-up)\b', r'\b(long eyelashes)\b',
                    r'\b(feminine)\b', r'\b(pregnant)\b', r'\b(nursing)\b',
                    r'\b(wearing a dress)\b', r'\b(in a skirt)\b'
                ]
                for attr in female_attributes:
                    description = re.sub(attr, '', description, flags=re.IGNORECASE)

                # Remap pronouns: she/her/hers â†’ he/him/his
                description = re.sub(r'\bshe\b', 'he', description, flags=re.IGNORECASE)
                description = re.sub(r'\bher\b(?!\s+\w+ing)', 'him', description, flags=re.IGNORECASE)  # Avoid "her walking"
                description = re.sub(r'\bhers\b', 'his', description, flags=re.IGNORECASE)
                description = re.sub(r'\bherself\b', 'himself', description, flags=re.IGNORECASE)

            # Clean up any double spaces left from attribute removal
            description = re.sub(r'\s{2,}', ' ', description).strip()

        return description

    def _purge_conflicting_attributes(self, description: str, old_subject: str, new_attributes: Dict) -> str:
        """
        ğŸ§¹ Strict Attribute Purging: Completely remove ALL conflicting descriptors before applying new Visual DNA.
        This ensures a CLEAN transformation with zero residues from the original subject.

        Purge Categories:
        1. Old subject name and variants
        2. Hair descriptions (color, style, length)
        3. Clothing descriptions
        4. Physical features and accessories
        5. Age descriptors
        6. Body type descriptors
        """
        # ğŸ” Separate technical tags from narrative (preserve tags)
        tag_pattern = r'\[([A-Z]+): ([^\]]+)\]'
        tags = re.findall(tag_pattern, description)
        narrative = re.sub(tag_pattern, '', description).strip()

        # ============================================
        # 1ï¸âƒ£ PURGE OLD SUBJECT NAME AND VARIANTS
        # ============================================
        old_lower = old_subject.lower()
        subject_variants = {
            'man': ['man', 'men', 'male', 'gentleman', 'guy', 'fellow', 'dude'],
            'woman': ['woman', 'women', 'female', 'lady', 'girl'],
            'boy': ['boy', 'boys', 'lad', 'young man', 'male child'],
            'girl': ['girl', 'girls', 'lass', 'young woman', 'female child'],
            'child': ['child', 'children', 'kid', 'kids', 'youngster'],
            'person': ['person', 'people', 'individual', 'figure'],
        }

        # Find which category the old subject belongs to
        variants_to_remove = [old_subject]
        for category, variants in subject_variants.items():
            if old_lower in variants or old_lower == category:
                variants_to_remove.extend(variants)
                break

        # Remove old subject and its variants (but keep the position for replacement)
        for variant in set(variants_to_remove):
            # Use word boundary to avoid partial matches
            narrative = re.sub(rf'\ba\s+{re.escape(variant)}\b', 'SUBJECT_PLACEHOLDER', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\bthe\s+{re.escape(variant)}\b', 'SUBJECT_PLACEHOLDER', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\b{re.escape(variant)}\b', 'SUBJECT_PLACEHOLDER', narrative, flags=re.IGNORECASE)

        # ============================================
        # 2ï¸âƒ£ PURGE ALL HAIR DESCRIPTIONS
        # ============================================
        hair_colors = [
            'black', 'brown', 'blonde', 'blond', 'golden', 'silver', 'gray', 'grey',
            'white', 'red', 'auburn', 'ginger', 'brunette', 'chestnut', 'platinum',
            'dark', 'light', 'dirty blonde', 'strawberry blonde', 'jet black',
            'salt and pepper', 'highlighted', 'dyed', 'colored'
        ]
        hair_styles = [
            'short', 'long', 'medium', 'curly', 'straight', 'wavy', 'frizzy',
            'bald', 'balding', 'shaved', 'buzz cut', 'crew cut', 'mohawk',
            'ponytail', 'bun', 'braided', 'braids', 'dreadlocks', 'dreads',
            'afro', 'pixie', 'bob', 'shoulder-length', 'flowing', 'slicked back',
            'messy', 'neat', 'tousled', 'spiky', 'receding', 'thinning',
            'thick', 'fine', 'wispy', 'layered'
        ]

        # Remove hair color + "hair" combinations
        for color in hair_colors:
            narrative = re.sub(rf'\b{color}\s+hair(ed)?\b', '', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\b{color}-hair(ed)?\b', '', narrative, flags=re.IGNORECASE)

        # Remove hair style + "hair" combinations
        for style in hair_styles:
            narrative = re.sub(rf'\b{style}\s+hair(ed)?\b', '', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\b{style}-hair(ed)?\b', '', narrative, flags=re.IGNORECASE)

        # Remove complex hair descriptions
        narrative = re.sub(r'\bwith\s+[\w\s]+\s+hair\b', '', narrative, flags=re.IGNORECASE)
        narrative = re.sub(r'\b[\w\s]+\s+haired\b', '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 3ï¸âƒ£ PURGE ALL CLOTHING DESCRIPTIONS
        # ============================================
        clothing_patterns = [
            r'\bwearing\s+[\w\s,]+(?:shirt|dress|suit|jacket|coat|pants|jeans|skirt|blouse|sweater|hoodie|t-shirt|tee|top|shorts|trousers|uniform|outfit|attire|clothes|clothing|garment)\b',
            r'\bin\s+(?:a\s+)?[\w\s]+(?:shirt|dress|suit|jacket|coat|pants|jeans|skirt|blouse|sweater|hoodie|t-shirt|tee|top|shorts|trousers|uniform|outfit|attire)\b',
            r'\bdressed\s+in\s+[\w\s,]+\b',
            r'\bclad\s+in\s+[\w\s,]+\b',
            # Specific clothing items with colors
            r'\b(?:red|blue|black|white|green|yellow|pink|purple|orange|brown|gray|grey)\s+(?:shirt|dress|suit|jacket|coat|pants|jeans|skirt|blouse|sweater|hoodie|t-shirt|top)\b',
        ]
        for pattern in clothing_patterns:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 4ï¸âƒ£ PURGE PHYSICAL FEATURES & ACCESSORIES
        # ============================================
        # Facial features - comprehensive patterns for mustache/beard/goatee
        facial_features = [
            # Mustache - ALL variations
            r'\bwith\s+(?:a\s+)?(?:\w+\s+)*mustache\b',
            r'\bwith\s+(?:a\s+)?(?:\w+\s+)*moustache\b',
            r'\bhas\s+(?:a\s+)?(?:\w+\s+)*mustache\b',
            r'\bhas\s+(?:a\s+)?(?:\w+\s+)*moustache\b',
            r'\bhaving\s+(?:a\s+)?(?:\w+\s+)*mustache\b',
            r'\bsporting\s+(?:a\s+)?(?:\w+\s+)*mustache\b',
            r'\b(?:his|the|a)\s+(?:\w+\s+)*mustache\b',
            r'\b(?:his|the|a)\s+(?:\w+\s+)*moustache\b',
            r'\b\w+\s+mustache\b',
            r'\b\w+\s+moustache\b',
            r'\bmustached\b', r'\bmoustached\b',
            r'\bmustache\b', r'\bmoustache\b',
            # Beard - ALL variations
            r'\bwith\s+(?:a\s+)?(?:\w+\s+)*beard\b',
            r'\bhas\s+(?:a\s+)?(?:\w+\s+)*beard\b',
            r'\b(?:his|the|a)\s+(?:\w+\s+)*beard\b',
            r'\b\w+\s+beard\b',
            r'\bbearded\b', r'\bbeard\b',
            # Goatee
            r'\bwith\s+(?:a\s+)?(?:\w+\s+)*goatee\b',
            r'\b(?:his|the|a)\s+(?:\w+\s+)*goatee\b',
            r'\bgoatee\b',
            r'\bwith\s+stubble\b', r'\bstubbled\b',
            r'\bwith\s+freckles\b', r'\bfreckled\b',
            r'\bwith\s+(?:a\s+)?scar\b', r'\bscarred\b',
            r'\bwith\s+dimples\b',
            r'\bwith\s+wrinkles\b', r'\bwrinkled\b',
            r'\bwith\s+(?:a\s+)?tattoo\b', r'\btattooed\b',
        ]
        for pattern in facial_features:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # Eye descriptions
        eye_colors = ['blue', 'green', 'brown', 'hazel', 'gray', 'grey', 'black', 'amber', 'violet']
        for color in eye_colors:
            narrative = re.sub(rf'\b{color}\s+eyes?\b', '', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\b{color}-eyed\b', '', narrative, flags=re.IGNORECASE)
        narrative = re.sub(r'\bwith\s+[\w\s]+\s+eyes\b', '', narrative, flags=re.IGNORECASE)

        # Accessories
        accessories = [
            r'\bwearing\s+(?:a\s+)?(?:glasses|sunglasses|spectacles)\b',
            r'\bwith\s+(?:a\s+)?(?:glasses|sunglasses|spectacles)\b',
            r'\bwearing\s+(?:a\s+)?(?:hat|cap|beanie|helmet)\b',
            r'\bwith\s+(?:a\s+)?(?:hat|cap|beanie|helmet)\b',
            r'\bwearing\s+(?:a\s+)?(?:necklace|earrings|bracelet|watch|ring)\b',
            r'\bwith\s+(?:a\s+)?(?:necklace|earrings|bracelet|watch|ring)\b',
            r'\bwearing\s+(?:a\s+)?(?:scarf|tie|bowtie|bow tie)\b',
            r'\bwith\s+(?:a\s+)?(?:scarf|tie|bowtie|bow tie)\b',
        ]
        for pattern in accessories:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 5ï¸âƒ£ PURGE AGE DESCRIPTORS
        # ============================================
        age_patterns = [
            r'\byoung\b', r'\bold\b', r'\belderly\b', r'\bmiddle-aged\b', r'\bmiddle aged\b',
            r'\bteenage\b', r'\bteen\b', r'\badult\b', r'\bsenior\b', r'\bjuvenile\b',
            r'\bin (?:his|her|their) (?:20s|30s|40s|50s|60s|70s|80s|90s|twenties|thirties|forties|fifties|sixties|seventies|eighties|nineties)\b',
        ]
        for pattern in age_patterns:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 6ï¸âƒ£ PURGE BODY TYPE DESCRIPTORS
        # ============================================
        body_patterns = [
            r'\b(?:tall|short|slim|slender|thin|skinny|fat|heavy|overweight|muscular|athletic|petite|stocky|lanky|burly|chubby|plump)\b',
            r'\bwell-built\b', r'\bwell built\b',
            r'\bbroad[- ]shouldered\b',
        ]
        for pattern in body_patterns:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 7ï¸âƒ£ PURGE SKIN TONE DESCRIPTORS
        # ============================================
        skin_patterns = [
            r'\bfair[- ]skinned\b', r'\bfair skin\b',
            r'\bdark[- ]skinned\b', r'\bdark skin\b',
            r'\bpale[- ]skinned\b', r'\bpale skin\b',
            r'\btan[- ]skinned\b', r'\btanned skin\b', r'\btanned\b',
            r'\bolive[- ]skinned\b', r'\bolive skin\b',
        ]
        for pattern in skin_patterns:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # CLEANUP
        # ============================================
        # Remove orphaned articles and prepositions
        narrative = re.sub(r'\ba\s+,', ',', narrative)
        narrative = re.sub(r'\bthe\s+,', ',', narrative)
        narrative = re.sub(r'\bwith\s+,', ',', narrative)
        narrative = re.sub(r'\bwearing\s+,', ',', narrative)
        narrative = re.sub(r'\bin\s+,', ',', narrative)

        # Remove multiple spaces and clean up
        narrative = re.sub(r'\s{2,}', ' ', narrative)
        narrative = re.sub(r'\s+,', ',', narrative)
        narrative = re.sub(r',\s*,', ',', narrative)
        narrative = re.sub(r'^\s*,\s*', '', narrative)
        narrative = re.sub(r'\s*,\s*$', '', narrative)
        narrative = narrative.strip()

        # Reconstruct with preserved tags
        if tags:
            tag_lines = [f"[{tag}: {value}]" for tag, value in tags]
            return narrative + "\n" + "\n".join(tag_lines)
        return narrative

    def _get_shot_by_id(self, shot_id: str) -> Optional[Dict]:
        for s in self.workflow.get("shots", []):
            if s.get("shot_id") == shot_id: return s
        return None

    def merge_videos(self) -> str:
        """æ‰§è¡Œæ— æŸåˆå¹¶"""
        ffmpeg_path = get_ffmpeg_path()
        success_shots = [s for s in self.workflow.get("shots", []) if s["status"].get("video_generate") == "SUCCESS"]
        if not success_shots: raise RuntimeError("æ²¡æœ‰å¯åˆå¹¶çš„åˆ†é•œè§†é¢‘ã€‚")
        success_shots.sort(key=lambda x: x["shot_id"])
        concat_list_path = self.job_dir / "concat_list.txt"
        output_video_path = self.job_dir / "final_output.mp4"
        with open(concat_list_path, "w", encoding="utf-8") as f:
            for s in success_shots:
                v_rel_path = s["assets"].get("video")
                if v_rel_path:
                    abs_v_path = (self.job_dir / v_rel_path).absolute()
                    f.write(f"file '{abs_v_path}'\n")
        cmd = [ffmpeg_path, "-y", "-f", "concat", "-safe", "0", "-i", str(concat_list_path), "-c", "copy", str(output_video_path)]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0: raise RuntimeError(f"åˆå¹¶å¤±è´¥: {result.stderr}")
        if "global_stages" in self.workflow:
            self.workflow["global_stages"]["merge"] = "SUCCESS"
        self.save()
        return "final_output.mp4"

    # ============================================================
    # Film IR é›†æˆ
    # ============================================================

    def get_film_ir_manager(self) -> FilmIRManager:
        """
        è·å– Film IR ç®¡ç†å™¨å®ä¾‹

        Returns:
            FilmIRManager å®ä¾‹
        """
        if not self.job_id:
            raise RuntimeError("Job ID not set")
        return FilmIRManager(self.job_id, self.project_dir)

    def has_film_ir(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨ Film IR"""
        if not self.job_id:
            return False
        return film_ir_exists(self.job_dir)