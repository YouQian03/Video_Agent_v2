# core/meta_prompts/intent_parser.py
"""
Meta Prompt: æ„å›¾è§£æå™¨ (Intent Parser)
å°†ç”¨æˆ·è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºç»“æ„åŒ–çš„ ParsedIntent JSON

æ”¯æŒçš„å˜æ›´ç±»å‹ (Remixable):
- ä¸»ä½“æ›¿æ¢ (1:1 æˆ– 1:N)
- ç¯å¢ƒ/åœºæ™¯å…¨æ›¿æ¢
- è§†è§‰è‰ºæœ¯é£æ ¼è¿ç§»
- æƒ…ç»ª/æ°›å›´è°ƒæ•´
- å…¨å±€å…‰å½±é‡è®¾

ä¿ç•™çš„è¾¹ç•Œ (Preserved):
- beatTag (å™äº‹èŠ‚æ‹) - é™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚æ”¹å˜èŠ‚å¥
- cameraPreserved (æ‘„å½±éª¨æ¶) - é•œå¤´è¯­è¨€å§‹ç»ˆä¿ç•™
"""

from typing import Dict, Any, Optional, List


INTENT_PARSER_PROMPT = """
# Role: Cinematic Intent Analyst (ç”µå½±æ„å›¾åˆ†æå¸ˆ)

You are an expert at understanding creative remix instructions and converting them into structured execution plans. Your task is to parse natural language instructions into a precise JSON format that can drive an AI video generation pipeline.

---

## Input Data
1. **User Instruction**: The natural language remix request
2. **Reference Images** (optional): Paths to uploaded reference images for character/element replacement
3. **Source Film IR Abstract**: The abstract template from the original video analysis
4. **Character Ledger**: Entity registry containing all detected characters with their visual signatures
5. **Environment Ledger**: Entity registry containing all detected environments/settings

---

## ğŸ¯ Entity Matching Protocol (CRITICAL)

When the user mentions a character or environment, you MUST identify the corresponding `entityId` from the Ledgers using this **THREE-STEP FUZZY MATCHING** process:

### Step 1: displayName Match
Try exact or partial match against entity `displayName`:
- User says "ä¸»è§’" â†’ look for displayName containing "ä¸»è§’", "protagonist", "main character"
- User says "é‚£ä¸ªå¥³å­©" â†’ look for displayName containing "å¥³å­©", "girl", "woman"

### Step 2: visualSignature Match
If Step 1 fails, search within `visualSignature` field for descriptive matches:
- User says "çº¢è¡£æœçš„äºº" â†’ search for "red" in visualSignature
- User says "æˆ´å¸½å­çš„" â†’ search for "hat", "cap" in visualSignature

### Step 3: visualCues Match (from Shot Appearances)
If Steps 1-2 fail, examine entity appearances in specific shots:
- Check `appearances[].visualCues` for additional visual details
- Match against user's descriptive language

### Matching Result
- **If matched**: Output the EXACT `entityId` from the Ledger (e.g., `"orig_char_01"`, `"orig_env_02"`)
  - âš ï¸ CRITICAL: Copy the entityId EXACTLY as it appears in the Ledger, including the "orig_" prefix!
  - Example: Ledger has `entityId: "orig_char_01"` â†’ use `"orig_char_01"` (NOT "char_01", NOT "new_char_01")
- **If NOT matched** (user introduces an ADDITIONAL character that doesn't exist in source video): Generate NEW_ENTITY ID:
  - For characters: `"new_char_01"`, `"new_char_02"`, etc.
  - For environments: `"new_env_01"`, `"new_env_02"`, etc.
  - âš ï¸ IMPORTANT: "Replace X with Y" means X is the ORIGINAL entity, Y is the new APPEARANCE - this is NOT a new entity!
  - Only use new_char_XX when user ADDS a character that wasn't in the original video at all

---

## Parsing Rules

### 1. Subject Mapping (ä¸»ä½“æ˜ å°„)
Identify ALL subject replacement requests:
- **1:1 Replacement**: "æŠŠçŒ«æ¢æˆç‹—" â†’ single subject swap
- **1:N Replacement**: "æŠŠä¸»è§’æ¢æˆä¸‰ä¸ªå°å­©" â†’ one-to-many expansion
- **Attribute Inheritance**: Extract persistent visual attributes (e.g., "çº¢è‰²æŠ«é£", "é‡‘å±å¤–å£³")
- **Reference Image Binding**: If user provides a reference image for a character, bind the image path

**CRITICAL - Entity ID Binding**:
- You MUST use the Entity Matching Protocol above to find the `originalEntityId` from Character Ledger
- âš ï¸ REPLACEMENT vs NEW ENTITY:
  - "Replace people with LEGO" â†’ Match "people" to Ledger entity (e.g., orig_char_01), describe LEGO as new appearance
  - "æŠŠäººç‰©æ¢æˆä¹é«˜" â†’ Same: originalEntityId = orig_char_01 (from Ledger), toDescription = "LEGO minifigure"
  - User says "Add a new robot character" â†’ This IS a new entity: use new_char_XX
- Copy the `entityId` from Ledger EXACTLY (including "orig_" prefix)
- Only use `"new_char_XX"` when user adds a character that doesn't replace any existing entity
- The `detailedDescription` field MUST be 80-120 words and focus on VISUAL properties only:
  - âœ… Materials, textures, surface finishes (e.g., "brushed aluminum chassis with chrome accents")
  - âœ… Lighting behavior (e.g., "subsurface scattering on translucent skin")
  - âœ… Proportions and anatomical details (e.g., "elongated limbs, oversized head")
  - âœ… Color specifics with hex codes or Pantone references when possible
  - âŒ Avoid narrative language ("heroic", "mysterious", "powerful")
  - âŒ Avoid camera language ("viewed from below", "in the foreground")

### 2. Environment Mapping (ç¯å¢ƒæ˜ å°„)
Identify scene/setting changes:
- **Full Replacement**: "èƒŒæ™¯æ¢æˆæµ·è¾¹" â†’ complete environment swap
- **Partial Modification**: "åŠ ä¸Šä¸‹é›¨æ•ˆæœ" â†’ overlay/addition
- **Temporal Shift**: "æ”¹æˆå¤œæ™š" â†’ lighting/time-of-day change

**CRITICAL - Entity ID Binding**:
- You MUST use the Entity Matching Protocol to find the `originalEntityId` from Environment Ledger
- If the user introduces a completely NEW environment, use `"new_env_XX"` format
- The `detailedDescription` field MUST be 80-120 words focusing on:
  - âœ… Architectural materials (e.g., "weathered concrete with exposed rebar")
  - âœ… Lighting conditions (e.g., "diffused overcast light, soft shadows at 45Â°")
  - âœ… Atmospheric elements (e.g., "visible moisture particles, volumetric fog density 0.3")
  - âœ… Scale indicators and depth cues
  - âŒ Avoid mood descriptors without visual grounding

### 3. Style Instruction (é£æ ¼æŒ‡ä»¤)
Detect global visual style requests:
- **Art Style**: "ä¹é«˜é£æ ¼", "èµ›åšæœ‹å…‹", "æ°´å½©ç”»é£"
- **Material Implications**: Automatically derive material/texture needs from style
- **Lighting Implications**: Automatically derive lighting setup from style

### 4. Mood & Tone (æƒ…ç»ªåŸºè°ƒ)
Identify emotional atmosphere shifts:
- **From â†’ To Pattern**: Detect contrast (e.g., "ä»æ‚²ä¼¤æ”¹æˆå²è¯—æ„Ÿ")
- **Intensity Level**: "æ›´ç´§å¼ ", "æ›´è½»æ¾"
- **Genre Shift**: "æ”¹æˆå–œå‰§é£æ ¼"

### 5. Plot Restructuring (å‰§æƒ…é‡æ„) - Advanced
For complex requests involving narrative changes:
- **Theme Preservation**: Does user want to keep the core theme?
- **Story Arc Changes**: New conflict, climax, or resolution?
- **Character Dynamics**: New relationships or interactions?

### 6. Scope Detection (èŒƒå›´æ£€æµ‹)
Determine the scope of changes:
- **GLOBAL**: Affects all shots (e.g., "å…¨ç‰‡æ”¹æˆé»‘ç™½")
- **PARTIAL**: Affects specific shots or elements (e.g., "åªæ”¹ç¬¬ä¸€ä¸ªé•œå¤´çš„è½¦")
- **SINGLE_ELEMENT**: Minimal change (e.g., "æŠŠè½¦æ¢æˆæ³•æ‹‰åˆ©")

---

## Output Format (Strict JSON)

{
  "parseSuccess": true,
  "intentType": "ELEMENT_SWAP | STYLE_TRANSFER | PLOT_RESTRUCTURE | HYBRID",
  "scope": "GLOBAL | PARTIAL | SINGLE_ELEMENT",

  "subjectMapping": [
    {
      "originalEntityId": "char_01 (from Ledger) or new_char_01 (if new entity)",
      "fromPlaceholder": "[PROTAGONIST_A]",
      "fromDescription": "original subject description from Ledger visualSignature",
      "toDescription": "brief user-facing description (for UI display)",
      "detailedDescription": "80-120 word VISUAL description for Veo2 prompt generation. MUST include: materials/textures, lighting behavior, exact proportions, color specifics. MUST NOT include: narrative language, camera terms, mood without visual grounding.",
      "persistentAttributes": ["attribute1", "attribute2"],
      "imageReference": "path/to/reference/image.jpg or null",
      "affectedShots": ["all"] or ["shot_01", "shot_05"],
      "isNewEntity": false
    }
  ],

  "environmentMapping": [
    {
      "originalEntityId": "env_01 (from Ledger) or new_env_01 (if new entity)",
      "fromPlaceholder": "[SETTING]",
      "fromDescription": "original environment description from Ledger",
      "toDescription": "brief user-facing description (for UI display)",
      "detailedDescription": "80-120 word VISUAL description for Veo2 prompt generation. MUST include: architectural materials, lighting angles and quality, atmospheric density values, scale indicators. MUST NOT include: abstract mood terms without visual grounding.",
      "timeOfDay": "dawn | day | dusk | night | unchanged",
      "weather": "clear | rainy | snowy | foggy | unchanged",
      "affectedShots": ["all"] or ["shot_01", "shot_05"],
      "isNewEntity": false
    }
  ],

  "styleInstruction": {
    "artStyle": "style name or null",
    "materialImplications": "derived material/texture description",
    "lightingImplications": "derived lighting setup description",
    "colorPalette": "color scheme description or null"
  },

  "moodTone": {
    "originalMood": "detected from source",
    "targetMood": "user requested mood",
    "intensityShift": "increase | decrease | maintain",
    "genreShift": "new genre or null"
  },

  "plotRestructure": {
    "enabled": false,
    "themePreserved": true,
    "newConflict": "description or null",
    "newClimax": "description or null",
    "newResolution": "description or null",
    "narrativeNotes": "additional restructuring guidance"
  },

  "preservedElements": {
    "beatTagsPreserved": true,
    "cameraPreserved": true,
    "rhythmPreserved": true,
    "overrideReason": "null or user's explicit request to change"
  },

  "complianceCheck": {
    "passedSafetyCheck": true,
    "flaggedContent": [],
    "aspectRatioLocked": "16:9"
  },

  "parsingConfidence": 0.95,
  "ambiguities": ["any unclear points that may need clarification"]
}

---

## Safety & Compliance Rules

1. **Visual Compliance**: Automatically reject and flag requests for:
   - Violence, gore, or graphic content
   - Content violating Google AI safety guidelines
   - Inappropriate or harmful imagery

2. **Aspect Ratio Lock**: ALL outputs must enforce 16:9 aspect ratio

3. **Physical Realism**: Even in stylized modes (LEGO, anime), maintain:
   - Gravity and basic physics
   - Collision and interaction logic
   - Fluid dynamics where applicable

---

## Input to Analyze

**User Instruction**: {user_instruction}

**Reference Images**: {reference_images}

**Source Abstract Template**: {source_abstract}

**Character Ledger** (Entity Registry):
{character_ledger}

**Environment Ledger** (Entity Registry):
{environment_ledger}

---

## Final Checklist Before Output
1. âœ… Every `originalEntityId` is either from the Ledger OR uses `new_char_XX`/`new_env_XX` format
2. âœ… Every `detailedDescription` is 80-120 words with VISUAL properties only
3. âœ… Camera parameters are flagged as PRESERVED in `preservedElements`
4. âœ… All entity matches used the THREE-STEP fuzzy matching protocol

Output ONLY the JSON object. No markdown, no explanation.
"""


def parse_intent_result(ai_output: dict) -> dict:
    """
    éªŒè¯å¹¶è§„èŒƒåŒ– AI è¾“å‡ºçš„ ParsedIntent ç»“æ„

    Args:
        ai_output: Gemini è¿”å›çš„åŸå§‹ JSON

    Returns:
        è§„èŒƒåŒ–çš„ ParsedIntent å­—å…¸
    """
    # å¤„ç† Gemini è¿”å›åˆ—è¡¨è€Œéå­—å…¸çš„æƒ…å†µ
    if isinstance(ai_output, list):
        if len(ai_output) > 0 and isinstance(ai_output[0], dict):
            print(f"âš ï¸ Intent parsing received list, extracting first element")
            ai_output = ai_output[0]
        else:
            raise ValueError(f"Intent parsing returned invalid list format: {type(ai_output)}")

    if not isinstance(ai_output, dict):
        raise ValueError(f"Intent parsing expected dict, got: {type(ai_output)}")

    # è§„èŒƒåŒ– subjectMapping ä¸­çš„æ¯ä¸ªæ¡ç›®
    if "subjectMapping" in ai_output:
        for mapping in ai_output["subjectMapping"]:
            # ç¡®ä¿ isNewEntity æ­£ç¡®è®¾ç½®
            if "originalEntityId" in mapping:
                mapping["isNewEntity"] = mapping.get("isNewEntity", False) or \
                    mapping["originalEntityId"].startswith("new_")

    # è§„èŒƒåŒ– environmentMapping ä¸­çš„æ¯ä¸ªæ¡ç›®
    if "environmentMapping" in ai_output:
        for mapping in ai_output["environmentMapping"]:
            if "originalEntityId" in mapping:
                mapping["isNewEntity"] = mapping.get("isNewEntity", False) or \
                    mapping["originalEntityId"].startswith("new_")

    # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
    defaults = {
        "parseSuccess": True,
        "intentType": "ELEMENT_SWAP",
        "scope": "GLOBAL",
        "subjectMapping": [],
        "environmentMapping": [],
        "styleInstruction": {
            "artStyle": None,
            "materialImplications": "",
            "lightingImplications": "",
            "colorPalette": None
        },
        "moodTone": {
            "originalMood": "",
            "targetMood": "",
            "intensityShift": "maintain",
            "genreShift": None
        },
        "plotRestructure": {
            "enabled": False,
            "themePreserved": True,
            "newConflict": None,
            "newClimax": None,
            "newResolution": None,
            "narrativeNotes": ""
        },
        "preservedElements": {
            "beatTagsPreserved": True,
            "cameraPreserved": True,
            "rhythmPreserved": True,
            "overrideReason": None
        },
        "complianceCheck": {
            "passedSafetyCheck": True,
            "flaggedContent": [],
            "aspectRatioLocked": "16:9"
        },
        "parsingConfidence": 0.9,
        "ambiguities": []
    }

    # åˆå¹¶é»˜è®¤å€¼
    result = {**defaults}

    for key, value in ai_output.items():
        if key in result:
            if isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = {**result[key], **value}
            else:
                result[key] = value

    return result


def extract_subject_mappings(parsed_intent: dict) -> List[dict]:
    """
    æå–ä¸»ä½“æ˜ å°„åˆ—è¡¨ï¼Œç”¨äºåç»­ Identity Anchor ç”Ÿæˆ

    Returns:
        List of {originalEntityId, fromPlaceholder, toDescription, detailedDescription,
                 persistentAttributes, imageReference, affectedShots, isNewEntity}
    """
    mappings = parsed_intent.get("subjectMapping", [])
    return [
        {
            "originalEntityId": m.get("originalEntityId", "new_char_01"),
            "fromPlaceholder": m.get("fromPlaceholder", "[SUBJECT]"),
            "fromDescription": m.get("fromDescription", ""),
            "toDescription": m.get("toDescription", ""),
            "detailedDescription": m.get("detailedDescription", ""),
            "persistentAttributes": m.get("persistentAttributes", []),
            "imageReference": m.get("imageReference"),
            "affectedShots": m.get("affectedShots", ["all"]),
            "isNewEntity": m.get("isNewEntity", False) or m.get("originalEntityId", "").startswith("new_")
        }
        for m in mappings
    ]


def extract_environment_mappings(parsed_intent: dict) -> List[dict]:
    """
    æå–ç¯å¢ƒæ˜ å°„åˆ—è¡¨

    Returns:
        List of {originalEntityId, fromPlaceholder, toDescription, detailedDescription,
                 timeOfDay, weather, affectedShots, isNewEntity}
    """
    mappings = parsed_intent.get("environmentMapping", [])
    return [
        {
            "originalEntityId": m.get("originalEntityId", "new_env_01"),
            "fromPlaceholder": m.get("fromPlaceholder", "[SETTING]"),
            "fromDescription": m.get("fromDescription", ""),
            "toDescription": m.get("toDescription", ""),
            "detailedDescription": m.get("detailedDescription", ""),
            "timeOfDay": m.get("timeOfDay", "unchanged"),
            "weather": m.get("weather", "unchanged"),
            "affectedShots": m.get("affectedShots", ["all"]),
            "isNewEntity": m.get("isNewEntity", False) or m.get("originalEntityId", "").startswith("new_")
        }
        for m in mappings
    ]


def get_intent_summary(parsed_intent: dict) -> str:
    """
    ç”Ÿæˆæ„å›¾æ‘˜è¦ï¼Œç”¨äºæ—¥å¿—å’Œè°ƒè¯•

    Returns:
        Human-readable summary string
    """
    intent_type = parsed_intent.get("intentType", "UNKNOWN")
    scope = parsed_intent.get("scope", "UNKNOWN")

    subjects = len(parsed_intent.get("subjectMapping", []))
    environments = len(parsed_intent.get("environmentMapping", []))

    style = parsed_intent.get("styleInstruction", {}).get("artStyle", "None")
    mood = parsed_intent.get("moodTone", {}).get("targetMood", "unchanged")

    plot_enabled = parsed_intent.get("plotRestructure", {}).get("enabled", False)

    summary_parts = [
        f"Type: {intent_type}",
        f"Scope: {scope}",
        f"Subject Changes: {subjects}",
        f"Environment Changes: {environments}",
        f"Style: {style}",
        f"Mood: {mood}",
        f"Plot Restructure: {'Yes' if plot_enabled else 'No'}"
    ]

    return " | ".join(summary_parts)


def check_compliance(parsed_intent: dict) -> tuple:
    """
    æ£€æŸ¥æ„å›¾æ˜¯å¦é€šè¿‡åˆè§„æ£€æŸ¥

    Returns:
        (is_compliant: bool, issues: List[str])
    """
    compliance = parsed_intent.get("complianceCheck", {})

    is_compliant = compliance.get("passedSafetyCheck", True)
    flagged = compliance.get("flaggedContent", [])

    issues = []

    if not is_compliant:
        issues.append("Safety check failed")

    if flagged:
        issues.extend([f"Flagged: {item}" for item in flagged])

    # æ£€æŸ¥ aspect ratio
    ar = compliance.get("aspectRatioLocked", "16:9")
    if ar != "16:9":
        issues.append(f"Invalid aspect ratio: {ar} (must be 16:9)")

    return (len(issues) == 0, issues)
