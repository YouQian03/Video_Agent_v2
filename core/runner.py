# core/runner.py
from pathlib import Path
import shutil
import subprocess
import time
import os
import requests
import io
from PIL import Image

from .workflow_io import save_workflow, load_workflow
from .utils import get_ffmpeg_path
from .film_ir_io import load_film_ir, film_ir_exists
from typing import Dict, Any, Optional, Tuple


def ensure_videos_dir(job_dir: Path) -> Path:
    videos_dir = job_dir / "videos"
    videos_dir.mkdir(parents=True, exist_ok=True)
    return videos_dir


def get_remix_shot_data(job_dir: Path, shot_id: str) -> Tuple[Optional[Dict], Optional[Dict], Optional[Dict]]:
    """
    ğŸ¬ è·å– Remix åçš„åˆ†é•œæ•°æ®

    æ£€æŸ¥ Film IR æ˜¯å¦æœ‰ remixed å±‚ï¼Œå¦‚æœæœ‰åˆ™è¿”å›ï¼š
    1. remixed i2v prompt æ•°æ®
    2. identity anchors (è§’è‰²/ç¯å¢ƒé”šç‚¹)
    3. visual style é…ç½®

    Returns:
        Tuple of (i2v_prompt_data, identity_anchors, visual_style) or (None, None, None)
    """
    if not film_ir_exists(job_dir):
        return None, None, None

    try:
        ir = load_film_ir(job_dir)
        if not ir:
            return None, None, None

        # ğŸ¯ å…³é”®ï¼šremixedLayer åœ¨ userIntent ä¸‹ï¼Œä¸åœ¨ pillars ä¸‹
        remixed = ir.get("userIntent", {}).get("remixedLayer", {})

        if not remixed:
            return None, None, None

        # æŸ¥æ‰¾å¯¹åº”çš„ shot
        remixed_shots = remixed.get("shots", [])
        target_shot = None
        for shot in remixed_shots:
            if shot.get("shotId") == shot_id:
                target_shot = shot
                break

        if not target_shot:
            return None, None, None

        # è·å– identity anchors
        identity_anchors = remixed.get("identityAnchors", {})

        # è·å– visual style é…ç½®
        render_strategy = ir.get("pillars", {}).get("IV_renderStrategy", {})
        visual_style = render_strategy.get("visualStyleConfig", {})

        print(f"ğŸ¬ [Remix Data] Found remixed data for {shot_id}")
        return target_shot, identity_anchors, visual_style

    except Exception as e:
        print(f"âš ï¸ [Remix Data] Error loading remix data: {e}")
        return None, None, None


def build_remix_prompt(remixed_shot: Dict, identity_anchors: Dict, visual_style: Dict) -> str:
    """
    ğŸ¨ æ„å»ºåŸºäº Remix æ•°æ®çš„ç”Ÿæˆ Prompt

    æ•´åˆï¼š
    1. remixed shot çš„ i2v prompt
    2. identity anchors çš„è¯¦ç»†æè¿°
    3. visual style çš„é£æ ¼é…ç½®
    """
    # åŸºç¡€ prompt - ä» remixed shot è·å–
    base_prompt = remixed_shot.get("remixedI2VPrompt", "") or remixed_shot.get("subject", "")

    # å¦‚æœæœ‰å®Œæ•´çš„ i2v prompt ç»“æ„
    if remixed_shot.get("i2vPrompt"):
        base_prompt = remixed_shot.get("i2vPrompt", {}).get("prompt", base_prompt)

    # æ„å»º identity æè¿°
    identity_parts = []

    # æ·»åŠ è§’è‰²é”šç‚¹
    characters = identity_anchors.get("characters", [])
    for char in characters:
        anchor_id = char.get("anchorId", "")
        # æ£€æŸ¥è¿™ä¸ª shot æ˜¯å¦ä½¿ç”¨äº†è¿™ä¸ªè§’è‰²
        applied_anchors = remixed_shot.get("appliedAnchors", {}).get("characters", [])
        if anchor_id in applied_anchors or not applied_anchors:
            desc = char.get("detailedDescription", "")
            if desc:
                identity_parts.append(f"Character: {desc}")

    # æ·»åŠ ç¯å¢ƒé”šç‚¹
    environments = identity_anchors.get("environments", [])
    for env in environments:
        anchor_id = env.get("anchorId", "")
        applied_anchors = remixed_shot.get("appliedAnchors", {}).get("environments", [])
        if anchor_id in applied_anchors or not applied_anchors:
            desc = env.get("detailedDescription", "")
            if desc:
                identity_parts.append(f"Environment: {desc}")

    # æ„å»º visual style æè¿°
    style_parts = []
    if visual_style.get("artStyle"):
        style_parts.append(f"Art Style: {visual_style['artStyle']}")
    if visual_style.get("colorPalette"):
        style_parts.append(f"Color: {visual_style['colorPalette']}")
    if visual_style.get("lightingMood"):
        style_parts.append(f"Lighting: {visual_style['lightingMood']}")
    if visual_style.get("cameraStyle"):
        style_parts.append(f"Camera: {visual_style['cameraStyle']}")

    # ç»„åˆæœ€ç»ˆ prompt
    final_prompt = base_prompt

    if identity_parts:
        final_prompt += "\n\n" + "\n".join(identity_parts)

    if style_parts:
        final_prompt += "\n\n[VISUAL STYLE]\n" + ", ".join(style_parts)

    return final_prompt


def get_effective_shot_data(job_dir: Path, wf: dict, shot: dict) -> Tuple[str, dict]:
    """
    ğŸ¯ è·å–æœ‰æ•ˆçš„åˆ†é•œæ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ Remix æ•°æ®ï¼‰

    é€»è¾‘ï¼š
    1. å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ remixed å±‚
    2. å¦‚æœæœ‰ï¼Œä½¿ç”¨ remixed prompt + identity anchors + visual style
    3. å¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨åŸå§‹ workflow çš„ description

    Returns:
        Tuple of (effective_prompt, effective_cinematography)
    """
    shot_id = shot.get("shot_id")

    # å°è¯•è·å– remix æ•°æ®
    remixed_shot, identity_anchors, visual_style = get_remix_shot_data(job_dir, shot_id)

    if remixed_shot:
        # ä½¿ç”¨ remix æ•°æ®
        effective_prompt = build_remix_prompt(remixed_shot, identity_anchors, visual_style)

        # è·å–æ‘„å½±å‚æ•° - ä¼˜å…ˆä½¿ç”¨ remixed çš„ camera æ•°æ®
        camera_data = remixed_shot.get("camera", {})
        if not camera_data:
            camera_data = remixed_shot.get("cameraPreserved", {})

        effective_cinema = {
            "shot_scale": camera_data.get("shotSize", shot.get("cinematography", {}).get("shot_scale", "")),
            "subject_frame_position": shot.get("cinematography", {}).get("subject_frame_position", ""),
            "subject_orientation": camera_data.get("cameraAngle", shot.get("cinematography", {}).get("subject_orientation", "")),
            "gaze_direction": shot.get("cinematography", {}).get("gaze_direction", ""),
            "motion_vector": camera_data.get("cameraMovement", shot.get("cinematography", {}).get("motion_vector", "static")),
            "camera_type": shot.get("cinematography", {}).get("camera_type", "")
        }

        print(f"âœ… [Effective Data] Using REMIXED data for {shot_id}")
        return effective_prompt, effective_cinema
    else:
        # ä½¿ç”¨åŸå§‹æ•°æ®
        effective_prompt = shot.get("description", "")
        effective_cinema = shot.get("cinematography", {})
        print(f"ğŸ“‹ [Effective Data] Using ORIGINAL workflow data for {shot_id}")
        return effective_prompt, effective_cinema


def ai_stylize_frame(job_dir: Path, wf: dict, shot: dict) -> str:
    """
    ğŸ’¡ ä½¿ç”¨ Imagen 4.0 æˆ– Gemini 2.0 Image Gen ç¡®ä¿å®šå¦†å›¾ç”ŸæˆæˆåŠŸ
    ğŸ¬ Cinematography Fidelity: Hard-coded enforcement of source shot parameters
    """
    from google import genai
    from google.genai import types

    api_key = os.getenv("GEMINI_API_KEY")
    client = genai.Client(api_key=api_key, http_options={'api_version': 'v1beta'})

    src = job_dir / shot["assets"]["first_frame"]
    dst = job_dir / "stylized_frames" / f"{shot['shot_id']}.png"
    dst.parent.mkdir(parents=True, exist_ok=True)

    if dst.exists(): os.remove(dst)

    global_style = wf.get("global", {}).get("style_prompt", "Cinematic")

    # ğŸ¬ è·å–æœ‰æ•ˆæ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ Remix æ•°æ®ï¼‰
    description, cinema = get_effective_shot_data(job_dir, wf, shot)

    # ğŸ¬ Extract cinematography parameters for fidelity enforcement
    shot_scale = cinema.get("shot_scale", "")
    subject_position = cinema.get("subject_frame_position", "")
    subject_orientation = cinema.get("subject_orientation", "")
    gaze_direction = cinema.get("gaze_direction", "")
    motion_vector = cinema.get("motion_vector", "")

    # ğŸ¯ Build cinematography constraint block
    cinema_constraints = []

    # 1ï¸âƒ£ Shot Scale Mapping
    scale_instructions = {
        "EXTREME_WIDE": "EXTREME WIDE SHOT - Subject very small in frame, vast environment dominates",
        "WIDE": "WIDE SHOT - Full body visible, significant environment context",
        "MEDIUM_WIDE": "MEDIUM WIDE SHOT - Subject from knees up, environmental context",
        "MEDIUM": "MEDIUM SHOT - Subject from waist up, balanced framing",
        "MEDIUM_CLOSE": "MEDIUM CLOSE-UP - Subject from chest up, intimate but contextual",
        "CLOSE_UP": "CLOSE-UP - Face fills most of frame, minimal background",
        "EXTREME_CLOSE_UP": "EXTREME CLOSE-UP - Single feature (eyes, lips) fills frame"
    }
    if shot_scale and shot_scale in scale_instructions:
        cinema_constraints.append(f"ğŸ“ SHOT SCALE: {scale_instructions[shot_scale]}")

    # 2ï¸âƒ£ Subject Position in Frame
    if subject_position:
        cinema_constraints.append(f"ğŸ“ FRAME POSITION: Subject MUST be positioned at {subject_position} of the 16:9 frame")

    # 3ï¸âƒ£ Orientation & Facing
    if subject_orientation:
        orientation_map = {
            "facing-camera": "Subject facing directly toward camera (frontal view)",
            "back-to-camera": "Subject's back facing camera (rear view)",
            "profile-left": "Subject in left profile (nose pointing to frame left)",
            "profile-right": "Subject in right profile (nose pointing to frame right)",
            "three-quarter-left": "Subject in 3/4 view facing left (showing right side of face)",
            "three-quarter-right": "Subject in 3/4 view facing right (showing left side of face)"
        }
        orient_desc = orientation_map.get(subject_orientation, subject_orientation)
        cinema_constraints.append(f"ğŸ§­ BODY ORIENTATION: {orient_desc}")

    # 4ï¸âƒ£ Gaze Direction
    if gaze_direction:
        gaze_map = {
            "looking-at-camera": "Eyes looking directly into camera lens",
            "looking-left": "Eyes directed toward the left side of frame",
            "looking-right": "Eyes directed toward the right side of frame",
            "looking-up": "Eyes directed upward",
            "looking-down": "Eyes directed downward",
            "looking-off-screen-left": "Eyes looking past the left edge of frame",
            "looking-off-screen-right": "Eyes looking past the right edge of frame"
        }
        gaze_desc = gaze_map.get(gaze_direction, gaze_direction)
        cinema_constraints.append(f"ğŸ‘ï¸ GAZE DIRECTION: {gaze_desc}")

    # 5ï¸âƒ£ Motion Vector
    if motion_vector and motion_vector != "static":
        cinema_constraints.append(f"ğŸƒ MOTION VECTOR: Capture mid-action of '{motion_vector}' - body pose and motion blur should indicate this movement")

    # Build final constraint string
    cinematography_block = ""
    if cinema_constraints:
        cinematography_block = "\n\nğŸ¬ CINEMATOGRAPHY FIDELITY - MANDATORY CONSTRAINTS (from source shot):\n" + "\n".join(cinema_constraints) + "\nâš ï¸ These parameters are LOCKED and must be preserved exactly as specified."

    # ğŸ¨ Conditional Design Elements: Only trigger graphic layouts if explicitly requested
    design_keywords = ['poster', 'layout', 'magazine', 'border', 'collage', 'graphic design', 'storyboard paper']
    style_lower = global_style.lower()
    is_design_style = any(kw in style_lower for kw in design_keywords)

    if is_design_style:
        # User explicitly requested a design/layout style
        prompt = f"""STYLIZED GRAPHIC DESIGN COMPOSITION.
Create a {global_style} layout with intentional design elements.
Subject: {description}.
Style: {global_style} - Apply graphic design aesthetics as requested.
Format: 16:9 aspect ratio with artistic layout elements.{cinematography_block}"""
    else:
        # ğŸ¬ DEFAULT: Full-bleed cinematic film still using structured prompt format
        # Format: [Subject], [Action/Pose], [Environment], [Style & Atmosphere], [Lighting & Color], [Camera & Tech Specs]

        # Extract action/pose from motion vector
        action_pose = motion_vector if motion_vector and motion_vector != "static" else "in a natural pose"

        # Build structured prompt components
        subject_block = f"[SUBJECT]: {description}"
        action_block = f"[ACTION/POSE]: {action_pose}, captured mid-motion with dynamic energy"
        environment_block = "[ENVIRONMENT]: Immersive scene environment extending to all edges of the 16:9 frame, rich background details"
        style_block = f"[STYLE & ATMOSPHERE]: {global_style} aesthetic, visually striking, enhanced visual impact with refined details and textures"
        lighting_block = "[LIGHTING & COLOR]: Dramatic cinematic lighting, rich color grading, depth through light and shadow layers, volumetric atmosphere"
        tech_block = "[CAMERA & TECH]: 35mm cinematic lens, 8K ultra high resolution, shallow depth of field, natural bokeh, film grain texture"

        prompt = f"""PROFESSIONAL CINEMATIC FILM STILL - TEXT-TO-IMAGE GENERATION

{subject_block}
{action_block}
{environment_block}
{style_block}
{lighting_block}
{tech_block}
{cinematography_block}

COMPOSITION RULES:
- Full-bleed edge-to-edge rendering filling 100% of the 16:9 canvas
- ZERO borders, margins, or white space - render as if captured from cinema camera sensor
- Subject photographed as cinematic scene, NOT shrunk into centered box
- Professional cinematography with rule of thirds and depth of field
- ALL cinematography constraints above MUST be strictly followed

QUALITY ENHANCEMENT:
- More visually impactful than standard output
- Rich detail textures and refined material quality
- Dramatic light/shadow interplay for depth
- Cinematic color palette with professional grading

FORBIDDEN:
- Any white/black borders or margins
- Changing shot scale, subject position, orientation, or gaze from source
- Poster layouts, magazine compositions, or storyboard aesthetics
- Any graphic design elements unless explicitly in style prompt

--ar 16:9"""

    print(f"ğŸ¨ AI æ­£åœ¨ç”Ÿæˆå®šå¦†å›¾: {shot['shot_id']}")

    try:
        # ä½¿ç”¨ Gemini 3 Pro Image Preview (ä¸ä¸‰è§†å›¾ç”Ÿæˆä¸€è‡´)
        print(f"ğŸ“¡ è°ƒç”¨ Gemini 3 Pro Image (gemini-3-pro-image-preview)...")
        response = client.models.generate_images(
            model="gemini-3-pro-image-preview",
            prompt=prompt,
            config=types.GenerateImagesConfig(
                number_of_images=1,
                aspect_ratio="16:9"
            )
        )
        if response.generated_images:
            gen_img = response.generated_images[0]
            if hasattr(gen_img.image, 'save'):
                gen_img.image.save(dst)
            else:
                with open(dst, 'wb') as f: f.write(gen_img.image.image_bytes)
            print(f"âœ… Gemini 3 Pro Image ç”ŸæˆæˆåŠŸï¼")
            return f"stylized_frames/{dst.name}"
    except Exception as e:
        print(f"âŒ Gemini 3 Pro Image è°ƒç”¨å¤±è´¥: {str(e)[:100]}...")

    print("âš ï¸ æ‰§è¡ŒåŸå›¾å ä½ã€‚")
    shutil.copyfile(src, dst)
    return f"stylized_frames/{dst.name}"


def mock_generate_video(job_dir: Path, shot: dict) -> str:
    videos_dir = ensure_videos_dir(job_dir)
    out_path = videos_dir / f"{shot['shot_id']}.mp4"
    if out_path.exists(): os.remove(out_path)
    src_video = job_dir / "input.mp4"
    ffmpeg = get_ffmpeg_path()
    cmd = [ffmpeg, "-y", "-i", str(src_video), "-t", "1.0", "-c", "copy", str(out_path)]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return f"videos/{out_path.name}"


def veo_generate_video(job_dir: Path, wf: dict, shot: dict) -> str:
    from google import genai
    from google.genai import types

    api_key = os.getenv("GEMINI_API_KEY")
    # ä½¿ç”¨ä¸ video_generator.py ç›¸åŒçš„å®¢æˆ·ç«¯åˆå§‹åŒ–æ–¹å¼
    client = genai.Client(api_key=api_key)

    videos_dir = ensure_videos_dir(job_dir)
    out_path = videos_dir / f"{shot['shot_id']}.mp4"
    if out_path.exists(): os.remove(out_path)

    img_rel = shot.get("assets", {}).get("stylized_frame") or f"stylized_frames/{shot['shot_id']}.png"
    img_path = job_dir / img_rel

    if not img_path.exists():
        ai_stylize_frame(job_dir, wf, shot)

    print(f"ğŸš€ [Veo 3.1] æ­£åœ¨æ¸²æŸ“åˆ†é•œè§†é¢‘: {shot['shot_id']}")

    image_bytes = img_path.read_bytes()
    style = wf.get('global', {}).get('style_prompt', '')

    # ğŸ¬ è·å–æœ‰æ•ˆæ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨ Remix æ•°æ®ï¼‰
    description, cinema = get_effective_shot_data(job_dir, wf, shot)

    # ğŸ¬ Extract cinematography parameters for video fidelity
    shot_scale = cinema.get("shot_scale", "")
    subject_position = cinema.get("subject_frame_position", "")
    subject_orientation = cinema.get("subject_orientation", "")
    gaze_direction = cinema.get("gaze_direction", "")
    motion_vector = cinema.get("motion_vector", "static")

    # Build video-specific cinematography constraints
    video_constraints = []
    if shot_scale:
        video_constraints.append(f"Maintain {shot_scale} framing throughout")
    if subject_position:
        video_constraints.append(f"Subject stays at {subject_position} of frame")
    if subject_orientation:
        video_constraints.append(f"Subject maintains {subject_orientation} body angle")
    if gaze_direction:
        video_constraints.append(f"Gaze direction: {gaze_direction}")

    constraints_str = ". ".join(video_constraints) if video_constraints else ""

    # ğŸ¬ Structured Image-to-Video Prompt Format
    # Format: [Camera Movement], [Specific Action], [Physics Details], [Atmosphere Change]

    # Determine camera movement based on motion vector
    if motion_vector and motion_vector != "static":
        if "walking" in motion_vector or "running" in motion_vector:
            camera_movement = "subtle tracking shot following subject movement"
        elif "toward" in motion_vector:
            camera_movement = "gentle dolly back as subject approaches"
        elif "away" in motion_vector:
            camera_movement = "slow push in as subject recedes"
        else:
            camera_movement = "steady shot with minimal camera drift"
        specific_action = f"Subject performs: {motion_vector}"
    else:
        camera_movement = "locked static shot with subtle breathing movement"
        specific_action = "Subject maintains pose with natural micro-movements (breathing, blinking, subtle weight shifts)"

    # Physics details for realism
    physics_details = "natural physics: hair/fabric responds to movement, ambient particles float in light beams, subtle environmental motion (leaves, dust, reflections)"

    # Atmosphere continuity
    atmosphere_change = f"maintain {style} atmosphere throughout, consistent lighting evolution, seamless style continuity"

    prompt = f"""PROFESSIONAL IMAGE-TO-VIDEO GENERATION - 3-5 SECOND CINEMATIC CLIP

[CAMERA MOVEMENT]: {camera_movement}
[SPECIFIC ACTION]: {specific_action}
[PHYSICS DETAILS]: {physics_details}
[ATMOSPHERE]: {atmosphere_change}

SCENE CONTEXT: {description}
ART STYLE: {style} - Maintain CONSISTENT style across ALL frames

ğŸ¬ CINEMATOGRAPHY LOCK (from source shot - DO NOT CHANGE):
{constraints_str}

MOTION QUALITY REQUIREMENTS:
- High motion quality, cinematic fluidity
- Smooth interpolation between frames
- Subject position and composition MUST remain STABLE
- No sudden flips, mirror effects, or jarring camera changes
- Preserve exact shot scale and framing from reference image

PHYSICS ENHANCEMENT:
- Realistic material physics (cloth flow, hair dynamics)
- Environmental interaction (wind effects, light particles)
- Natural motion blur on moving elements
- Atmospheric depth continuity

CRITICAL: Cinematography parameters are LOCKED - preserve exactly as specified.
high motion quality, cinematic, professional cinematography"""

    # ğŸ”„ è‡ªæ„ˆå¼é‡è¯•é€»è¾‘ï¼šé‡åˆ° 429 é”™è¯¯æ—¶è‡ªåŠ¨ç­‰å¾…å¹¶é‡è¯•
    max_retries = 3
    retry_wait_seconds = 60

    for attempt in range(max_retries):
        try:
            # image ä½œä¸ºç‹¬ç«‹å‚æ•°ä¼ é€’ï¼Œä¸åœ¨ config å†…
            operation = client.models.generate_videos(
                model="veo-3.1-generate-preview",
                prompt=prompt,
                image=types.Image(
                    image_bytes=image_bytes,
                    mime_type="image/png"
                ),
                config=types.GenerateVideosConfig(
                    aspect_ratio="16:9"
                )
            )

            print(f"â³ è§†é¢‘æ­£åœ¨äº‘ç«¯æ¸²æŸ“ (Operation ID: {operation.name})")

            poll_count = 0
            max_polls = 60  # 20 minutes max
            while not operation.done:
                poll_count += 1
                if poll_count > max_polls:
                    raise RuntimeError(f"Veo è½®è¯¢è¶…æ—¶: å·²ç­‰å¾…è¶…è¿‡ 20 åˆ†é’Ÿ")
                print(f"â³ è§†é¢‘æ¸²æŸ“ä¸­... (è½®è¯¢ {poll_count})")
                time.sleep(20)
                operation = client.operations.get(operation)

            # æ£€æŸ¥é”™è¯¯
            if operation.error:
                raise RuntimeError(f"Veo åç«¯æŠ¥é”™: {operation.error}")

            # æ£€æŸ¥ç»“æœ
            if not operation.result or not operation.result.generated_videos:
                raise RuntimeError("Veo ä»»åŠ¡å®Œæˆä½†æœªè¿”å›è§†é¢‘æ•°æ®ã€‚åŸå› ï¼šå¯èƒ½è§¦å‘äº†å†…å®¹å®‰å…¨å®¡æ ¸æ‹¦æˆªã€‚")

            generated_video = operation.result.generated_videos[0]

            # ä¼˜å…ˆä½¿ç”¨ SDK åŸç”Ÿ save æ–¹æ³•
            try:
                generated_video.video.save(str(out_path))
                print(f"ğŸ’¾ è§†é¢‘ç”ŸæˆæˆåŠŸ (SDK save): {out_path}")
                return f"videos/{out_path.name}"
            except Exception as save_err:
                print(f"âš ï¸ SDK save å¤±è´¥ ({save_err})ï¼Œå°è¯•æ‰‹åŠ¨ä¸‹è½½...")

            # å¤‡ç”¨ï¼šæ‰‹åŠ¨ä¸‹è½½
            file_id = None
            video_obj = generated_video.video if hasattr(generated_video, 'video') else generated_video

            if hasattr(video_obj, 'name') and video_obj.name:
                file_id = video_obj.name if "/" in video_obj.name else f"files/{video_obj.name}"
            elif hasattr(video_obj, 'uri') and video_obj.uri:
                file_id = f"files/{video_obj.uri.split('/')[-1]}"

            if not file_id:
                raise RuntimeError(f"æ— æ³•ä»å“åº”ä¸­è§£ææœ‰æ•ˆçš„ File ID: {type(video_obj).__name__}")

            # é˜²å¾¡æ€§ä¿®å¤ï¼šfile_id å¯èƒ½è‡ªå¸¦ ?alt=media æˆ– ?key=...
            clean_file_id = file_id.split("?", 1)[0]

            print(f"âœ… ç”ŸæˆæˆåŠŸï¼Œæ­£åœ¨ä¸‹è½½æ–‡ä»¶: {clean_file_id}")

            download_url = f"https://generativelanguage.googleapis.com/v1beta/{clean_file_id}"
            query_params = {
                "alt": "media",
                "key": api_key,
            }

            response = requests.get(
                download_url,
                params=query_params,
                stream=True,
            )

            if response.status_code == 200:
                with open(out_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=1024*1024): f.write(chunk)
                print(f"ğŸ’¾ è§†é¢‘ç”ŸæˆæˆåŠŸ (æ‰‹åŠ¨ä¸‹è½½): {out_path}")
                return f"videos/{out_path.name}"
            else:
                raise RuntimeError(f"ä¸‹è½½å¤±è´¥: çŠ¶æ€ç  {response.status_code}")

        except Exception as e:
            error_str = str(e).lower()
            is_rate_limit = "429" in error_str or "rate" in error_str or "quota" in error_str or "resource_exhausted" in error_str

            if is_rate_limit and attempt < max_retries - 1:
                wait_time = retry_wait_seconds * (attempt + 1)  # é€’å¢ç­‰å¾…æ—¶é—´
                print(f"âš ï¸ è§¦å‘ RPM é™åˆ¶ (429)ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ({attempt + 1}/{max_retries})...")
                time.sleep(wait_time)
                continue
            else:
                print(f"âŒ Veo å¤±è´¥: {str(e)}")
                raise e

    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    raise RuntimeError(f"Veo ç”Ÿæˆå¤±è´¥ï¼šå·²é‡è¯• {max_retries} æ¬¡")


def run_stylize(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    shots_to_process = []
    for shot in wf.get("shots", []):
        sid = shot.get("shot_id")
        if target_shot and sid != target_shot: continue
        status = shot.get("status", {}).get("stylize", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"): continue
        shots_to_process.append(shot)

    for idx, shot in enumerate(shots_to_process):
        sid = shot.get("shot_id")

        # ğŸš¦ RPM é™æµï¼šæ‰¹é‡æ‰§è¡Œæ—¶ï¼Œæ¯ä¸ªåˆ†é•œä¹‹é—´ä¼‘çœ  35 ç§’
        if idx > 0 and target_shot is None:
            print(f"â³ RPM é™æµï¼šç­‰å¾… 35 ç§’åå¤„ç†ä¸‹ä¸€ä¸ªåˆ†é•œ...")
            time.sleep(35)

        shot.setdefault("status", {})["stylize"] = "RUNNING"
        save_workflow(job_dir, wf)
        try:
            rel_path = ai_stylize_frame(job_dir, wf, shot)
            shot.setdefault("assets", {})["stylized_frame"] = rel_path
            shot["status"]["stylize"] = "SUCCESS"
            print(f"âœ… Stylize SUCCESS: {sid}")
        except Exception as e:
            shot["status"]["stylize"] = "FAILED"
            shot.setdefault("errors", {})["stylize"] = str(e)
        save_workflow(job_dir, wf)


def run_video_generate(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    shots_to_process = []
    for shot in wf.get("shots", []):
        sid = shot.get("shot_id")
        if target_shot and sid != target_shot: continue
        status = shot.get("status", {}).get("video_generate", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"): continue
        shots_to_process.append(shot)

    for idx, shot in enumerate(shots_to_process):
        sid = shot.get("shot_id")

        # ğŸš¦ RPM é™æµï¼šæ‰¹é‡æ‰§è¡Œæ—¶ï¼Œæ¯ä¸ªåˆ†é•œä¹‹é—´ä¼‘çœ  35 ç§’
        if idx > 0 and target_shot is None:
            print(f"â³ RPM é™æµï¼šç­‰å¾… 35 ç§’åå¤„ç†ä¸‹ä¸€ä¸ªåˆ†é•œ...")
            time.sleep(35)

        shot.setdefault("status", {})["video_generate"] = "RUNNING"
        save_workflow(job_dir, wf)
        try:
            video_model = wf.get("global", {}).get("video_model", "mock")
            if video_model == "veo":
                rel_video_path = veo_generate_video(job_dir, wf, shot)
            else:
                rel_video_path = mock_generate_video(job_dir, shot)
            shot.setdefault("assets", {})["video"] = rel_video_path
            shot["status"]["video_generate"] = "SUCCESS"
            print(f"âœ… Video SUCCESS: {sid}")
        except Exception as e:
            shot["status"]["video_generate"] = "FAILED"
            shot.setdefault("errors", {})["video_generate"] = str(e)
            print(f"âŒ Video FAILED: {sid} -> {e}")
        save_workflow(job_dir, wf)


def run_pipeline(job_dir: Path, target_shot: str | None = None) -> None:
    wf = load_workflow(job_dir)
    run_stylize(job_dir, wf, target_shot=target_shot)
    wf = load_workflow(job_dir)
    run_video_generate(job_dir, wf, target_shot=target_shot)






