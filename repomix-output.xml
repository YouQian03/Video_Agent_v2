This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
core/
  __init__.py
  agent_engine.py
  changes.py
  runner.py
  utils.py
  workflow_io.py
  workflow_manager.py
.gitignore
agent_demo.py
analyze_video.py
app.py
apply_changes.py
build_workflow.py
extract_frames.py
index.html
merge_workflow.py
nixpacks.toml
railway.json
requirements.txt
run_workflow.py
smoke_test_core.py
stylize_frames.py
test_gemini.py
test_v3.py
vibe_check.py
video_generator.py
workflow_cli.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(tree:*)"
    ]
  }
}
</file>

<file path="core/__init__.py">

</file>

<file path="core/changes.py">
from typing import Optional

def apply_global_style(wf: dict, new_style_prompt: str, cascade: bool = True) -> int:
    wf.setdefault("global", {})["style_prompt"] = new_style_prompt

    affected = 0
    if cascade:
        for shot in wf.get("shots", []):
            shot.setdefault("status", {})["stylize"] = "NOT_STARTED"
            shot.setdefault("status", {})["video_generate"] = "NOT_STARTED"
            affected += 1
    return affected

def replace_entity_reference(wf: dict, entity_id: str, new_ref_image: str) -> int:
    entities = wf.setdefault("entities", {})
    if entity_id not in entities:
        raise KeyError(f"entity ä¸å­˜åœ¨ï¼š{entity_id}")

    entities[entity_id]["reference_image"] = new_ref_image

    affected = 0
    for shot in wf.get("shots", []):
        shot_entities = shot.get("entities", [])
        if entity_id in shot_entities:
            shot.setdefault("status", {})["stylize"] = "NOT_STARTED"
            shot.setdefault("status", {})["video_generate"] = "NOT_STARTED"
            affected += 1
    return affected
</file>

<file path="core/utils.py">
# core/utils.py
import shutil

def get_ffmpeg_path() -> str:
    """
    è·¨å¹³å°è·å– ffmpeg è·¯å¾„
    - Railway/Linux: ä½¿ç”¨ PATH ä¸­çš„ ffmpeg
    - macOS æœ¬åœ°: ä½¿ç”¨ Homebrew è·¯å¾„
    """
    # ä¼˜å…ˆä» PATH æŸ¥æ‰¾
    ffmpeg = shutil.which("ffmpeg")
    if ffmpeg:
        return ffmpeg

    # macOS Homebrew å¤‡ç”¨è·¯å¾„
    macos_path = "/opt/homebrew/bin/ffmpeg"
    import os
    if os.path.exists(macos_path):
        return macos_path

    # Intel Mac å¤‡ç”¨è·¯å¾„
    intel_mac_path = "/usr/local/bin/ffmpeg"
    if os.path.exists(intel_mac_path):
        return intel_mac_path

    raise RuntimeError("ffmpeg not found. Please install ffmpeg.")
</file>

<file path="core/workflow_io.py">
import json
from pathlib import Path

def load_workflow(job_dir: Path) -> dict:
    wf_path = job_dir / "workflow.json"
    return json.loads(wf_path.read_text(encoding="utf-8"))

def save_workflow(job_dir: Path, wf: dict) -> None:
    wf_path = job_dir / "workflow.json"
    wf_path.write_text(
        json.dumps(wf, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
</file>

<file path=".gitignore">
# å¿½ç•¥ Python è™šæ‹Ÿç¯å¢ƒ
venv/
.venv/
__pycache__/
*.pyc

# å¿½ç•¥æ•æ„Ÿä¿¡æ¯
.env

# å¿½ç•¥ç”Ÿæˆçš„å¤§å‹èµ„æºæ–‡ä»¶ï¼ˆåªä¼ ä»£ç ï¼Œä¸ä¼ ç´ æï¼‰
jobs/
downloads/
outputs/
frames/
stylized_frames/
all_code.txt
repomix-output.xml
</file>

<file path="agent_demo.py">
# agent_demo.py
import os
from core.workflow_manager import WorkflowManager
from core.agent_engine import AgentEngine

def main():
    # 1. åˆå§‹åŒ–
    manager = WorkflowManager("demo_job_001")
    agent = AgentEngine()
    
    print("--- AI çˆ†æ¬¾äºŒåˆ› Agent é©±åŠ¨æ¨¡å¼ ---")
    print("å½“å‰é£æ ¼:", manager.workflow.get("global", {}).get("style_prompt"))
    print("å½“å‰å®ä½“:", list(manager.workflow.get("entities", {}).keys()))
    print("-" * 30)
    
    while True:
        user_text = input("\nğŸ¤– æ‚¨æƒ³å¯¹è§†é¢‘åšä½•ä¿®æ”¹ï¼Ÿ(è¾“å…¥ 'exit' é€€å‡º): ")
        if user_text.lower() == 'exit':
            break
            
        # å‡†å¤‡å·¥ä½œæµæ‘˜è¦ï¼ˆå‘Šè¯‰ Gemini å½“å‰æœ‰ä»€ä¹ˆï¼Œå®ƒæ‰èƒ½æ”¹ï¼‰
        summary = f"Style: {manager.workflow.get('global', {}).get('style_prompt')}\n"
        summary += f"Entities: {json.dumps(manager.workflow.get('entities', {}), indent=2)}"
        
        print("ğŸ” Agent æ­£åœ¨æ€è€ƒ...")
        action = agent.get_action_from_text(user_text, summary)
        
        print(f"ğŸ¯ è§£ææŒ‡ä»¤: {action}")
        
        if action.get("op") != "none" and action.get("op") != "error":
            # æ‰§è¡Œä¿®æ”¹
            res = manager.apply_agent_action(action)
            print(f"âœ… æ‰§è¡ŒæˆåŠŸï¼å—å½±å“åˆ†é•œæ•°: {res['affected_shots']}")
            print(f"ğŸ”„ æ‰€æœ‰å—å½±å“çš„åˆ†é•œçŠ¶æ€å·²é‡ç½®ï¼Œå‡†å¤‡é‡æ–°ç”Ÿæˆã€‚")
        else:
            print(f"âš ï¸ æ— æ³•æ‰§è¡Œ: {action.get('reason')}")

if __name__ == "__main__":
    import json
    main()
</file>

<file path="apply_changes.py">
import json
import argparse
from pathlib import Path

PROJECT_DIR = Path(__file__).parent
DEFAULT_JOB_ID = "demo_job_001"

def load_workflow(job_dir: Path) -> dict:
    return json.loads((job_dir / "workflow.json").read_text(encoding="utf-8"))

def save_workflow(job_dir: Path, wf: dict) -> None:
    (job_dir / "workflow.json").write_text(json.dumps(wf, ensure_ascii=False, indent=2), encoding="utf-8")

def apply_global_style(wf: dict, new_style_prompt: str, cascade: bool = True) -> int:
    """
    ä¿®æ”¹å…¨å±€é£æ ¼ï¼Œå¹¶çº§è”ä½¿ç›¸å…³èŠ‚ç‚¹éœ€è¦é‡è·‘ã€‚
    cascade=Trueï¼šæŠŠæ‰€æœ‰ shots çš„ stylize & video_generate é‡ç½®ä¸º NOT_STARTED
    è¿”å›ï¼šå—å½±å“ shots æ•°é‡
    """
    wf.setdefault("global", {})["style_prompt"] = new_style_prompt

    affected = 0
    if cascade:
        for shot in wf.get("shots", []):
            # é£æ ¼æ”¹äº†ï¼Œé£æ ¼åŒ–å›¾å°±åº”è¯¥é‡æ–°ç”Ÿæˆï¼ˆçœŸå®äº§å“é‡Œå¯èƒ½æœ‰ç¼“å­˜ç­–ç•¥ï¼Œdemo å…ˆå…¨é‡è·‘ï¼‰
            shot.setdefault("status", {})["stylize"] = "NOT_STARTED"
            shot.setdefault("status", {})["video_generate"] = "NOT_STARTED"
            affected += 1
    return affected

def replace_entity_reference(wf: dict, entity_id: str, new_ref_image: str) -> int:
    """
    æ›¿æ¢æŸä¸ª entity çš„ reference_imageï¼Œå¹¶åªå½±å“å¼•ç”¨å®ƒçš„ shotsï¼š
    - æ ‡è®° stylize / video_generate ä¸º NOT_STARTED
    è¿”å›ï¼šå—å½±å“ shots æ•°é‡
    """
    entities = wf.setdefault("entities", {})
    if entity_id not in entities:
        raise KeyError(f"entity ä¸å­˜åœ¨ï¼š{entity_id}")

    entities[entity_id]["reference_image"] = new_ref_image

    affected = 0
    for shot in wf.get("shots", []):
        shot_entities = shot.get("entities", [])
        if entity_id in shot_entities:
            shot.setdefault("status", {})["stylize"] = "NOT_STARTED"
            shot.setdefault("status", {})["video_generate"] = "NOT_STARTED"
            affected += 1
    return affected


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--job_id", default=DEFAULT_JOB_ID)
    parser.add_argument("--set_global_style", default=None, help="è®¾ç½®æ–°çš„ global.style_prompt")
    parser.add_argument("--no_cascade", action="store_true", help="ä¸è§¦å‘çº§è”é‡è·‘ï¼ˆé»˜è®¤ä¼šè§¦å‘ï¼‰")
    parser.add_argument("--replace_entity", default=None, help="è¦æ›¿æ¢çš„ entity_idï¼Œä¾‹å¦‚ entity_1")
    parser.add_argument("--new_ref", default=None, help="æ–°çš„ reference_image è·¯å¾„ï¼Œä¾‹å¦‚ stylized_frames/shot_02.png")

    args = parser.parse_args()

    job_dir = PROJECT_DIR / "jobs" / args.job_id
    wf = load_workflow(job_dir)

    if args.set_global_style is not None:
        affected = apply_global_style(wf, args.set_global_style, cascade=(not args.no_cascade))
        save_workflow(job_dir, wf)
        print(f"âœ… å·²æ›´æ–° global.style_prompt")
        print(f"âœ… å—å½±å“ shotsï¼š{affected}ï¼ˆstylize/video_generate å·²æ ‡è®°ä¸º NOT_STARTEDï¼‰")
    else:
        print("æ²¡æœ‰æŒ‡å®šä»»ä½•ä¿®æ”¹å‚æ•°ã€‚ç¤ºä¾‹ï¼š")
        print('  python apply_changes.py --set_global_style "cinematic noir, high contrast"')
    
    if args.replace_entity and args.new_ref:
        affected = replace_entity_reference(wf, args.replace_entity, args.new_ref)
        save_workflow(job_dir, wf)
        print(f"âœ… å·²æ›¿æ¢ {args.replace_entity} çš„ reference_image -> {args.new_ref}")
        print(f"âœ… å—å½±å“ shotsï¼š{affected}ï¼ˆstylize/video_generate å·²æ ‡è®°ä¸º NOT_STARTEDï¼‰")
        return

if __name__ == "__main__":
    main()
</file>

<file path="build_workflow.py">
import json
from pathlib import Path

PROJECT_DIR = Path(__file__).parent
JOB_DIR = PROJECT_DIR / "jobs" / "demo_job_001"

STORYBOARD_PATH = JOB_DIR / "storyboard.json"
FRAMES_DIR = JOB_DIR / "frames"
STYLIZED_DIR = JOB_DIR / "stylized_frames"
WORKFLOW_PATH = JOB_DIR / "workflow.json"

def to_seconds(t):
    if t is None:
        return None
    if isinstance(t, (int, float)):
        return float(t)
    s = str(t).strip()
    if not s:
        return None
    try:
        return float(s)
    except ValueError:
        pass
    parts = s.split(":")
    try:
        parts = [float(p) for p in parts]
    except ValueError:
        return None
    if len(parts) == 3:
        hh, mm, ss = parts
        return hh * 3600 + mm * 60 + ss
    if len(parts) == 2:
        mm, ss = parts
        return mm * 60 + ss
    if len(parts) == 1:
        return parts[0]
    return None

def main():
    storyboard = json.loads(STORYBOARD_PATH.read_text(encoding="utf-8"))

    shots = []
    for s in storyboard:
        shot_number = s.get("shot_number")
        if shot_number is None:
            continue
        sid = f"shot_{int(shot_number):02d}"

        start = to_seconds(s.get("start_time"))
        end = to_seconds(s.get("end_time"))
        desc = s.get("frame_description") or s.get("content_analysis") or ""

        frame_path = f"frames/{sid}.png"
        stylized_path = f"stylized_frames/{sid}.png"

        shots.append({
            "shot_id": sid,
            "start_time": start,
            "end_time": end,
            "description": desc,
            "voiceover": s.get("voiceover"),
            "assets": {
                "first_frame": frame_path if (JOB_DIR / frame_path).exists() else None,
                "stylized_frame": stylized_path if (JOB_DIR / stylized_path).exists() else None,
                "video": None
            },
            "status": {
                "analyze": "SUCCESS",
                "extract_frames": "SUCCESS",
                "stylize": "SUCCESS",
                "video_generate": "NOT_STARTED"
            }
        })

    workflow = {
        "job_id": "demo_job_001",
        "source_video": "input.mp4",
        "global": {
            "aspect_ratio": "16:9",
            "style_prompt": "de-replication stylization"
        },
        "entities": {},  # å…ˆç•™ç©ºï¼Œåç»­æˆ‘ä»¬åŠ â€œäººç‰©/èµ„äº§å…¨å±€æ›¿æ¢â€
        "shots": shots
    }

    WORKFLOW_PATH.write_text(json.dumps(workflow, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"âœ… workflow.json å·²ç”Ÿæˆï¼š{WORKFLOW_PATH}")
    print(f"shots æ•°é‡ï¼š{len(shots)}")

if __name__ == "__main__":
    main()
</file>

<file path="merge_workflow.py">
# merge_workflow.py
import argparse
import sys
from pathlib import Path
from core.workflow_manager import WorkflowManager

def main():
    # 1. è®¾ç½®å‘½ä»¤è¡Œå‚æ•°ï¼Œæ–¹ä¾¿ä½ æŒ‡å®šè¦åˆå¹¶å“ªä¸ª Job
    parser = argparse.ArgumentParser(description="ä¸€é”®åˆå¹¶åˆ†é•œè§†é¢‘ä¸ºæœ€ç»ˆæˆç‰‡")
    parser.add_argument("--job_id", required=True, help="è¯·è¾“å…¥ Job ID (ä¾‹å¦‚: job_6db68d0c)")
    args = parser.parse_args()

    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨åˆå¹¶ç¨‹åºï¼Œç›®æ ‡ Job: {args.job_id}")

    try:
        # 2. åˆå§‹åŒ–ç®¡ç†å™¨å¹¶å®šä½åˆ°è¯¥ Job
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¼ å…¥äº† project_rootï¼Œç¡®ä¿è·¯å¾„ç»å¯¹æ­£ç¡®
        project_root = Path(__file__).parent
        manager = WorkflowManager(args.job_id, project_root=project_root)
        
        if not manager.workflow:
            print(f"âŒ æ‰¾ä¸åˆ°è¯¥ Job çš„æ•°æ®ï¼Œè¯·æ£€æŸ¥ jobs/{args.job_id} æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ã€‚")
            return

        # 3. è°ƒç”¨åˆšæ‰åœ¨ core é‡Œå†™å¥½çš„åˆå¹¶é€»è¾‘
        result_file = manager.merge_videos()
        
        print("\n" + "="*30)
        print(f"âœ… åˆå¹¶æˆåŠŸï¼")
        print(f"ğŸ“ æˆç‰‡è·¯å¾„: jobs/{args.job_id}/{result_file}")
        print("="*30)

    except Exception as e:
        print(f"\nâŒ åˆå¹¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        # æ‰“å°è¯¦ç»†æŠ¥é”™æ–¹ä¾¿æˆ‘ä»¬æ’é›·
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
</file>

<file path="railway.json">
{
    "$schema": "https://railway.app/railway.schema.json",
    "build": {
      "builder": "NIXPACKS"
    },
    "deploy": {
      "startCommand": "uvicorn app:app --host 0.0.0.0 --port $PORT",
      "restartPolicyType": "ON_FAILURE",
      "restartPolicyMaxRetries": 10
    }
  }
</file>

<file path="requirements.txt">
fastapi
uvicorn[standard]
python-multipart
pillow
requests
google-genai
</file>

<file path="smoke_test_core.py">
from pathlib import Path
from core.workflow_io import load_workflow, save_workflow
from core.changes import replace_entity_reference
from core.runner import run_pipeline

JOB_DIR = Path("jobs/demo_job_001")

def main():
    wf = load_workflow(JOB_DIR)
    print("âœ… load_workflow ok, shots =", len(wf.get("shots", [])))

    # åšä¸€æ¬¡æ— å®³çš„ entity reference æ›¿æ¢ï¼ˆæ¢æˆè‡ªå·±å·²æœ‰çš„æ–‡ä»¶ï¼‰
    if "entity_1" in wf.get("entities", {}):
        replace_entity_reference(wf, "entity_1", "stylized_frames/shot_03.png")
        save_workflow(JOB_DIR, wf)
        print("âœ… replace_entity_reference ok")

    # è·‘ pipelineï¼ˆä¼šæŒ‰ NOT_STARTED æ‰§è¡Œï¼‰
    run_pipeline(JOB_DIR)
    print("âœ… run_pipeline ok")

if __name__ == "__main__":
    main()
</file>

<file path="test_gemini.py">
import os
from google import genai

def main():
    # ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– API Key
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("æ²¡æœ‰æ£€æµ‹åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡")

    client = genai.Client(api_key=api_key)

    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents="ç”¨ä¸€å¥è¯å‘Šè¯‰æˆ‘ï¼Œä½ æ˜¯è°ï¼Ÿ"
    )

    print("Gemini å›å¤ï¼š")
    print(response.text)

if __name__ == "__main__":
    main()
</file>

<file path="test_v3.py">
# test_v3.py
from core.workflow_manager import WorkflowManager

manager = WorkflowManager("demo_job_001")

# æµ‹è¯•å½¢æ€ 3ï¼šæ‰‹åŠ¨å¾®è°ƒç¬¬ 2 ä¸ªé•œå¤´çš„æè¿°
print("æµ‹è¯•å½¢æ€ 3ï¼šæ‰‹åŠ¨å¾®è°ƒåˆ†é•œ...")
manager.apply_agent_action({
    "op": "update_shot_params",
    "shot_id": "shot_02",
    "description": "ä¸€åªæ­£åœ¨æˆ´ç€å¢¨é•œè·³èˆçš„é…·ç‹—"
})

manager.load()
shot2 = [s for s in manager.workflow["shots"] if s["shot_id"] == "shot_02"][0]
print(f"åˆ†é•œ 2 æ–°æè¿°: {shot2['description']}")
print(f"åˆ†é•œ 2 çŠ¶æ€å·²é‡ç½®: {shot2['status']['video_generate']}") # åº”è¯¥æ˜¯ NOT_STARTED
print(f"å…¨å±€ Video Gen é˜¶æ®µçŠ¶æ€: {manager.workflow['global_stages']['video_gen']}")
</file>

<file path="vibe_check.py">
# vibe_check.py
from core.workflow_manager import WorkflowManager

# 1. åˆå§‹åŒ–ï¼ˆä½¿ç”¨ä½ å·²æœ‰çš„ demo_job_001ï¼‰
manager = WorkflowManager("demo_job_001")

# 2. æ¨¡æ‹Ÿå½¢æ€ 3ï¼šç›´æ¥æ”¹å…¨å±€é£æ ¼
print("æ­£åœ¨å°è¯•ä¿®æ”¹å…¨å±€é£æ ¼...")
res = manager.apply_agent_action({"op": "set_global_style", "value": "Cyberpunk Neon"})
print(f"å—å½±å“åˆ†é•œæ•°: {res['affected_shots']}")

# 3. éªŒè¯çŠ¶æ€æ˜¯å¦å˜å›äº† NOT_STARTED (è¿™æ˜¯æˆ‘ä»¬ changes.py é‡Œçš„çº§è”é€»è¾‘)
manager.load() # é‡æ–°åŠ è½½çœ‹ç£ç›˜ä¸Šçš„ç»“æœ
first_shot_status = manager.workflow["shots"][0]["status"]["video_generate"]
print(f"ä¿®æ”¹é£æ ¼åï¼ŒShot 01 çš„ç”ŸæˆçŠ¶æ€æ˜¯: {first_shot_status}")

if first_shot_status == "NOT_STARTED":
    print("âœ… åº•åº§é€»è¾‘éªŒè¯æˆåŠŸï¼")
else:
    print("âŒ çŠ¶æ€æ²¡æœ‰æ­£ç¡®é‡ç½®ï¼Œè¯·æ£€æŸ¥ core/changes.py")
</file>

<file path="video_generator.py">
import os
import time
from google import genai
from google.genai import types

# ç¡®ä¿ç¯å¢ƒå˜é‡å·²åŠ è½½
api_key = os.environ.get("GEMINI_API_KEY")
client = genai.Client(api_key=api_key)

def run_veo_generation(shot_id, prompt, image_path, output_dir="output_videos"):
    """
    é’ˆå¯¹ 2026 å¹´ Gemini 3 / Veo ç”Ÿæ€ä¼˜åŒ–çš„è§†é¢‘ç”Ÿæˆå‡½æ•°
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 1. ä»¥äºŒè¿›åˆ¶è¯»å–é£æ ¼åŒ–åçš„å‚è€ƒå›¾
    try:
        with open(image_path, 'rb') as f:
            image_bytes = f.read()
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶: {image_path}")
        return None

    print(f"ğŸš€ å¯åŠ¨ Veo 3.1 ä»»åŠ¡ | åˆ†é•œ: {shot_id}")
    
    try:
        # 2. è°ƒç”¨ä¸“é—¨çš„ generate_videos æ¥å£
        # ä¿®å¤ç‚¹ï¼šå¿…é¡»ä½¿ç”¨ generate_videos è€Œé generate_content
        operation = client.models.generate_videos(
            model="veo-3.1-generate-preview",
            prompt=prompt,
            config=types.GenerateVideosConfig(
                # ä¿®å¤ç‚¹ï¼šå‚è€ƒå›¾å¿…é¡»æ”¾åœ¨è¿™ä¸ª image å­—æ®µé‡Œ
                image=types.Part.from_bytes(
                    data=image_bytes,
                    mime_type="image/png"
                ),
                aspect_ratio="16:9"
            )
        )

        # 3. å¼‚æ­¥è½®è¯¢ (Veo è§†é¢‘ç”Ÿæˆä¸æ˜¯å³æ—¶çš„)
        print(f"â³ è§†é¢‘æ­£åœ¨äº‘ç«¯æ¸²æŸ“ (Operation ID: {operation.name})")
        while not operation.done:
            print(".", end="", flush=True)
            time.sleep(10)  # æ¯ 10 ç§’æŸ¥è¯¢ä¸€æ¬¡è¿›åº¦
            operation = client.operations.get(operation.name)

        # 4. æ£€æŸ¥ç»“æœå¹¶ä¿å­˜
        if operation.result and operation.result.generated_videos:
            generated_video = operation.result.generated_videos[0]
            output_path = os.path.join(output_dir, f"{shot_id}.mp4")
            
            # ä½¿ç”¨ SDK åŸç”Ÿ save æ–¹æ³•
            generated_video.video.save(output_path)
            print(f"\nâœ… è§†é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
            return output_path
        else:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥ï¼ŒåŸå› : {operation.error}")
            return None

    except Exception as e:
        print(f"\nâŒ è°ƒç”¨ Veo API å‡ºç°ä¸¥é‡å¼‚å¸¸: {str(e)}")
        return None

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç  (ä½ å¯ä»¥ç›´æ¥è¿è¡Œ python video_generator.py éªŒè¯)
    test_prompt = "A cinematic drone shot of a neon cyberpunk city in the rain."
    test_image = "stylized_frames/shot_01.png"
    run_veo_generation("shot_01", test_prompt, test_image)
</file>

<file path="workflow_cli.py">
import argparse
from pathlib import Path

from core.workflow_io import load_workflow, save_workflow
from core.changes import apply_global_style, replace_entity_reference
from core.runner import run_pipeline

DEFAULT_JOB_ID = "demo_job_001"
PROJECT_DIR = Path(__file__).parent

def job_dir_from_id(job_id: str) -> Path:
    return PROJECT_DIR / "jobs" / job_id

def cmd_list(job_dir: Path) -> None:
    wf = load_workflow(job_dir)
    print(f"job_id: {wf.get('job_id')}")
    print(f"global.style_prompt: {wf.get('global', {}).get('style_prompt')}")
    print("-" * 60)
    for s in wf.get("shots", []):
        sid = s.get("shot_id")
        st = s.get("status", {})
        print(f"{sid:7}  stylize={st.get('stylize')}  video={st.get('video_generate')}")

def cmd_set_style(job_dir: Path, style: str, cascade: bool) -> None:
    wf = load_workflow(job_dir)
    affected = apply_global_style(wf, style, cascade=cascade)
    save_workflow(job_dir, wf)
    print(f"âœ… style å·²æ›´æ–°ï¼š{style}")
    print(f"âœ… å—å½±å“ shotsï¼š{affected}ï¼ˆcascade={cascade}ï¼‰")

def cmd_replace_entity(job_dir: Path, entity_id: str, new_ref: str) -> None:
    wf = load_workflow(job_dir)
    affected = replace_entity_reference(wf, entity_id, new_ref)
    save_workflow(job_dir, wf)
    print(f"âœ… å·²æ›¿æ¢ {entity_id}.reference_image -> {new_ref}")
    print(f"âœ… å—å½±å“ shotsï¼š{affected}")

def cmd_run(job_dir: Path, shot: str | None) -> None:
    run_pipeline(job_dir, target_shot=shot)
    print("âœ… runner æ‰§è¡Œå®Œæˆ")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--job_id", default=DEFAULT_JOB_ID)

    sub = parser.add_subparsers(dest="cmd", required=True)

    p_list = sub.add_parser("list", help="åˆ—å‡º shots çŠ¶æ€")
    p_list.set_defaults(func="list")

    p_style = sub.add_parser("set-style", help="è®¾ç½®å…¨å±€ style_prompt")
    p_style.add_argument("style")
    p_style.add_argument("--no-cascade", action="store_true", help="ä¸çº§è”é‡è·‘ï¼ˆé»˜è®¤ä¼šçº§è”ï¼‰")
    p_style.set_defaults(func="set-style")

    p_ent = sub.add_parser("replace-entity", help="æ›¿æ¢ entity çš„ reference_imageï¼Œå¹¶æ ‡è®°å—å½±å“ shots")
    p_ent.add_argument("entity_id")
    p_ent.add_argument("new_ref")
    p_ent.set_defaults(func="replace-entity")

    p_run = sub.add_parser("run", help="è¿è¡Œ runnerï¼ˆæŒ‰ NOT_STARTED æ‰§è¡Œï¼‰")
    p_run.add_argument("--shot", default=None, help="åªè·‘æŸä¸ª shotï¼Œä¾‹å¦‚ shot_03")
    p_run.set_defaults(func="run")

    args = parser.parse_args()
    job_dir = job_dir_from_id(args.job_id)
    if not job_dir.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° job ç›®å½•ï¼š{job_dir}")

    if args.func == "list":
        cmd_list(job_dir)
    elif args.func == "set-style":
        cmd_set_style(job_dir, args.style, cascade=(not args.no_cascade))
    elif args.func == "replace-entity":
        cmd_replace_entity(job_dir, args.entity_id, args.new_ref)
    elif args.func == "run":
        cmd_run(job_dir, args.shot)

if __name__ == "__main__":
    main()
</file>

<file path="analyze_video.py">
import os
import json
import time
from pathlib import Path
from google import genai

PROJECT_DIR = Path(__file__).parent
VIDEO_PATH = PROJECT_DIR / "downloads" / "input.mp4"
OUT_PATH = PROJECT_DIR / "outputs" / "storyboard.json"

DIRECTOR_METAPROMPT = r"""
è¯·ä½ æ‰®æ¼”ä¸€ä½ä¸“ä¸šçš„å½±è§†åˆ†é•œåˆ†æå¸ˆï¼Œå°†è§†é¢‘æ‹†è§£ä¸ºè¯¦ç»†çš„åˆ†é•œè¡¨ã€‚

ğŸ“‹ ã€æ ¸å¿ƒè¦æ±‚ - è¯­ä¹‰åˆ†å±‚ã€‘
ä½ å¿…é¡»å°†æè¿°åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹å±‚æ¬¡ï¼š

ğŸ­ **å™äº‹å±‚ (Narrative Layer)** - frame_description å­—æ®µï¼š
- ä»…æè¿°è§†è§‰å†…å®¹ï¼šäººç‰©ã€ç‰©ä½“ã€èƒŒæ™¯ã€ç¯å¢ƒã€æƒ…èŠ‚
- âŒ ç¦æ­¢åŒ…å«ä»»ä½•æŠ€æœ¯æ€§é•œå¤´è¯­è¨€ï¼ˆå¦‚"é•œå¤´æ¨è¿›"ã€"camera pans"ï¼‰
- âœ… æ­£ç¡®ç¤ºä¾‹ï¼š"A woman stands on a desolate beach at sunset, waves crashing behind her"
- âŒ é”™è¯¯ç¤ºä¾‹ï¼š"The camera slowly pans to reveal a woman on a beach"ï¼ˆåŒ…å«æŠ€æœ¯æè¿°ï¼‰

ğŸ¥ **æŠ€æœ¯å±‚ (Technical Layer)** - ç‹¬ç«‹çš„å…ƒæ•°æ®å­—æ®µï¼š
- æ‰€æœ‰é•œå¤´æŠ€æœ¯ä¿¡æ¯å¿…é¡»æ”¾å…¥ä¸“ç”¨å­—æ®µï¼Œä¸å¾—æ··å…¥å™äº‹æè¿°

ğŸ“ ã€ç”»å¹…çº¦æŸã€‘
æ‰€æœ‰æè¿°åŸºäº 16:9 å®½å±ç”µå½±ç”»å¹…ã€‚

ğŸ¬ ã€å…³é”®ï¼šæ‘„å½±å‚æ•°ç²¾å‡†æå– - Cinematography Fidelityã€‘
ä½ å¿…é¡»ä»æºè§†é¢‘ç”»é¢ä¸­ç²¾ç¡®åˆ†æå¹¶æå–ä»¥ä¸‹å‚æ•°ï¼Œè¿™äº›å‚æ•°å°†è¢«ç¡¬ç¼–ç åˆ°ç”Ÿæˆæç¤ºä¸­ï¼š

1ï¸âƒ£ **æ™¯åˆ« (Shot Scale)** - shot_scale å­—æ®µï¼š
   å¿…é¡»ä¸¥æ ¼é€‰æ‹©ä»¥ä¸‹ä¹‹ä¸€ï¼š
   - "EXTREME_WIDE": å¤§è¿œæ™¯ï¼Œäººç‰©æå°ï¼Œç¯å¢ƒä¸ºä¸»
   - "WIDE": å…¨æ™¯ï¼Œå®Œæ•´äººä½“å¯è§ï¼Œç¯å¢ƒå æ¯”å¤§
   - "MEDIUM_WIDE": ä¸­å…¨æ™¯ï¼Œè†ç›–ä»¥ä¸Šå¯è§
   - "MEDIUM": ä¸­æ™¯ï¼Œè…°éƒ¨ä»¥ä¸Šå¯è§
   - "MEDIUM_CLOSE": ä¸­è¿‘æ™¯ï¼Œèƒ¸éƒ¨ä»¥ä¸Šå¯è§
   - "CLOSE_UP": ç‰¹å†™ï¼Œé¢éƒ¨ä¸ºä¸»
   - "EXTREME_CLOSE_UP": å¤§ç‰¹å†™ï¼Œçœ¼ç›/å˜´å”‡ç­‰å±€éƒ¨

2ï¸âƒ£ **ä¸»ä½“ç”»é¢åæ ‡ (Subject Frame Position)** - subject_frame_position å­—æ®µï¼š
   å¿…é¡»ç²¾ç¡®æè¿°ä¸»ä½“åœ¨16:9ç”»å¹…ä¸­çš„ä½ç½®ï¼ˆä½¿ç”¨ä¹å®«æ ¼åæ ‡ï¼‰ï¼š
   - "top-left", "top-center", "top-right"
   - "center-left", "center", "center-right"
   - "bottom-left", "bottom-center", "bottom-right"
   æˆ–æ›´ç²¾ç¡®çš„æè¿°å¦‚ "left-third-vertical-center", "right-third-lower"

3ï¸âƒ£ **ä¸»ä½“æœå‘ä¸è§†çº¿ (Orientation & Gaze)** - subject_orientation å’Œ gaze_direction å­—æ®µï¼š
   subject_orientationï¼ˆèº«ä½“æœå‘ï¼‰ï¼š
   - "facing-camera": æ­£é¢é¢å¯¹é•œå¤´
   - "back-to-camera": èƒŒå¯¹é•œå¤´
   - "profile-left": å·¦ä¾§é¢ï¼ˆé¢æœç”»é¢å·¦ä¾§ï¼‰
   - "profile-right": å³ä¾§é¢ï¼ˆé¢æœç”»é¢å³ä¾§ï¼‰
   - "three-quarter-left": 3/4ä¾§é¢æœå·¦
   - "three-quarter-right": 3/4ä¾§é¢æœå³

   gaze_directionï¼ˆè§†çº¿æ–¹å‘ï¼‰ï¼š
   - "looking-at-camera": ç›´è§†é•œå¤´
   - "looking-left": çœ‹å‘ç”»é¢å·¦ä¾§
   - "looking-right": çœ‹å‘ç”»é¢å³ä¾§
   - "looking-up": å‘ä¸Šçœ‹
   - "looking-down": å‘ä¸‹çœ‹
   - "looking-off-screen-left": çœ‹å‘ç”»å¤–å·¦ä¾§
   - "looking-off-screen-right": çœ‹å‘ç”»å¤–å³ä¾§

4ï¸âƒ£ **è¿åŠ¨çŸ¢é‡ (Motion Vector)** - motion_vector å­—æ®µï¼š
   ç²¾ç¡®æè¿°ä¸»ä½“çš„è¿åŠ¨æ–¹å‘å’ŒåŠ¨ä½œï¼š
   - "static": é™æ­¢ä¸åŠ¨
   - "walking-left": å‘ç”»é¢å·¦ä¾§è¡Œèµ°
   - "walking-right": å‘ç”»é¢å³ä¾§è¡Œèµ°
   - "walking-toward-camera": å‘é•œå¤´èµ°æ¥
   - "walking-away-from-camera": èƒŒå‘é•œå¤´èµ°å»
   - "running-left/right": å¥”è·‘æ–¹å‘
   - "turning-left/right": è½¬èº«æ–¹å‘
   - "gesturing-left/right": æ‰‹åŠ¿æ–¹å‘
   - å¤åˆåŠ¨ä½œå¦‚ "walking-right-while-looking-left"

ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚
æ ¹å…ƒç´ ä¸ºåŒ…å«å¤šä¸ªåˆ†é•œå¯¹è±¡çš„JSONæ•°ç»„ã€‚

æ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
=== å™äº‹å±‚å­—æ®µ ===
- shot_number: åˆ†é•œåºå·
- frame_description: çº¯è§†è§‰å™äº‹æè¿°ï¼ˆç¦æ­¢åŒ…å«é•œå¤´æŠ€æœ¯è¯æ±‡ï¼‰
- content_analysis: åœºæ™¯å†…å®¹ä¸æƒ…èŠ‚åˆ†æ

=== æ‘„å½±å‚æ•°å­—æ®µï¼ˆå¿…é¡»ç²¾ç¡®å¡«å†™ï¼‰===
- shot_scale: æ™¯åˆ«ï¼ˆå¿…å¡«ï¼Œä»ä¸Šè¿°7ä¸ªé€‰é¡¹ä¸­é€‰æ‹©ï¼‰
- subject_frame_position: ä¸»ä½“åœ¨ç”»é¢ä¸­çš„ç²¾ç¡®åæ ‡ä½ç½®
- subject_orientation: ä¸»ä½“èº«ä½“æœå‘
- gaze_direction: ä¸»ä½“è§†çº¿æ–¹å‘
- motion_vector: è¿åŠ¨çŸ¢é‡æè¿°

=== æŠ€æœ¯å±‚å­—æ®µï¼ˆå…ƒæ•°æ®æ ‡ç­¾ï¼‰===
- camera_type: é•œå¤´ç±»å‹ï¼Œå¦‚ "Static", "Dolly", "Pan", "Tilt", "Zoom", "Handheld", "Crane"
- camera_movement: é•œå¤´è¿åŠ¨æè¿°ï¼Œå¦‚ "Slow push in", "Pan left to right", "Static"

=== æ—¶é—´ä¸å…¶ä»–å­—æ®µ ===
- start_time, end_time, duration_seconds: æ—¶é—´ä¿¡æ¯
- shot_type, camera_angle: æ™¯åˆ«ä¸è§’åº¦
- focus_and_depth, lighting: å…¶ä»–ä¿¡æ¯

æ— ä¿¡æ¯è¯·å¡« nullã€‚
ä»…è¾“å‡ºçº¯ JSONã€‚
""".strip()


def ensure_api_key() -> str:
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError(
            "æ²¡æœ‰æ£€æµ‹åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚\n"
            "è¯·åœ¨å½“å‰ç»ˆç«¯æ‰§è¡Œï¼š\n"
            '  export GEMINI_API_KEY="ä½ çš„key"\n'
            "ç„¶åå†è¿è¡Œæœ¬è„šæœ¬ã€‚"
        )
    return api_key


def extract_json_array(text: str):
    import re
    if not text:
        raise ValueError("æ¨¡å‹æ²¡æœ‰è¿”å›æ–‡æœ¬ï¼ˆresponse.text ä¸ºç©ºï¼‰ã€‚")

    s = text.strip()

    # Find JSON array bounds
    l = s.find("[")
    r = s.rfind("]")
    if l == -1 or r == -1 or r <= l:
        raise ValueError(
            "æœªèƒ½ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSON æ•°ç»„ã€‚\n"
            "è¯·æŠŠæ¨¡å‹åŸå§‹è¾“å‡ºå¤åˆ¶å‡ºæ¥æ£€æŸ¥ï¼ˆå®ƒå¯èƒ½æ²¡æœ‰æŒ‰è¦æ±‚è¾“å‡º JSONï¼‰ã€‚"
        )

    json_str = s[l : r + 1]

    # ğŸ”§ Fix common JSON formatting issues from LLM output
    # 1. Remove trailing commas before ] or }
    json_str = re.sub(r',\s*]', ']', json_str)
    json_str = re.sub(r',\s*}', '}', json_str)

    # 2. Remove JavaScript-style comments
    json_str = re.sub(r'//.*?\n', '\n', json_str)
    json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)

    # 3. Fix unquoted keys (common LLM error)
    # Match patterns like { key: or , key: and ensure key is quoted
    json_str = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', json_str)

    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        # Print debug info for troubleshooting
        print(f"âš ï¸ JSON è§£æå¤±è´¥ï¼Œå°è¯•è¿›ä¸€æ­¥ä¿®å¤...")
        print(f"é”™è¯¯ä½ç½®: {e.msg} at line {e.lineno} col {e.colno}")

        # 4. Last resort: try to fix single quotes
        json_str_fixed = json_str.replace("'", '"')
        try:
            return json.loads(json_str_fixed)
        except:
            pass

        raise ValueError(
            f"JSON è§£æå¤±è´¥: {e.msg}\n"
            f"ä½ç½®: line {e.lineno}, column {e.colno}\n"
            "è¯·æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ ¼å¼ã€‚"
        )


def wait_until_file_active(client: genai.Client, file_obj, timeout_s: int = 120, poll_s: int = 2):
    """
    Files API ä¸Šä¼ åï¼Œæ–‡ä»¶å¯èƒ½å¤„äº PROCESSING çŠ¶æ€ï¼Œå¿…é¡»ç­‰åˆ° ACTIVE æ‰èƒ½ä½¿ç”¨ã€‚
    """
    file_name = getattr(file_obj, "name", None)
    if not file_name:
        # æå°‘æ•°æƒ…å†µä¸‹å¯¹è±¡ç»“æ„ä¸åŒï¼Œç›´æ¥è¿”å›è¯•è¯•
        return file_obj

    start = time.time()
    last_state = None

    while True:
        f = client.files.get(name=file_name)
        state = getattr(f, "state", None)

        if state != last_state:
            print(f"æ–‡ä»¶çŠ¶æ€ï¼š{state}")
            last_state = state

        if state == "ACTIVE":
            return f

        if time.time() - start > timeout_s:
            raise TimeoutError(
                f"ç­‰å¾…æ–‡ä»¶å˜ä¸º ACTIVE è¶…æ—¶ï¼ˆ>{timeout_s}sï¼‰ã€‚"
                "ä½ å¯ä»¥é‡è¯•ä¸€æ¬¡ï¼Œæˆ–è€…æ¢æ›´å°/æ›´å¸¸è§ç¼–ç çš„ mp4ã€‚"
            )

        time.sleep(poll_s)


def main():
    api_key = ensure_api_key()

    if not VIDEO_PATH.exists():
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶ï¼š{VIDEO_PATH}\n"
            "è¯·ç¡®è®¤ä½ å·²æŠŠè§†é¢‘æ”¾åˆ° downloads/ é‡Œï¼Œå¹¶å‘½åä¸º input.mp4"
        )

    size_mb = VIDEO_PATH.stat().st_size / (1024 * 1024)
    print(f"å‡†å¤‡å¤„ç†è§†é¢‘ï¼š{VIDEO_PATH.name} ({size_mb:.1f} MB)")

    client = genai.Client(api_key=api_key)

    print("å¼€å§‹ä¸Šä¼ è§†é¢‘åˆ° Files APIâ€¦")
    uploaded = client.files.upload(file=str(VIDEO_PATH))
    print(f"âœ… ä¸Šä¼ å®Œæˆï¼š{uploaded.name}")

    print("ç­‰å¾…æ–‡ä»¶å˜ä¸º ACTIVEâ€¦")
    video_file = wait_until_file_active(client, uploaded, timeout_s=180, poll_s=2)
    print("âœ… æ–‡ä»¶å·² ACTIVEï¼Œå¯ä»¥å¼€å§‹åˆ†æ")

    print("å¼€å§‹åˆ†æè§†é¢‘ï¼ˆç”Ÿæˆåˆ†é•œ JSONï¼‰â€¦")
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=[DIRECTOR_METAPROMPT, video_file],
    )

    raw_text = getattr(response, "text", None) or ""
    storyboard = extract_json_array(raw_text)

    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    OUT_PATH.write_text(
        json.dumps(storyboard, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    print(f"\nâœ… å·²ç”Ÿæˆåˆ†é•œ JSONï¼š{OUT_PATH}")
    print(f"åˆ†é•œæ•°é‡ï¼š{len(storyboard)}")


if __name__ == "__main__":
    main()
</file>

<file path="extract_frames.py">
import json
import subprocess
from pathlib import Path

from core.utils import get_ffmpeg_path

PROJECT_DIR = Path(__file__).parent
VIDEO_PATH = PROJECT_DIR / "downloads" / "input.mp4"
STORYBOARD_PATH = PROJECT_DIR / "outputs" / "storyboard.json"
FRAMES_DIR = PROJECT_DIR / "frames"

def to_seconds(t):
    if t is None:
        return None
    if isinstance(t, (int, float)):
        return float(t)

    s = str(t).strip()
    if not s:
        return None

    try:
        return float(s)
    except ValueError:
        pass

    parts = s.split(":")
    try:
        parts = [float(p) for p in parts]
    except ValueError:
        return None

    if len(parts) == 3:
        hh, mm, ss = parts
        return hh * 3600 + mm * 60 + ss
    if len(parts) == 2:
        mm, ss = parts
        return mm * 60 + ss
    if len(parts) == 1:
        return parts[0]
    return None

def main():
    if not VIDEO_PATH.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è§†é¢‘ï¼š{VIDEO_PATH}")
    if not STORYBOARD_PATH.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° storyboardï¼š{STORYBOARD_PATH}")

    FRAMES_DIR.mkdir(parents=True, exist_ok=True)

    storyboard = json.loads(STORYBOARD_PATH.read_text(encoding="utf-8"))
    saved = 0

    for shot in storyboard:
        shot_num = shot.get("shot_number", saved + 1)
        start_time = shot.get("start_time", None)
        ts = to_seconds(start_time)

        if ts is None:
            continue

        out_path = FRAMES_DIR / f"shot_{int(shot_num):02d}.png"

        cmd = [
            get_ffmpeg_path(),
            "-y",
            "-ss", str(ts),
            "-i", str(VIDEO_PATH),
            "-frames:v", "1",
            "-q:v", "2",
            str(out_path)
        ]

        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        if out_path.exists():
            saved += 1

    print(f"âœ… æˆªå¸§å®Œæˆï¼š{saved} å¼ ï¼Œä¿å­˜åœ¨ {FRAMES_DIR}")

if __name__ == "__main__":
    main()
</file>

<file path="run_workflow.py">
import json
import argparse
from pathlib import Path
import shutil

from core.utils import get_ffmpeg_path

PROJECT_DIR = Path(__file__).parent
DEFAULT_JOB_ID = "demo_job_001"

def load_workflow(job_dir: Path) -> dict:
    wf_path = job_dir / "workflow.json"
    return json.loads(wf_path.read_text(encoding="utf-8"))

def save_workflow(job_dir: Path, wf: dict) -> None:
    wf_path = job_dir / "workflow.json"
    wf_path.write_text(json.dumps(wf, ensure_ascii=False, indent=2), encoding="utf-8")

def find_shot(wf: dict, shot_id: str) -> dict | None:
    for s in wf.get("shots", []):
        if s.get("shot_id") == shot_id:
            return s
    return None

def ensure_videos_dir(job_dir: Path) -> Path:
    videos_dir = job_dir / "videos"
    videos_dir.mkdir(parents=True, exist_ok=True)
    return videos_dir

def mock_generate_video(job_dir: Path, shot: dict) -> str:
    """
    Demo ç‰ˆæœ¬ï¼šç”Ÿæˆä¸€ä¸ªâ€œå ä½è§†é¢‘æ–‡ä»¶â€ï¼Œç”¨æ¥éªŒè¯ runner çš„å·¥ä½œæ–¹å¼ã€‚
    åç»­æ¥ Seedance/Veo æ—¶ï¼Œåªéœ€æ›¿æ¢è¿™ä¸ªå‡½æ•°ã€‚
    """
    videos_dir = ensure_videos_dir(job_dir)
    out_path = videos_dir / f"{shot['shot_id']}.mp4"

    # ç”¨ input.mp4 çš„å‰ 1 ç§’å¤åˆ¶æˆä¸€ä¸ªå°æ–‡ä»¶ï¼ˆç¡®ä¿æ˜¯å¯æ’­æ”¾ mp4ï¼‰
    src_video = job_dir / "input.mp4"
    if not src_video.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æºè§†é¢‘ï¼š{src_video}")

    # ç›´æ¥å¤åˆ¶ä¼šå¾ˆå¤§ï¼›ä¸ºäº†å¿«ï¼Œæˆ‘ä»¬å¤åˆ¶ä¸€ä¸ªå°ç‰‡æ®µï¼ˆç”¨ ffmpegï¼‰
    # ä½¿ç”¨è·¨å¹³å° ffmpeg è·¯å¾„è·å–
    ffmpeg = get_ffmpeg_path()

    import subprocess
    cmd = [
        ffmpeg, "-y",
        "-i", str(src_video),
        "-t", "1.0",
        "-c", "copy",
        str(out_path)
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    return f"videos/{out_path.name}"

def run_video_generate(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    """
    æ‰§è¡Œ video_generate èŠ‚ç‚¹ï¼š
    - target_shot=Noneï¼šè·‘æ‰€æœ‰ NOT_STARTED æˆ– FAILED çš„ shot
    - target_shot=shot_03ï¼šåªè·‘æŒ‡å®š shotï¼ˆå•èŠ‚ç‚¹é‡è·‘ï¼‰
    """
    shots = wf.get("shots", [])
    for shot in shots:
        sid = shot.get("shot_id")

        if target_shot and sid != target_shot:
            continue

        status = shot.get("status", {}).get("video_generate", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"):
            continue

        # æ ‡è®°è¿è¡Œä¸­
        shot.setdefault("status", {})["video_generate"] = "RUNNING"
        shot.setdefault("errors", {})["video_generate"] = None
        save_workflow(job_dir, wf)

        try:
            rel_video_path = mock_generate_video(job_dir, shot)
            shot.setdefault("assets", {})["video"] = rel_video_path
            shot["status"]["video_generate"] = "SUCCESS"
            print(f"âœ… video_generate SUCCESS: {sid} -> {rel_video_path}")
        except Exception as e:
            shot["status"]["video_generate"] = "FAILED"
            shot.setdefault("errors", {})["video_generate"] = str(e)
            print(f"âŒ video_generate FAILED: {sid} -> {e}")

        save_workflow(job_dir, wf)

def mock_stylize_frame(job_dir: Path, shot: dict) -> str:
    """
    Demo ç‰ˆé£æ ¼åŒ–ï¼šæŠŠ first_frame å¤åˆ¶æˆæ–°çš„ stylized_frameï¼ˆè¦†ç›–å†™ï¼‰ã€‚
    åç»­æ¥ Nano Banana æ—¶ï¼Œåªæ›¿æ¢è¿™é‡Œã€‚
    """
    src = job_dir / shot["assets"]["first_frame"]
    if not src.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° first_frameï¼š{src}")

    dst = job_dir / "stylized_frames" / f"{shot['shot_id']}.png"
    dst.parent.mkdir(parents=True, exist_ok=True)
    shutil.copyfile(src, dst)
    return f"stylized_frames/{dst.name}"

def run_stylize(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    shots = wf.get("shots", [])
    for shot in shots:
        sid = shot.get("shot_id")

        if target_shot and sid != target_shot:
            continue

        status = shot.get("status", {}).get("stylize", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"):
            continue

        shot.setdefault("status", {})["stylize"] = "RUNNING"
        shot.setdefault("errors", {})["stylize"] = None
        save_workflow(job_dir, wf)

        try:
            rel_path = mock_stylize_frame(job_dir, shot)
            shot.setdefault("assets", {})["stylized_frame"] = rel_path
            shot["status"]["stylize"] = "SUCCESS"
            print(f"âœ… stylize SUCCESS: {sid} -> {rel_path}")
        except Exception as e:
            shot["status"]["stylize"] = "FAILED"
            shot.setdefault("errors", {})["stylize"] = str(e)
            print(f"âŒ stylize FAILED: {sid} -> {e}")

        save_workflow(job_dir, wf)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--job_id", default=DEFAULT_JOB_ID)
    parser.add_argument("--node", choices=["video_generate"], default="video_generate")
    parser.add_argument("--shot", default=None, help="åªè¿è¡ŒæŸä¸ª shotï¼Œä¾‹å¦‚ shot_03")
    args = parser.parse_args()

    job_dir = PROJECT_DIR / "jobs" / args.job_id
    if not job_dir.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° job ç›®å½•ï¼š{job_dir}")

    wf = load_workflow(job_dir)

    if args.node == "video_generate":
        # â‘  å…ˆè·‘ stylize èŠ‚ç‚¹
        run_stylize(job_dir, wf, target_shot=args.shot)

        # â‘¡ é‡æ–°åŠ è½½ workflowï¼ˆç¡®ä¿çŠ¶æ€æœ€æ–°ï¼‰
        wf = load_workflow(job_dir)

        # â‘¢ å†è·‘ video_generate èŠ‚ç‚¹
        run_video_generate(job_dir, wf, target_shot=args.shot)


    print("âœ… runner æ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    main()
</file>

<file path="stylize_frames.py">
import os
from pathlib import Path

from google import genai
from google.genai import types

PROJECT_DIR = Path(__file__).parent
FRAMES_DIR = PROJECT_DIR / "frames"
OUT_DIR = PROJECT_DIR / "stylized_frames"
OUT_DIR.mkdir(exist_ok=True)

MODEL = "gemini-2.5-flash-image"  # Nano Bananaï¼ˆéªŒè¯æ‰¹é‡ç¨³å®šæ€§ï¼‰
# å¦‚æœä¹‹åç”¨ Proï¼šMODEL = "gemini-3-pro-image-preview"

PROMPT = """
You are given a storyboard reference frame from a viral short video.

Goal: "De-replication stylization" (same structure, new details).
- Preserve: composition, camera angle, subject placement, overall color palette, lighting mood, and emotional tone.
- Must change: all fine details must be newly designed (faces, clothing details, textures, materials, patterns, background objects, any text/logos).
- Avoid pixel-level similarity. Do NOT copy any identifiable characters, logos, or exact text.
- Keep it cinematic and coherent.

Output:
- 16:9 image
- high clarity, rich details
"""

def save_first_image_from_response(response, out_path: Path) -> bool:
    """
    Nano Banana responses can include text parts and image parts.
    We scan parts and save the first image we find.
    """
    for part in response.parts:
        if part.inline_data is not None:
            img = part.as_image()   # requires pillow
            img.save(out_path)
            return True
    return False

def main():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("æ²¡æœ‰æ£€æµ‹åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡ï¼ˆè¯·å…ˆ export GEMINI_API_KEY=ä½ çš„keyï¼‰")

    if not FRAMES_DIR.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° frames æ–‡ä»¶å¤¹ï¼š{FRAMES_DIR}")

    frame_paths = sorted(FRAMES_DIR.glob("shot_*.png"))
    if not frame_paths:
        raise FileNotFoundError(f"frames é‡Œæ²¡æœ‰ shot_*.pngï¼š{FRAMES_DIR}")

    client = genai.Client(api_key=api_key)

    print(f"å°†å¤„ç† {len(frame_paths)} å¼ åˆ†é•œå›¾ï¼Œè¾“å‡ºåˆ°ï¼š{OUT_DIR}")

    for img_path in frame_paths:
        out_path = OUT_DIR / img_path.name

        # ä¼ å…¥å›¾ç‰‡ï¼ˆå®˜æ–¹æ¨èï¼štypes.Part.from_bytesï¼‰
        image_part = types.Part.from_bytes(
            data=img_path.read_bytes(),
            mime_type="image/png",
        )

        print(f"ğŸ–¼ï¸  Stylize {img_path.name} ...")

        response = client.models.generate_content(
            model=MODEL,
            contents=[
                image_part,
                PROMPT
            ],
            # å¯é€‰ï¼šå¦‚æœä½ ç”¨ gemini-3-pro-image-preview æƒ³æŒ‡å®šè¾“å‡ºå‚æ•°ï¼Œå¯ä»¥æ‰“å¼€ä¸‹é¢ config
            # config=types.GenerateContentConfig(
            #     response_modalities=["TEXT", "IMAGE"],
            #     image_config=types.ImageConfig(aspect_ratio="16:9", image_size="2K"),
            # )
        )

        ok = save_first_image_from_response(response, out_path)
        if ok:
            print(f"âœ… saved -> {out_path}")
        else:
            # æœ‰æ—¶æ¨¡å‹åªå›æ–‡å­—ï¼ˆè¡¨ç¤ºæ²¡å‡ºå›¾æˆ–è¢«æ‹’ç»/é™çº§ï¼‰ï¼ŒæŠŠæ–‡å­—æ‰“å°å‡ºæ¥ä¾¿äºä½ åšå¯è¡Œæ€§åˆ¤æ–­
            print("âš ï¸ æ²¡æ‹¿åˆ°å›¾ç‰‡è¾“å‡ºï¼Œæ¨¡å‹è¿”å›æ–‡æœ¬å¦‚ä¸‹ï¼š")
            print(response.text)

    print("âœ… å…¨éƒ¨å®Œæˆ")

if __name__ == "__main__":
    main()
</file>

<file path="nixpacks.toml">
[phases.setup]
nixPkgs = ["python312", "ffmpeg"]

[phases.install]
cmds = ["python -m venv --copies /opt/venv", ". /opt/venv/bin/activate", "pip install -r requirements.txt"]

[start]
cmd = "uvicorn app:app --host 0.0.0.0 --port $PORT"
</file>

<file path="core/agent_engine.py">
# core/agent_engine.py
import os
import json
import re
from google import genai
from google.genai import types # ğŸ’¡ å¼•å…¥ç±»å‹å®šä¹‰
from typing import Dict, Any, List, Union

class AgentEngine:
    def __init__(self):
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise RuntimeError("æœªæ£€æµ‹åˆ° GEMINI_API_KEY")
        self.client = genai.Client(api_key=api_key)
        self.model_id = "gemini-2.0-flash" 

    def get_action_from_text(self, user_input: str, workflow_summary: str) -> Union[Dict, List]:
        system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å¯¼æ¼”åŠ©ç†ã€‚ä½ å¿…é¡»æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆå·¥ä½œæµä¿®æ”¹æŒ‡ä»¤ã€‚

[å½“å‰å·¥ä½œæµçŠ¶æ€æ‘˜è¦]
{workflow_summary}

ğŸ¬ [æ‘„å½±å‚æ•°ä¿çœŸåŸåˆ™ - CINEMATOGRAPHY FIDELITY - æœ€é«˜ä¼˜å…ˆçº§]
æ¯ä¸ªåˆ†é•œéƒ½æœ‰ä»æºè§†é¢‘ä¸­æå–çš„æ‘„å½±å‚æ•°ï¼ˆæ ‡ç­¾å½¢å¼å­˜å‚¨åœ¨æè¿°ä¸­ï¼‰ï¼Œè¿™äº›å‚æ•°å¿…é¡»è¢«ä¿æŠ¤ï¼š
- [SCALE: ...] - æ™¯åˆ«ï¼ˆWIDE/MEDIUM/CLOSE_UPç­‰ï¼‰
- [POSITION: ...] - ä¸»ä½“åœ¨ç”»é¢ä¸­çš„ä½ç½®åæ ‡
- [ORIENTATION: ...] - ä¸»ä½“èº«ä½“æœå‘
- [GAZE: ...] - ä¸»ä½“è§†çº¿æ–¹å‘
- [MOTION: ...] - è¿åŠ¨çŸ¢é‡

âš ï¸ é™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ä¿®æ”¹è¿™äº›æ‘„å½±å‚æ•°ï¼Œå¦åˆ™ä»»ä½•æ“ä½œéƒ½å¿…é¡»ä¿ç•™å®ƒä»¬ä¸å˜ï¼
- é£æ ¼ä¿®æ”¹ï¼šåªæ”¹å˜è‰ºæœ¯é£æ ¼ï¼Œä¸æ”¹å˜æ‘„å½±å‚æ•°
- ä¸»ä½“æ›¿æ¢ï¼šåªæ›¿æ¢ä¸»ä½“åè¯ï¼Œä¿ç•™æ‰€æœ‰æ‘„å½±å‚æ•°æ ‡ç­¾
- æè¿°å¢å¼ºï¼šåªæ·»åŠ æ–°å†…å®¹ï¼Œä¸åˆ é™¤æˆ–ä¿®æ”¹ç°æœ‰æ‘„å½±å‚æ•°æ ‡ç­¾

[æŒ‡ä»¤é€»è¾‘è§„èŒƒ - æå…¶é‡è¦]
1. ä¿®æ”¹å…¨å±€é£æ ¼: {{"op": "set_global_style", "value": "é£æ ¼å…³é”®è¯"}}
   - ğŸ¨ é£æ ¼è¾“å‡ºæ ¼å¼ï¼švalue å­—æ®µå¿…é¡»æ˜¯ç®€æ´çš„é£æ ¼å…³é”®è¯ï¼Œç¦æ­¢ä½¿ç”¨å†—é•¿å¥å­
     * âœ… æ­£ç¡®: "Cyberpunk Neon"
     * âœ… æ­£ç¡®: "Studio Ghibli Anime"
     * âœ… æ­£ç¡®: "Film Noir Cinematic"
     * âœ… æ­£ç¡®: "Watercolor Impressionist"
     * âŒ é”™è¯¯: "Total transformation into Cyberpunk Neon style"ï¼ˆç¦æ­¢ä½¿ç”¨å¡«å……è¯ï¼‰
     * âŒ é”™è¯¯: "Hyper-stylized in Studio Ghibli anime aesthetic"ï¼ˆç¦æ­¢å†—é•¿æè¿°ï¼‰
   - ğŸ“ ç”»å¹…çº¦æŸç”±ç³»ç»Ÿè‡ªåŠ¨å¼ºåˆ¶ 16:9ï¼Œæ— éœ€åœ¨ value ä¸­æŒ‡å®š
   - ğŸ¬ æ‘„å½±ä¿çœŸï¼šé£æ ¼ä¿®æ”¹ä¸ä¼šå½±å“æ‘„å½±å‚æ•°ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä¿ç•™

ğŸ†” [å…¨å±€èº«ä»½é”šå®š - GLOBAL IDENTITY ANCHOR - è§’è‰²ä¸€è‡´æ€§åŸåˆ™]
å½“ç”¨æˆ·å®šä¹‰ä¸€ä¸ªå…·ä½“è§’è‰²ï¼ˆå¦‚"é‡‘è‰²çŸ­å‘çš„å¥³äºº"ï¼‰ï¼Œè¯¥æè¿°å°†æˆä¸ºå…¨å±€èº«ä»½é”šå®š(Global Identity Anchor)ã€‚
- âœ… ç³»ç»Ÿä¼šè‡ªåŠ¨å°†å®Œå…¨ç›¸åŒçš„è§’è‰²æè¿°ä¼ æ’­åˆ°æ‰€æœ‰ç›¸å…³åˆ†é•œï¼Œç¡®ä¿è§†è§‰ä¸€è‡´æ€§
- âœ… è§’è‰²ç‰¹å¾ï¼ˆå‘å‹ã€å‘è‰²ã€æœè£…ç­‰ï¼‰åœ¨æ‰€æœ‰ä¸»è§’é•œå¤´ä¸­ä¿æŒ100%ä¸€è‡´
- âš ï¸ æ™ºèƒ½åœºæ™¯æ£€æµ‹ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨è·³è¿‡ä»¥ä¸‹ç±»å‹çš„åˆ†é•œï¼ˆä¸æ³¨å…¥è§’è‰²ï¼‰ï¼š
  * çº¯é£æ™¯/ç¯å¢ƒé•œå¤´ï¼ˆlandscape, cityscape, nature sceneï¼‰
  * ç©ºåœºæ™¯/å»ºç­‘å†…æ™¯ï¼ˆempty room, establishing shotï¼‰
  * ç‰©ä½“ç‰¹å†™ï¼ˆobject close-up, food, vehicleï¼‰
  * æ— äººç‰©çš„è¿‡æ¸¡é•œå¤´

2. å…¨å±€ä¸»ä½“æ›¿æ¢ï¼ˆç®€å•ï¼‰: {{"op": "global_subject_swap", "old_subject": "è‹±æ–‡åŸè¯", "new_subject": "è‹±æ–‡æ–°è¯"}}
   - æ–¹å‘é€»è¾‘ï¼š"æŠŠ A æ¢æˆ B"æ„å‘³ç€ A æ˜¯æ—§çš„(old)ï¼ŒB æ˜¯æ–°çš„(new)ã€‚
   - åŒ¹é…è¦æ±‚ï¼šä½ å¿…é¡»è§‚å¯Ÿ [æ‘˜è¦] ä¸­çš„ Shot Descriptionsï¼Œæ‰¾å‡ºå…¶ä¸­çœŸæ­£å­˜åœ¨çš„è‹±æ–‡å•è¯ä½œä¸º "old_subject"ã€‚
   - ç¿»è¯‘è¦æ±‚ï¼šå¦‚æœç”¨æˆ·è¯´"ç”·äºº"ï¼Œè€Œæ‘˜è¦é‡Œæ˜¯ "man"ï¼Œè¯·ä½¿ç”¨ "man"ï¼›å¦‚æœç”¨æˆ·è¯´"å°å­©"ï¼Œè¯·ç¿»è¯‘ä¸º "child"ã€‚
   - ğŸ¬ æ‘„å½±ä¿çœŸï¼šä¸»ä½“æ›¿æ¢æ—¶ï¼Œæ‰€æœ‰æ‘„å½±å‚æ•°æ ‡ç­¾ä¼šè¢«è‡ªåŠ¨ä¿ç•™ï¼ˆä½ç½®ã€æœå‘ã€è§†çº¿ã€åŠ¨ä½œä¸å˜ï¼‰
   - ğŸ†” èº«ä»½é”šå®šï¼šç³»ç»Ÿè‡ªåŠ¨å¤„ç†ï¼Œé£æ™¯/ç©ºé•œå¤´ä¼šè¢«æ™ºèƒ½è·³è¿‡
   - âš ï¸ ä»…ç”¨äºæ— å±æ€§æè¿°çš„ç®€å•æ›¿æ¢ï¼ˆå¦‚"æŠŠç”·äººæ¢æˆå¥³äºº"ï¼‰

2b. ğŸ¨ ç»†ç²’åº¦è§’è‰²æ›¿æ¢ï¼ˆå¸¦è§†è§‰å±æ€§ï¼‰: {{"op": "detailed_subject_swap", "old_subject": "è‹±æ–‡åŸè¯", "new_subject": "è‹±æ–‡æ–°ä¸»ä½“", "attributes": {{...}}}}
   - ğŸ” è§¦å‘æ¡ä»¶ï¼šå½“ç”¨æˆ·æä¾›è¯¦ç»†çš„è§†è§‰æè¿°æ—¶ä½¿ç”¨æ­¤æ“ä½œï¼Œä¾‹å¦‚ï¼š
     * "æŠŠç”·äººæ¢æˆä¸€ä¸ªé‡‘è‰²çŸ­å‘ã€ç©¿çº¢è¡£æœçš„å¥³äºº"
     * "Replace the man with a young woman with blue eyes and silver hair"
     * "æŠŠå°å­©æ¢æˆä¸€ä¸ªæˆ´çœ¼é•œçš„è€äºº"
   - ğŸ“ attributes å¯¹è±¡å¿…é¡»æå–æ‰€æœ‰è§†è§‰æè¿°ç¬¦ï¼ˆåªå¡«å†™ç”¨æˆ·æ˜ç¡®æåˆ°çš„ï¼‰ï¼š
     {{
       "hair_style": "short/long/curly/straight/bald/ponytail...",
       "hair_color": "golden/black/brown/silver/red/blonde...",
       "eye_color": "blue/green/brown/hazel...",
       "skin_tone": "fair/tan/dark/pale...",
       "age_descriptor": "young/elderly/middle-aged/child...",
       "clothing": "red dress/black suit/white shirt/casual attire...",
       "accessories": "glasses/hat/necklace/earrings...",
       "body_type": "slim/muscular/petite/tall...",
       "facial_features": "beard/freckles/scar/dimples...",
       "other_visual": "ä»»ä½•å…¶ä»–è§†è§‰ç‰¹å¾..."
     }}
   - âš ï¸ åªå¡«å†™ç”¨æˆ·æ˜ç¡®æåˆ°çš„å±æ€§ï¼ŒæœªæåŠçš„å±æ€§ä¸è¦æ·»åŠ 
   - ğŸ¬ æ‘„å½±ä¿çœŸï¼šæ‰€æœ‰è§†è§‰å±æ€§å°†è¢«æ³¨å…¥å™äº‹å±‚ï¼Œä½†æ‘„å½±å‚æ•°æ ‡ç­¾ä¿æŒä¸å˜
   - ğŸ†” èº«ä»½é”šå®šï¼šè§’è‰²æè¿°å°†ä½œä¸ºGlobal Identity Anchorå­˜å‚¨ï¼Œå¹¶ä¼ æ’­åˆ°æ‰€æœ‰ä¸»è§’åˆ†é•œ
   - ğŸï¸ åœºæ™¯ä¿æŠ¤ï¼šé£æ™¯/ç©ºé•œå¤´è‡ªåŠ¨è·³è¿‡ï¼Œä¿æŒåŸå§‹è¯­ä¹‰æ„å›¾

3. å¢å¼ºåˆ†é•œæè¿°: {{"op": "enhance_shot_description", "shot_id": "shot_XX", "spatial_info": "ç©ºé—´ä½ç½®æè¿°", "style_boost": "é£æ ¼å¼ºåŒ–æè¿°"}}
   - ğŸ“ ç©ºé—´æ„ŸçŸ¥ï¼ˆå¿…é¡»ä¿æŒ 16:9 å®½å±æ„å›¾ï¼‰ï¼š
     * "subject positioned on the left side of the 16:9 widescreen frame"
     * "character facing right in cinematic widescreen composition"
     * "object in the foreground, widescreen depth of field"
     * "centered composition with 16:9 cinematic framing"
   - âš ï¸ ä¸¥ç¦ä»»ä½• 1:1 æ­£æ–¹å½¢æˆ–ç«–å±æ„å›¾æè¿°
   - ğŸ¬ æ‘„å½±ä¿çœŸï¼šå¢å¼ºæè¿°æ—¶ï¼Œç°æœ‰çš„æ‘„å½±å‚æ•°æ ‡ç­¾ä¼šè¢«è‡ªåŠ¨ä¿ç•™

4. ä¿®æ”¹æ‘„å½±å‚æ•°ï¼ˆä»…å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚æ—¶ä½¿ç”¨ï¼‰: {{"op": "update_cinematography", "shot_id": "shot_XX", "param": "å‚æ•°å", "value": "æ–°å€¼"}}
   - å‚æ•°åå¯ä»¥æ˜¯: "shot_scale", "subject_frame_position", "subject_orientation", "gaze_direction", "motion_vector"
   - âš ï¸ åªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¯´"æŠŠé•œå¤´æ”¹æˆç‰¹å†™"ã€"è®©äººç‰©è½¬å‘å·¦è¾¹"ç­‰æ˜ç¡®ä¿®æ”¹æ‘„å½±å‚æ•°çš„æŒ‡ä»¤æ—¶æ‰ä½¿ç”¨æ­¤æ“ä½œ

[è¾“å‡ºè¦æ±‚]
- å¿…é¡»è¯†åˆ«ç”¨æˆ·çš„æ‰€æœ‰æ„å›¾ã€‚
- å¿…é¡»è¿”å›ä¸€ä¸ªåŒ…å«æŒ‡ä»¤å¯¹è±¡çš„ JSON åˆ—è¡¨ []ï¼Œå³ä½¿åªæœ‰ä¸€æ¡æŒ‡ä»¤ä¹Ÿè¦æ”¾åœ¨åˆ—è¡¨é‡Œã€‚
- ä¸¥ç¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œåªè¾“å‡ºçº¯ JSON å­—ç¬¦ä¸²ã€‚
- set_global_style çš„ value å¿…é¡»æ˜¯ 2-4 ä¸ªå•è¯çš„ç®€æ´é£æ ¼å…³é”®è¯ï¼Œç¦æ­¢ä½¿ç”¨ "Total transformation"ã€"Hyper-stylized"ã€"Complete overhaul" ç­‰å¡«å……å¥å¼ã€‚
- é»˜è®¤ä¿æŠ¤æ‘„å½±å‚æ•°ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ä¿®æ”¹ã€‚
"""
        try:
            # ğŸ’¡ å¼ºåˆ¶ JSON æ¨¡å¼ï¼Œç¡®ä¿è¾“å‡ºç»“æ„ç¨³å®š
            response = self.client.models.generate_content(
                model=self.model_id,
                contents=[system_prompt, f"ç”¨æˆ·æŒ‡ä»¤: {user_input}"],
                config=types.GenerateContentConfig(
                    response_mime_type='application/json',
                )
            )
            
            # è‡ªåŠ¨è§£æ JSON å­—ç¬¦ä¸²
            res_json = json.loads(response.text)
            
            # è°ƒè¯•æ—¥å¿—ï¼šåœ¨ç»ˆç«¯æ‰“å° Agent çš„å†³ç­–é€»è¾‘
            print(f"ğŸ¤– Agent å†³ç­–æŒ‡ä»¤é›†: {res_json}")
            
            return res_json
            
        except Exception as e:
            print(f"âŒ Agent å†³ç­–è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {str(e)}")
            if 'response' in locals() and hasattr(response, 'candidates'):
                print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - åœæ­¢åŸå› : {response.candidates[0].finish_reason}")
            return {"op": "error", "reason": str(e)}
</file>

<file path="app.py">
# app.py
import os
import uuid
import shutil
from pathlib import Path
from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import Optional, Dict, Any, List, Union

from core.workflow_manager import WorkflowManager
from core.agent_engine import AgentEngine

app = FastAPI(title="AI å¯¼æ¼”å·¥ä½œå° API")
Path("jobs").mkdir(parents=True, exist_ok=True)
Path("temp_uploads").mkdir(parents=True, exist_ok=True)

# 1. è·¨åŸŸé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# 2. åˆå§‹åŒ–æ ¸å¿ƒå¼•æ“
# åˆ›å»ºå…¨å±€ manager å®ä¾‹
manager = WorkflowManager() 
agent = AgentEngine()

# --- æ•°æ®æ¨¡å‹ ---
class ChatRequest(BaseModel):
    message: str
    job_id: Optional[str] = None 

class ShotUpdateRequest(BaseModel):
    shot_id: str
    description: Optional[str] = None
    video_model: Optional[str] = None
    job_id: Optional[str] = None

# --- è·¯ç”±æ¥å£ ---

@app.get("/")
async def read_index():
    return FileResponse('index.html')

@app.post("/api/upload")
async def upload_video(file: UploadFile = File(...)):
    print(f"ğŸ“¥ [æ”¶åˆ°æ–‡ä»¶] æ­£åœ¨æ¥æ”¶ä¸Šä¼ : {file.filename}") 
    try:
        temp_dir = Path("temp_uploads")
        temp_dir.mkdir(exist_ok=True)
        temp_file_path = temp_dir / f"{uuid.uuid4()}_{file.filename}"
        
        with open(temp_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        print(f"ğŸ§  [AI å¯åŠ¨] æ­£åœ¨è°ƒç”¨ Gemini 1.5 Pro æ‹†è§£åˆ†é•œï¼Œè¯·è€å¿ƒç­‰å¾…...")
        new_job_id = manager.initialize_from_file(temp_file_path)
        
        if temp_file_path.exists():
            os.remove(temp_file_path)
            
        print(f"âœ… [å…¨éƒ¨å®Œæˆ] æ–°é¡¹ç›®å·²å°±ç»ª: {new_job_id}")
        return {"status": "success", "job_id": new_job_id}
    except Exception as e:
        print(f"âŒ [æŠ¥é”™] ä¸Šä¼ æ‹†è§£ç¯èŠ‚å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/workflow")
async def get_workflow(job_id: Optional[str] = None):
    """è·å–æœ€æ–°å…¨å±€çŠ¶æ€"""
    target_id = job_id or manager.job_id
    if not target_id:
        jobs_dir = Path("jobs")
        if jobs_dir.exists():
            existing_jobs = sorted([d.name for d in jobs_dir.iterdir() if d.is_dir()], reverse=True)
            if existing_jobs: target_id = existing_jobs[0]
    
    if not target_id:
        return {"error": "No jobs found"}
        
    # åŠ¨æ€åŒæ­¥çŠ¶æ€
    manager.job_id = target_id
    manager.job_dir = Path("jobs") / target_id
    return manager.load()

@app.post("/api/agent/chat")
async def agent_chat(req: ChatRequest):
    """Agent å…¨å±€æŒ‡æŒ¥"""
    if req.job_id: 
        manager.job_id = req.job_id
        manager.job_dir = Path("jobs") / req.job_id
        
    # å…ˆåŒæ­¥ç£ç›˜æ•°æ®åˆ°å†…å­˜
    wf = manager.load()
    
    # ğŸ’¡ å¿…é¡»åŒ…å«æ‰€æœ‰åˆ†é•œæè¿°ï¼ŒAgent æ‰èƒ½æ‰¾åˆ°æ‰€æœ‰ä¸»ä½“è¿›è¡Œæ›¿æ¢
    all_descriptions = []
    for i, shot in enumerate(wf.get("shots", [])):
        desc = shot.get("description", "")
        if desc:
            all_descriptions.append(f"Shot {i+1}: {desc}")
    descriptions_text = "\n".join(all_descriptions) if all_descriptions else "No shots"
    summary = f"Job ID: {manager.job_id}\nGlobal Style: {wf.get('global', {}).get('style_prompt')}\n\n[All Shot Descriptions]\n{descriptions_text}"
    
    action = agent.get_action_from_text(req.message, summary)
    if isinstance(action, list) or (isinstance(action, dict) and action.get("op") != "error"):
        res = manager.apply_agent_action(action)
        return {"action": action, "result": res}
    return {"action": action, "result": {"status": "error"}}

@app.post("/api/shot/update")
async def update_shot_params(req: ShotUpdateRequest):
    """å½¢æ€ 3ï¼šæ‰‹åŠ¨å¾®è°ƒå•ä¸ªåˆ†é•œ - ä¿®å¤ä¿å­˜é€»è¾‘"""
    if req.job_id:
        manager.job_id = req.job_id
        manager.job_dir = Path("jobs") / req.job_id
    
    # ğŸ’¡ æ ¸å¿ƒä¿®å¤ï¼šä¿®æ”¹å‰å¿…é¡»å¼ºåˆ¶åŠ è½½è¯¥ job çš„æœ€æ–°ç£ç›˜æ•°æ®ï¼Œé˜²æ­¢ç‰ˆæœ¬è¦†ç›–
    manager.load()
    
    action = {
        "op": "update_shot_params",
        "shot_id": req.shot_id,
        "description": req.description
    }
    
    res = manager.apply_agent_action(action)
    return res

@app.post("/api/run/{node_type}")
async def run_task(node_type: str, background_tasks: BackgroundTasks, shot_id: Optional[str] = None, job_id: Optional[str] = None):
    if job_id:
        manager.job_id = job_id
        manager.job_dir = Path("jobs") / job_id

    # å¤„ç†åˆå¹¶å¯¼å‡ºé€»è¾‘
    if node_type == "merge":
        manager.load()
        try:
            result_file = manager.merge_videos()
            return {"status": "success", "file": result_file, "job_id": manager.job_id}
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    if node_type not in ["stylize", "video_generate"]:
        raise HTTPException(status_code=400, detail="Invalid node type")
    
    background_tasks.add_task(manager.run_node, node_type, shot_id)
    return {"status": "started", "job_id": manager.job_id}

# --- æ ¸å¿ƒï¼šé˜²ç¼“å­˜ä¸­é—´ä»¶ ---
@app.middleware("http")
async def add_no_cache_header(request, call_next):
    response = await call_next(request)
    if request.url.path.startswith("/assets"):
        response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"
    return response

# æŒ‚è½½é™æ€èµ„æºç›®å½•
app.mount("/assets", StaticFiles(directory="jobs", check_dir=False), name="assets")

if __name__ == "__main__":
    import uvicorn
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="core/workflow_manager.py">
# core/workflow_manager.py
import json
import time
import os
import re
import uuid
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional, Union

from core.workflow_io import load_workflow, save_workflow
from core.changes import apply_global_style, replace_entity_reference
from core.runner import run_pipeline, run_stylize, run_video_generate
from core.utils import get_ffmpeg_path

# å¼•å…¥æ‹†è§£æ‰€éœ€çš„åº“å’Œé€»è¾‘
from google import genai
from analyze_video import DIRECTOR_METAPROMPT, wait_until_file_active, extract_json_array
from extract_frames import to_seconds

class WorkflowManager:
    def __init__(self, job_id: Optional[str] = None, project_root: Optional[Path] = None):
        self.project_dir = project_root or Path(__file__).parent.parent
        self.job_id = job_id
        self.workflow: Dict[str, Any] = {}
        
        if job_id:
            self.job_dir = self.project_dir / "jobs" / job_id
            if (self.job_dir / "workflow.json").exists():
                self.load()

    def initialize_from_file(self, temp_video_path: Path) -> str:
        """å…¨è‡ªåŠ¨åˆå§‹åŒ–ç®¡çº¿ï¼šå®Œæˆæ‹†è§£ä¸åŸå§‹ç´ ææå–"""
        new_id = f"job_{uuid.uuid4().hex[:8]}"
        self.job_id = new_id
        self.job_dir = self.project_dir / "jobs" / new_id
        
        self.job_dir.mkdir(parents=True, exist_ok=True)
        (self.job_dir / "frames").mkdir(exist_ok=True)
        (self.job_dir / "videos").mkdir(exist_ok=True)
        (self.job_dir / "source_segments").mkdir(exist_ok=True)
        (self.job_dir / "stylized_frames").mkdir(exist_ok=True)
        
        final_video_path = self.job_dir / "input.mp4"
        shutil.move(str(temp_video_path), str(final_video_path))
        
        print(f"ğŸš€ [Phase 1] æ­£åœ¨é€šè¿‡ Gemini æ‹†è§£è§†é¢‘: {new_id}...")
        storyboard = self._run_gemini_analysis(final_video_path)
        
        print(f"ğŸš€ [Phase 2] æ­£åœ¨æå–å…³é”®å¸§ä¸åŸå§‹åˆ†é•œçŸ­ç‰‡...")
        self._run_ffmpeg_extraction(final_video_path, storyboard)
        
        shots = []
        for s in storyboard:
            shot_num = int(s.get("shot_number", 1))
            sid = f"shot_{shot_num:02d}"

            # ğŸ“‹ Semantic Split: Narrative Layer (plot) + Technical Layer (metadata tags)
            # Narrative Layer - Pure visual/plot description (no camera technical terms)
            narrative_desc = s.get("frame_description") or s.get("content_analysis") or ""

            # ğŸ¬ Cinematography Fidelity Parameters - Hard-coded constraints from source analysis
            shot_scale = s.get("shot_scale", "")
            subject_frame_position = s.get("subject_frame_position", "")
            subject_orientation = s.get("subject_orientation", "")
            gaze_direction = s.get("gaze_direction", "")
            motion_vector = s.get("motion_vector", "")
            camera_type = s.get("camera_type") or s.get("camera_movement", "")

            # Build structured description with HARD-CODED cinematography constraints
            desc_lines = [narrative_desc]

            # ğŸ¯ CRITICAL: These are non-negotiable constraints that MUST be preserved
            if shot_scale:
                desc_lines.append(f"[SCALE: {shot_scale}]")
            if subject_frame_position:
                desc_lines.append(f"[POSITION: {subject_frame_position}]")
            if subject_orientation:
                desc_lines.append(f"[ORIENTATION: {subject_orientation}]")
            if gaze_direction:
                desc_lines.append(f"[GAZE: {gaze_direction}]")
            if motion_vector:
                desc_lines.append(f"[MOTION: {motion_vector}]")
            if camera_type:
                desc_lines.append(f"[CAMERA: {camera_type}]")

            # Join with newlines for clear separation
            full_description = "\n".join(desc_lines)

            # ğŸ’¾ Store raw cinematography data for downstream enforcement
            cinematography_data = {
                "shot_scale": shot_scale,
                "subject_frame_position": subject_frame_position,
                "subject_orientation": subject_orientation,
                "gaze_direction": gaze_direction,
                "motion_vector": motion_vector,
                "camera_type": camera_type
            }

            shots.append({
                "shot_id": sid,
                "start_time": s.get("start_time"),
                "end_time": s.get("end_time"),
                "description": full_description,
                "cinematography": cinematography_data,  # ğŸ¬ Hard-coded source cinematography for fidelity enforcement
                "entities": [],
                "assets": {
                    "first_frame": f"frames/{sid}.png",
                    "source_video_segment": f"source_segments/{sid}.mp4",
                    "stylized_frame": None, # ğŸ’¡ PMé€»è¾‘ï¼šåˆå§‹åŒ–ä¸ºç©ºï¼Œå¼ºåˆ¶è§¦å‘ AI ç”Ÿå›¾æµç¨‹
                    "video": None
                },
                "status": {
                    "stylize": "NOT_STARTED",
                    "video_generate": "NOT_STARTED"
                }
            })
            
        self.workflow = {
            "job_id": new_id,
            "source_video": "input.mp4",
            "global": {"style_prompt": "Cinematic Realistic", "video_model": "veo"},
            "global_stages": {
                "analyze": "SUCCESS", "extract": "SUCCESS", 
                "stylize": "NOT_STARTED", "video_gen": "NOT_STARTED", "merge": "NOT_STARTED"
            },
            "shots": shots,
            "meta": {"attempts": 0, "updated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
        }
        
        self.save()
        print(f"âœ… [Done] è§†é¢‘æ‹†è§£ä¸åˆ‡ç‰‡å®Œæˆï¼ŒJob ID: {new_id}")
        return new_id

    def _run_gemini_analysis(self, video_path: Path):
        api_key = os.getenv("GEMINI_API_KEY")
        client = genai.Client(api_key=api_key)
        uploaded = client.files.upload(file=str(video_path))
        video_file = wait_until_file_active(client, uploaded)
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=[DIRECTOR_METAPROMPT, video_file],
        )
        raw_shots = extract_json_array(response.text)

        # è¯­ä¹‰åŒ–åˆå¹¶ï¼šå‡å°‘è¿‡åº¦åˆ†é•œ
        merged_shots = self._merge_semantic_shots(raw_shots, client)
        return merged_shots

    def _merge_semantic_shots(self, shots: List[Dict], client) -> List[Dict]:
        """
        è¯­ä¹‰åŒ–åˆå¹¶ï¼šå°†è¿ç»­çš„ã€èƒŒæ™¯/è§’åº¦/ä¸»ä½“ç›¸ä¼¼çš„åˆ†é•œåˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´åˆ†é•œã€‚
        ä½¿ç”¨ AI åˆ¤æ–­å“ªäº›è¿ç»­åˆ†é•œåº”è¯¥åˆå¹¶ã€‚
        """
        if len(shots) <= 1:
            return shots

        # æ„å»ºåˆå¹¶åˆ¤æ–­æç¤º
        shots_summary = []
        for i, s in enumerate(shots):
            shots_summary.append({
                "index": i,
                "start_time": s.get("start_time"),
                "end_time": s.get("end_time"),
                "description": s.get("frame_description") or s.get("content_analysis"),
                "shot_type": s.get("shot_type"),
                "camera_angle": s.get("camera_angle"),
                "camera_movement": s.get("camera_movement")
            })

        merge_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å½±è§†å‰ªè¾‘å¸ˆã€‚è¯·åˆ†æä»¥ä¸‹åˆ†é•œåˆ—è¡¨ï¼Œåˆ¤æ–­å“ªäº›**è¿ç»­çš„**åˆ†é•œåº”è¯¥åˆå¹¶ã€‚

åˆå¹¶æ¡ä»¶ï¼ˆå¿…é¡»åŒæ—¶æ»¡è¶³ï¼‰ï¼š
1. åˆ†é•œæ˜¯**è¿ç»­çš„**ï¼ˆindex ç›¸é‚»ï¼‰
2. åœºæ™¯/èƒŒæ™¯æ²¡æœ‰æ˜¾è‘—å˜åŒ–
3. ä¸»ä½“/è§’è‰²æ²¡æœ‰åˆ‡æ¢
4. æœºä½è§’åº¦æ²¡æœ‰æ˜æ˜¾å˜åŒ–
5. å±äºåŒä¸€ä¸ªå®Œæ•´åŠ¨ä½œæˆ–äº‹ä»¶

åˆ†é•œåˆ—è¡¨ï¼š
{json.dumps(shots_summary, ensure_ascii=False, indent=2)}

è¯·è¾“å‡ºéœ€è¦åˆå¹¶çš„åˆ†é•œç»„ï¼Œæ ¼å¼ä¸ºçº¯JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªéœ€è¦åˆå¹¶çš„indexæ•°ç»„ã€‚
ä¾‹å¦‚ï¼š[[0,1,2], [5,6]] è¡¨ç¤ºå°†0-1-2åˆå¹¶ä¸ºä¸€ä¸ªåˆ†é•œï¼Œ5-6åˆå¹¶ä¸ºä¸€ä¸ªåˆ†é•œã€‚
å¦‚æœæ²¡æœ‰éœ€è¦åˆå¹¶çš„ï¼Œè¾“å‡ºç©ºæ•°ç»„ []ã€‚
ä»…è¾“å‡ºçº¯JSONï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚"""

        try:
            merge_response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=[merge_prompt],
            )
            merge_text = merge_response.text.strip()

            # æå–JSONæ•°ç»„
            if merge_text.startswith("["):
                merge_groups = json.loads(merge_text)
            else:
                l = merge_text.find("[")
                r = merge_text.rfind("]")
                if l != -1 and r != -1:
                    merge_groups = json.loads(merge_text[l:r+1])
                else:
                    merge_groups = []

            if not merge_groups:
                print(f"ğŸ“Š è¯­ä¹‰åˆ†æï¼šæ— éœ€åˆå¹¶ï¼Œä¿ç•™ {len(shots)} ä¸ªåˆ†é•œ")
                return shots

            # æ‰§è¡Œåˆå¹¶
            merged_indices = set()
            for group in merge_groups:
                if isinstance(group, list) and len(group) > 1:
                    merged_indices.update(group[1:])  # é™¤äº†ç¬¬ä¸€ä¸ªï¼Œå…¶ä½™æ ‡è®°ä¸ºè¢«åˆå¹¶

            result = []
            i = 0
            new_shot_num = 1
            while i < len(shots):
                shot = shots[i].copy()

                # æ£€æŸ¥æ˜¯å¦æ˜¯åˆå¹¶ç»„çš„èµ·å§‹
                merge_group = None
                for group in merge_groups:
                    if isinstance(group, list) and len(group) > 1 and group[0] == i:
                        merge_group = group
                        break

                if merge_group:
                    # åˆå¹¶è¯¥ç»„çš„æ‰€æœ‰åˆ†é•œ
                    last_idx = merge_group[-1]
                    shot["end_time"] = shots[last_idx].get("end_time")

                    # åˆå¹¶æè¿°
                    descriptions = []
                    for idx in merge_group:
                        if idx < len(shots):
                            desc = shots[idx].get("frame_description") or shots[idx].get("content_analysis")
                            if desc and desc not in descriptions:
                                descriptions.append(desc)
                    shot["frame_description"] = " â†’ ".join(descriptions[:3])  # æœ€å¤šä¿ç•™3æ®µæè¿°
                    shot["content_analysis"] = shot["frame_description"]

                    print(f"ğŸ”— åˆå¹¶åˆ†é•œ {[s+1 for s in merge_group]} -> shot_{new_shot_num:02d}")
                    i = last_idx + 1
                else:
                    if i not in merged_indices:
                        i += 1
                    else:
                        i += 1
                        continue

                shot["shot_number"] = new_shot_num
                result.append(shot)
                new_shot_num += 1

            print(f"ğŸ“Š è¯­ä¹‰åˆå¹¶å®Œæˆï¼š{len(shots)} ä¸ªåˆ†é•œ -> {len(result)} ä¸ªåˆ†é•œ")
            return result

        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰åˆå¹¶åˆ†æå¤±è´¥ ({e})ï¼Œä¿ç•™åŸå§‹åˆ†é•œ")
            return shots

    def _run_ffmpeg_extraction(self, video_path: Path, storyboard: List):
        """
        æ¯«ç§’çº§ç²¾å‡†æå–ï¼š
        - å…³é”®å¸§æå–ï¼šä»åˆ†é•œä¸­ç‚¹æå–ï¼Œç¡®ä¿ç”»é¢ä¸æè¿°ä¸€è‡´
        - è§†é¢‘ç‰‡æ®µï¼šä½¿ç”¨ç²¾å‡†åˆ‡å‰²æ¨¡å¼
        """
        ffmpeg_path = get_ffmpeg_path()
        for s in storyboard:
            ts = to_seconds(s.get("start_time"))
            end_ts = to_seconds(s.get("end_time"))
            duration = end_ts - ts
            sid = f"shot_{int(s['shot_number']):02d}"

            # ğŸ¯ å…³é”®å¸§æå–ï¼šä»åˆ†é•œçš„**ä¸­ç‚¹**æå–ï¼Œè€Œéèµ·å§‹ç‚¹
            # åŸå› ï¼šèµ·å§‹ç‚¹å¯èƒ½æ˜¯è½¬åœºç¬é—´ï¼Œä¸­ç‚¹æ‰æ˜¯è¯¥åˆ†é•œçš„ä»£è¡¨æ€§ç”»é¢
            mid_ts = ts + (duration / 2.0)
            img_out = self.job_dir / "frames" / f"{sid}.png"
            subprocess.run([
                ffmpeg_path, "-y",
                "-i", str(video_path),
                "-ss", str(mid_ts),       # ä»ä¸­ç‚¹æå–ï¼Œç¡®ä¿ç”»é¢ä¸æè¿°ä¸€è‡´
                "-frames:v", "1",
                "-q:v", "2",
                str(img_out)
            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

            # ğŸ¯ ç²¾å‡†è§†é¢‘ç‰‡æ®µåˆ‡å‰²
            video_segment_out = self.job_dir / "source_segments" / f"{sid}.mp4"
            subprocess.run([
                ffmpeg_path, "-y",
                "-i", str(video_path),
                "-ss", str(ts),           # è§†é¢‘ç‰‡æ®µä»èµ·å§‹ç‚¹å¼€å§‹
                "-t", str(duration),
                "-c:v", "libx264",        # é‡æ–°ç¼–ç ä»¥ç¡®ä¿ç²¾å‡†åˆ‡å‰²
                "-c:a", "aac",
                "-avoid_negative_ts", "make_zero",
                str(video_segment_out)
            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    def load(self):
        """åŠ è½½çŠ¶æ€å¹¶å¯¹é½ç‰©ç†æ–‡ä»¶çŠ¶æ€"""
        self.workflow = load_workflow(self.job_dir)
        if "global_stages" not in self.workflow:
            self.workflow["global_stages"] = {"analyze": "SUCCESS", "extract": "SUCCESS", "stylize": "NOT_STARTED", "video_gen": "NOT_STARTED", "merge": "NOT_STARTED"}

        updated = False
        shots = self.workflow.get("shots", [])
        for shot in shots:
            sid = shot.get("shot_id")
            status_node = shot.get("status", {})
            
            # 1. é£æ ¼åŒ–å‚è€ƒå›¾ç‰©ç†å¯¹é½
            stylized_path = self.job_dir / "stylized_frames" / f"{sid}.png"
            if stylized_path.exists() and status_node.get("stylize") != "SUCCESS":
                status_node["stylize"] = "SUCCESS"
                shot["assets"]["stylized_frame"] = f"stylized_frames/{sid}.png"
                updated = True

            # 2. è§†é¢‘äº§ç‰©ç‰©ç†å¯¹é½
            video_output_path = self.job_dir / "videos" / f"{sid}.mp4"
            current_video_status = status_node.get("video_generate")
            if video_output_path.exists() and current_video_status != "SUCCESS":
                status_node["video_generate"] = "SUCCESS"
                shot.setdefault("assets", {})["video"] = f"videos/{sid}.mp4"
                updated = True
            elif not video_output_path.exists() and current_video_status == "SUCCESS":
                status_node["video_generate"] = "NOT_STARTED"
                shot.setdefault("assets", {})["video"] = None
                updated = True
        
        # ğŸ’¡ æ ¸å¿ƒæ–°å¢ï¼šè®¡ç®—åˆå¹¶å°±ç»ªçŠ¶æ€ç»Ÿè®¡
        failed_count = sum(1 for s in shots if s["status"].get("video_generate") == "FAILED")
        pending_count = sum(1 for s in shots if s["status"].get("video_generate") in ["NOT_STARTED", "RUNNING"])
        
        self.workflow["merge_info"] = {
            "can_merge": failed_count == 0 and pending_count == 0 and len(shots) > 0,
            "failed_count": failed_count,
            "pending_count": pending_count,
            "message": ""
        }
        
        if failed_count > 0:
            self.workflow["merge_info"]["message"] = f"âš ï¸ {failed_count} shots failed and cannot be assembled."
        elif pending_count > 0:
            self.workflow["merge_info"]["message"] = "â³ Waiting for the shot list to be generated..."
        elif len(shots) > 0:
            self.workflow["merge_info"]["message"] = "âœ… All shots are ready and can be assembled into the final film."
        
        if updated: self.save()
        return self.workflow

    def save(self):
        self.workflow.setdefault("meta", {})["updated_at"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        save_workflow(self.job_dir, self.workflow)

    def apply_agent_action(self, action: Union[Dict, List]) -> Dict[str, Any]:
        """å¤„ç†ä¿®æ”¹æ„å›¾ï¼šå¼ºåˆ¶é‡ç½®åç»­æ‰€æœ‰ä¾èµ–èŠ‚ç‚¹"""
        actions = action if isinstance(action, list) else [action]
        total_affected = 0
        for act in actions:
            op = act.get("op")
            
            if op == "set_global_style":
                affected = apply_global_style(self.workflow, act.get("value"), cascade=True)
                if affected > 0:
                    for s in self.workflow.get("shots", []):
                        v_path = self.job_dir / "videos" / f"{s['shot_id']}.mp4"
                        if v_path.exists(): os.remove(v_path)
                        i_path = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                        if i_path.exists(): os.remove(i_path)
                        s["status"]["stylize"] = "NOT_STARTED"
                        s["status"]["video_generate"] = "NOT_STARTED"
                        s["assets"]["video"] = None
                        s["assets"]["stylized_frame"] = None
                total_affected += affected
                
            elif op == "global_subject_swap":
                old_subject = act.get("old_subject", "").lower()
                new_subject = act.get("new_subject", "").lower()
                if old_subject and new_subject:
                    for s in self.workflow.get("shots", []):
                        # ğŸï¸ Intelligent Scene Detection: Skip scenery/landscape shots
                        if self._is_scenery_shot(s["description"]):
                            print(f"ğŸï¸ Scenery shot skipped (no character injection): {s['shot_id']}")
                            continue

                        if old_subject in s["description"].lower():
                            desc = s["description"]

                            # ğŸ§¹ STEP 1: STRICT ATTRIBUTE PURGING for gender conflicts
                            # For simple swaps, purge gender-conflicting attributes
                            purged_desc = self._purge_conflicting_attributes(desc, old_subject, {})
                            print(f"ğŸ§¹ Purged conflicting attributes from {s['shot_id']}")

                            # ğŸ” STEP 2: Separate narrative layer from technical tags
                            tag_pattern = r'\[([A-Z]+): ([^\]]+)\]'
                            tags = re.findall(tag_pattern, purged_desc)
                            narrative_part = re.sub(tag_pattern, '', purged_desc).strip()

                            # ğŸ”„ STEP 3: Replace SUBJECT_PLACEHOLDER with new subject
                            if 'SUBJECT_PLACEHOLDER' in narrative_part:
                                # Replace ONLY the first placeholder with the subject
                                new_narrative = narrative_part.replace('SUBJECT_PLACEHOLDER', new_subject, 1)
                                # Replace remaining placeholders with "the [subject]"
                                new_narrative = new_narrative.replace('SUBJECT_PLACEHOLDER', f'the {new_subject}')
                            else:
                                # Fallback: direct replacement
                                new_narrative = re.sub(
                                    rf'\b{re.escape(old_subject)}\b',
                                    new_subject,
                                    narrative_part,
                                    flags=re.IGNORECASE
                                )

                            # ğŸ§¹ STEP 4: Semantic Sanitization for pronouns
                            new_narrative = self._semantic_sanitize_gender(new_narrative, old_subject, new_subject)

                            # ğŸ§¹ STEP 5: Clean up duplicates and grammar
                            new_narrative = re.sub(r',\s*,', ',', new_narrative)
                            new_narrative = re.sub(r'\s{2,}', ' ', new_narrative)
                            new_narrative = new_narrative.strip()
                            if new_narrative:
                                new_narrative = new_narrative[0].upper() + new_narrative[1:]

                            # Reconstruct description: narrative + preserved tags
                            tag_lines = [f"[{tag}: {value}]" for tag, value in tags]
                            s["description"] = new_narrative + ("\n" + "\n".join(tag_lines) if tag_lines else "")

                            s["status"]["stylize"] = "NOT_STARTED"
                            s["status"]["video_generate"] = "NOT_STARTED"
                            v_path = self.job_dir / "videos" / f"{s['shot_id']}.mp4"
                            if v_path.exists(): os.remove(v_path)
                            i_path = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                            if i_path.exists(): os.remove(i_path)
                            s["assets"]["video"] = None
                            s["assets"]["stylized_frame"] = None
                            total_affected += 1
                            print(f"ğŸ§¹ Clean swap applied: {s['shot_id']}")

            elif op == "detailed_subject_swap":
                # ğŸ¨ Fine-Grained Attribute Propagation: Detailed character replacement with visual attributes
                # ğŸ†” Global Identity Anchor: Store and propagate consistent character identity
                old_subject = act.get("old_subject", "").lower()
                new_subject = act.get("new_subject", "").lower()
                attributes = act.get("attributes", {})

                if old_subject and new_subject:
                    # Build the detailed character description from attributes
                    attr_parts = []

                    # Order attributes for natural reading: age, body, hair, eyes, skin, clothing, accessories, other
                    if attributes.get("age_descriptor"):
                        attr_parts.append(attributes["age_descriptor"])
                    if attributes.get("body_type"):
                        attr_parts.append(attributes["body_type"])

                    # Hair description (combine style and color)
                    hair_parts = []
                    if attributes.get("hair_color"):
                        hair_parts.append(attributes["hair_color"])
                    if attributes.get("hair_style"):
                        hair_parts.append(attributes["hair_style"])
                    if hair_parts:
                        attr_parts.append(" ".join(hair_parts) + " hair")

                    if attributes.get("eye_color"):
                        attr_parts.append(f"{attributes['eye_color']} eyes")
                    if attributes.get("skin_tone"):
                        attr_parts.append(f"{attributes['skin_tone']} skin")
                    if attributes.get("facial_features"):
                        attr_parts.append(f"with {attributes['facial_features']}")
                    if attributes.get("clothing"):
                        attr_parts.append(f"wearing {attributes['clothing']}")
                    if attributes.get("accessories"):
                        attr_parts.append(f"with {attributes['accessories']}")
                    if attributes.get("other_visual"):
                        attr_parts.append(attributes["other_visual"])

                    # Construct full character description
                    if attr_parts:
                        # Format: "a [attributes] [subject]" e.g., "a young golden short hair woman wearing red attire"
                        full_character_desc = f"a {' '.join(attr_parts)} {new_subject}"
                    else:
                        full_character_desc = new_subject

                    # ğŸ†” Store Global Identity Anchor in workflow for consistency tracking
                    self.workflow.setdefault("global", {})["identity_anchor"] = {
                        "base_subject": new_subject,
                        "full_description": full_character_desc,
                        "attributes": attributes,
                        "replaced_from": old_subject
                    }
                    print(f"ğŸ†” Global Identity Anchor set: '{full_character_desc}'")

                    shots_modified = 0
                    shots_skipped = 0

                    for s in self.workflow.get("shots", []):
                        # ğŸï¸ Intelligent Scene Detection: Skip scenery/landscape shots
                        if self._is_scenery_shot(s["description"]):
                            print(f"ğŸï¸ Scenery shot preserved (identity not injected): {s['shot_id']}")
                            shots_skipped += 1
                            continue

                        if old_subject in s["description"].lower():
                            desc = s["description"]

                            # ğŸ§¹ STEP 1: STRICT ATTRIBUTE PURGING
                            # Completely remove ALL conflicting descriptors before applying new Visual DNA
                            purged_desc = self._purge_conflicting_attributes(desc, old_subject, attributes)
                            print(f"ğŸ§¹ Purged conflicting attributes from {s['shot_id']}")

                            # ğŸ” STEP 2: Separate narrative layer from technical tags (tags preserved by purge)
                            tag_pattern = r'\[([A-Z]+): ([^\]]+)\]'
                            tags = re.findall(tag_pattern, purged_desc)
                            narrative_part = re.sub(tag_pattern, '', purged_desc).strip()

                            # ğŸ†” STEP 3: Replace SUBJECT_PLACEHOLDER with new identity
                            # The purge method leaves SUBJECT_PLACEHOLDER where the old subject was
                            if 'SUBJECT_PLACEHOLDER' in narrative_part:
                                # Replace ONLY the first placeholder with full description
                                new_narrative = narrative_part.replace('SUBJECT_PLACEHOLDER', full_character_desc, 1)
                                # Replace remaining placeholders with simple pronoun or "the [subject]"
                                new_narrative = new_narrative.replace('SUBJECT_PLACEHOLDER', f'the {new_subject}')
                            else:
                                # Fallback: append identity if placeholder not found
                                new_narrative = f"{full_character_desc}, {narrative_part}" if narrative_part else full_character_desc

                            # ğŸ§¹ STEP 4: Semantic Sanitization for pronouns
                            new_narrative = self._semantic_sanitize_gender(new_narrative, old_subject, new_subject)

                            # ğŸ§¹ STEP 5: Clean up duplicates and grammar
                            # Remove duplicate "a [subject]" patterns that may have been created
                            new_narrative = re.sub(rf'\b(a\s+{re.escape(new_subject)})\s*,\s*a\s+{re.escape(new_subject)}\b', r'\1', new_narrative, flags=re.IGNORECASE)
                            # Remove duplicate consecutive words
                            new_narrative = re.sub(r'\b(\w+)\s+\1\b', r'\1', new_narrative, flags=re.IGNORECASE)
                            # Clean up multiple commas/spaces
                            new_narrative = re.sub(r',\s*,', ',', new_narrative)
                            new_narrative = re.sub(r'\s{2,}', ' ', new_narrative)
                            # Capitalize first letter of sentence
                            new_narrative = new_narrative.strip()
                            if new_narrative:
                                new_narrative = new_narrative[0].upper() + new_narrative[1:]

                            # Reconstruct description: narrative + preserved tags
                            tag_lines = [f"[{tag}: {value}]" for tag, value in tags]
                            s["description"] = new_narrative + ("\n" + "\n".join(tag_lines) if tag_lines else "")

                            # Reset generation status
                            s["status"]["stylize"] = "NOT_STARTED"
                            s["status"]["video_generate"] = "NOT_STARTED"
                            v_path = self.job_dir / "videos" / f"{s['shot_id']}.mp4"
                            if v_path.exists(): os.remove(v_path)
                            i_path = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                            if i_path.exists(): os.remove(i_path)
                            s["assets"]["video"] = None
                            s["assets"]["stylized_frame"] = None
                            shots_modified += 1
                            print(f"ğŸ†” Clean identity applied: {s['shot_id']}")

                    total_affected += shots_modified
                    print(f"ğŸ¨ Identity Anchoring complete: {shots_modified} protagonist shots updated, {shots_skipped} scenery shots preserved")
                            
            elif op == "update_shot_params":
                sid = act.get("shot_id")
                for s in self.workflow.get("shots", []):
                    if s["shot_id"] == sid:
                        if "description" in act: s["description"] = act["description"]
                        s["status"]["stylize"] = "NOT_STARTED"
                        s["status"]["video_generate"] = "NOT_STARTED"
                        v_path = self.job_dir / "videos" / f"{sid}.mp4"
                        if v_path.exists(): os.remove(v_path)
                        i_path = self.job_dir / "stylized_frames" / f"{sid}.png"
                        if i_path.exists(): os.remove(i_path)
                        s["assets"]["video"] = None
                        s["assets"]["stylized_frame"] = None
                        total_affected += 1
                        break

            elif op == "enhance_shot_description":
                # ğŸ“ ç©ºé—´æ„ŸçŸ¥ + ğŸ¬ é£æ ¼å¼ºåŒ–ï¼šå¢å¼ºåˆ†é•œæè¿°
                sid = act.get("shot_id")
                spatial_info = act.get("spatial_info", "")
                style_boost = act.get("style_boost", "")
                for s in self.workflow.get("shots", []):
                    if s["shot_id"] == sid:
                        original_desc = s.get("description", "")
                        enhanced_parts = [original_desc]
                        if spatial_info:
                            enhanced_parts.append(f"[Spatial: {spatial_info}]")
                        if style_boost:
                            enhanced_parts.append(f"[Style: {style_boost}]")
                        s["description"] = " ".join(enhanced_parts)
                        s["status"]["stylize"] = "NOT_STARTED"
                        s["status"]["video_generate"] = "NOT_STARTED"
                        v_path = self.job_dir / "videos" / f"{sid}.mp4"
                        if v_path.exists(): os.remove(v_path)
                        i_path = self.job_dir / "stylized_frames" / f"{sid}.png"
                        if i_path.exists(): os.remove(i_path)
                        s["assets"]["video"] = None
                        s["assets"]["stylized_frame"] = None
                        total_affected += 1
                        print(f"ğŸ“ å¢å¼ºåˆ†é•œæè¿°: {sid} -> {s['description'][:80]}...")
                        break

            elif op == "update_cinematography":
                # ğŸ¬ æ‘„å½±å‚æ•°ä¿®æ”¹ï¼ˆä»…å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚æ—¶ï¼‰
                sid = act.get("shot_id")
                param = act.get("param", "")
                new_value = act.get("value", "")
                valid_params = ["shot_scale", "subject_frame_position", "subject_orientation", "gaze_direction", "motion_vector"]
                if param in valid_params and new_value:
                    for s in self.workflow.get("shots", []):
                        if s["shot_id"] == sid:
                            # Update the cinematography dict
                            s.setdefault("cinematography", {})[param] = new_value

                            # Update the description tags to match
                            tag_map = {
                                "shot_scale": "SCALE",
                                "subject_frame_position": "POSITION",
                                "subject_orientation": "ORIENTATION",
                                "gaze_direction": "GAZE",
                                "motion_vector": "MOTION"
                            }
                            tag_name = tag_map.get(param, param.upper())
                            desc = s.get("description", "")

                            # Replace existing tag or append new one
                            tag_pattern = rf'\[{tag_name}: [^\]]+\]'
                            new_tag = f"[{tag_name}: {new_value}]"
                            if re.search(tag_pattern, desc):
                                desc = re.sub(tag_pattern, new_tag, desc)
                            else:
                                desc = desc + f"\n{new_tag}"
                            s["description"] = desc

                            # Reset generation status
                            s["status"]["stylize"] = "NOT_STARTED"
                            s["status"]["video_generate"] = "NOT_STARTED"
                            v_path = self.job_dir / "videos" / f"{sid}.mp4"
                            if v_path.exists(): os.remove(v_path)
                            i_path = self.job_dir / "stylized_frames" / f"{sid}.png"
                            if i_path.exists(): os.remove(i_path)
                            s["assets"]["video"] = None
                            s["assets"]["stylized_frame"] = None
                            total_affected += 1
                            print(f"ğŸ¬ æ‘„å½±å‚æ•°æ›´æ–°: {sid} [{param}] -> {new_value}")
                            break

        if total_affected > 0: self.save()
        return {"status": "success", "affected_shots": total_affected}

    def run_node(self, node_type: str, shot_id: Optional[str] = None):
        """é€»è¾‘ç¼–æ’å¼•æ“ã€‚ç¡®ä¿â€˜å…ˆæœ‰å›¾ï¼Œåæœ‰è§†é¢‘â€™ä¸”æ— æ­»é”"""
        self.workflow.setdefault("meta", {}).setdefault("attempts", 0)
        self.workflow["meta"]["attempts"] += 1
        
        target_shots = [s for s in self.workflow.get("shots", []) if not shot_id or s["shot_id"] == shot_id]

        if node_type == "video_generate":
            for s in target_shots:
                if s["status"].get("stylize") != "SUCCESS":
                    print(f"ğŸ”— [Dependency] åˆ†é•œ {s['shot_id']} ç¼ºå°‘å®šå¦†å›¾ï¼Œæ­£åœ¨å‰ç½®ç”Ÿæˆ...")
                    run_stylize(self.job_dir, self.workflow, target_shot=s["shot_id"])
                    i_file = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                    if i_file.exists(): 
                        s["status"]["stylize"] = "SUCCESS"
                        s["assets"]["stylized_frame"] = f"stylized_frames/{s['shot_id']}.png"

        stage_key = "video_gen" if node_type == "video_generate" else "stylize"
        self.workflow["global_stages"][stage_key] = "RUNNING"

        for s in target_shots:
            if node_type == "video_generate":
                v_file = self.job_dir / "videos" / f"{s['shot_id']}.mp4"
                if v_file.exists(): os.remove(v_file)
                s["status"]["video_generate"] = "NOT_STARTED" 
                s["assets"]["video"] = None
            elif node_type == "stylize":
                i_file = self.job_dir / "stylized_frames" / f"{s['shot_id']}.png"
                if i_file.exists(): os.remove(i_file)
                s["status"]["stylize"] = "NOT_STARTED" 
                s["assets"]["stylized_frame"] = None

        self.save()

        if node_type == "stylize": 
            run_stylize(self.job_dir, self.workflow, target_shot=shot_id)
        elif node_type == "video_generate": 
            run_video_generate(self.job_dir, self.workflow, target_shot=shot_id)
        
        self.load() 

    def _is_scenery_shot(self, description: str) -> bool:
        """
        ğŸï¸ Intelligent Scene Detection: Determine if a shot is a scenery/landscape shot with no human protagonist.
        Returns True if the shot should be SKIPPED during character replacement.
        """
        desc_lower = description.lower()

        # Human subject indicators - if ANY of these are present, it's NOT a scenery shot
        human_indicators = [
            # Generic human terms
            'man', 'woman', 'person', 'people', 'human', 'figure', 'silhouette',
            'boy', 'girl', 'child', 'kid', 'baby', 'infant', 'toddler',
            'teenager', 'adult', 'elder', 'elderly', 'senior',
            # Relationship terms
            'father', 'mother', 'dad', 'mom', 'parent', 'son', 'daughter',
            'brother', 'sister', 'husband', 'wife', 'couple',
            'friend', 'stranger', 'visitor', 'guest',
            # Professional/role terms
            'worker', 'employee', 'boss', 'doctor', 'nurse', 'teacher', 'student',
            'driver', 'passenger', 'pedestrian', 'commuter',
            'actor', 'actress', 'performer', 'singer', 'dancer',
            'protagonist', 'character', 'hero', 'heroine',
            # Body parts (indicating human presence)
            'face', 'hand', 'hands', 'arm', 'arms', 'leg', 'legs',
            'head', 'body', 'shoulder', 'back', 'chest',
            'eye', 'eyes', 'hair', 'lips', 'mouth', 'nose',
            # Actions that imply human
            'walking', 'running', 'sitting', 'standing', 'talking', 'speaking',
            'looking', 'watching', 'holding', 'carrying', 'wearing',
            # Pronouns
            'he ', 'she ', 'his ', 'her ', 'him ', 'they ', 'their ',
        ]

        # Check for human presence
        has_human = any(indicator in desc_lower for indicator in human_indicators)

        if has_human:
            return False  # Not a scenery shot - has human protagonist

        # Scenery/landscape indicators - if these dominate without humans, it's scenery
        scenery_indicators = [
            # Nature scenes
            'landscape', 'scenery', 'vista', 'panorama', 'horizon',
            'mountain', 'valley', 'forest', 'woods', 'jungle', 'desert',
            'ocean', 'sea', 'lake', 'river', 'waterfall', 'beach', 'coast',
            'sky', 'clouds', 'sunset', 'sunrise', 'dawn', 'dusk', 'night sky',
            'field', 'meadow', 'prairie', 'grassland', 'garden', 'park',
            # Urban scenes without people
            'cityscape', 'skyline', 'building', 'architecture', 'street view',
            'empty room', 'interior', 'exterior', 'establishing shot',
            'aerial view', 'drone shot', 'bird\'s eye',
            # Object focus
            'close-up of object', 'food', 'vehicle', 'car ', 'train ', 'plane ',
            'furniture', 'decoration', 'artifact',
            # Transition/ambient
            'transition', 'time lapse', 'ambient', 'atmosphere', 'mood shot',
        ]

        # Check for scenery dominance
        has_scenery = any(indicator in desc_lower for indicator in scenery_indicators)

        return has_scenery  # Is scenery if indicators present and no humans

    def _semantic_sanitize_gender(self, description: str, old_subject: str, new_subject: str) -> str:
        """
        ğŸ§¹ Semantic Sanitization: Clean gender-conflicting attributes when swapping subjects.
        - Strips incompatible physical attributes (beard, mustache, Adam's apple, etc.)
        - Remaps gendered pronouns (he/him/his â†’ she/her/hers and vice versa)
        """
        # Define gender categories - expanded to catch variations
        male_keywords = {'man', 'men', 'boy', 'boys', 'male', 'males', 'gentleman', 'gentlemen',
                        'guy', 'guys', 'father', 'dad', 'husband', 'brother', 'son', 'uncle',
                        'grandfather', 'protagonist', 'protagonists', 'hero', 'heroes', 'actor'}
        female_keywords = {'woman', 'women', 'girl', 'girls', 'female', 'females', 'lady', 'ladies',
                         'mother', 'mom', 'wife', 'sister', 'daughter', 'aunt', 'grandmother',
                         'heroine', 'heroines', 'actress'}

        # Check if any male/female keyword appears in the subject string
        old_lower = old_subject.lower()
        new_lower = new_subject.lower()

        old_is_male = any(kw in old_lower for kw in male_keywords)
        old_is_female = any(kw in old_lower for kw in female_keywords)
        new_is_male = any(kw in new_lower for kw in male_keywords)
        new_is_female = any(kw in new_lower for kw in female_keywords)

        # Only sanitize if there's a gender change
        if (old_is_male and new_is_female) or (old_is_female and new_is_male):
            if old_is_male and new_is_female:
                # Male â†’ Female: Remove male-specific attributes
                male_attributes = [
                    # Mustache - ALL variations (most comprehensive)
                    r'\bwith\s+(?:a\s+)?(?:\w+\s+)*mustache\b',  # "with a thick mustache", "with a handlebar mustache"
                    r'\bwith\s+(?:a\s+)?(?:\w+\s+)*moustache\b', # British spelling
                    r'\bhas\s+(?:a\s+)?(?:\w+\s+)*mustache\b',   # "has a mustache"
                    r'\bhas\s+(?:a\s+)?(?:\w+\s+)*moustache\b',
                    r'\bhaving\s+(?:a\s+)?(?:\w+\s+)*mustache\b', # "having a mustache"
                    r'\bsporting\s+(?:a\s+)?(?:\w+\s+)*mustache\b', # "sporting a mustache"
                    r'\b(?:his|the|a)\s+(?:\w+\s+)*mustache\b',  # "his thick mustache", "the mustache"
                    r'\b(?:his|the|a)\s+(?:\w+\s+)*moustache\b',
                    r'\b\w+\s+mustache\b',  # "thick mustache", "handlebar mustache"
                    r'\b\w+\s+moustache\b',
                    r'\bmustached\b', r'\bmoustached\b',
                    r'\bmustache\b', r'\bmoustache\b',  # standalone as last resort
                    # Beard - ALL variations
                    r'\bwith\s+(?:a\s+)?(?:\w+\s+)*beard\b',
                    r'\bhas\s+(?:a\s+)?(?:\w+\s+)*beard\b',
                    r'\b(?:his|the|a)\s+(?:\w+\s+)*beard\b',
                    r'\b\w+\s+beard\b',  # "thick beard", "full beard"
                    r'\bbearded\b', r'\bbeard\b',
                    # Goatee
                    r'\bwith\s+(?:a\s+)?(?:\w+\s+)*goatee\b',
                    r'\b(?:his|the|a)\s+(?:\w+\s+)*goatee\b',
                    r'\bgoatee\b',
                    # Stubble
                    r'\bwith\s+(?:\w+\s+)*stubble\b',
                    r'\b\w+\s+stubble\b',  # "five o'clock stubble"
                    r'\bstubbled\b', r'\bstubble\b',
                    # Facial hair general
                    r'\bfacial\s+hair\b', r'\bwith\s+facial\s+hair\b',
                    # Other male-specific
                    r'\bAdam\'s\s+apple\b',
                    r'\bbald\b', r'\bbalding\b',
                    r'\bmuscular\b', r'\bbuff\b',
                    r'\bbroad[- ]shouldered\b',
                    r'\bchest\s+hair\b', r'\bwith\s+chest\s+hair\b',
                ]
                for attr in male_attributes:
                    description = re.sub(attr, '', description, flags=re.IGNORECASE)

                # Remap pronouns: he/him/his â†’ she/her/hers
                description = re.sub(r'\bhe\b', 'she', description, flags=re.IGNORECASE)
                description = re.sub(r'\bhim\b', 'her', description, flags=re.IGNORECASE)
                description = re.sub(r'\bhis\b', 'her', description, flags=re.IGNORECASE)
                description = re.sub(r'\bhimself\b', 'herself', description, flags=re.IGNORECASE)

            elif old_is_female and new_is_male:
                # Female â†’ Male: Remove female-specific attributes
                female_attributes = [
                    r'\b(lipstick)\b', r'\b(makeup|make-up)\b', r'\b(long eyelashes)\b',
                    r'\b(feminine)\b', r'\b(pregnant)\b', r'\b(nursing)\b',
                    r'\b(wearing a dress)\b', r'\b(in a skirt)\b'
                ]
                for attr in female_attributes:
                    description = re.sub(attr, '', description, flags=re.IGNORECASE)

                # Remap pronouns: she/her/hers â†’ he/him/his
                description = re.sub(r'\bshe\b', 'he', description, flags=re.IGNORECASE)
                description = re.sub(r'\bher\b(?!\s+\w+ing)', 'him', description, flags=re.IGNORECASE)  # Avoid "her walking"
                description = re.sub(r'\bhers\b', 'his', description, flags=re.IGNORECASE)
                description = re.sub(r'\bherself\b', 'himself', description, flags=re.IGNORECASE)

            # Clean up any double spaces left from attribute removal
            description = re.sub(r'\s{2,}', ' ', description).strip()

        return description

    def _purge_conflicting_attributes(self, description: str, old_subject: str, new_attributes: Dict) -> str:
        """
        ğŸ§¹ Strict Attribute Purging: Completely remove ALL conflicting descriptors before applying new Visual DNA.
        This ensures a CLEAN transformation with zero residues from the original subject.

        Purge Categories:
        1. Old subject name and variants
        2. Hair descriptions (color, style, length)
        3. Clothing descriptions
        4. Physical features and accessories
        5. Age descriptors
        6. Body type descriptors
        """
        # ğŸ” Separate technical tags from narrative (preserve tags)
        tag_pattern = r'\[([A-Z]+): ([^\]]+)\]'
        tags = re.findall(tag_pattern, description)
        narrative = re.sub(tag_pattern, '', description).strip()

        # ============================================
        # 1ï¸âƒ£ PURGE OLD SUBJECT NAME AND VARIANTS
        # ============================================
        old_lower = old_subject.lower()
        subject_variants = {
            'man': ['man', 'men', 'male', 'gentleman', 'guy', 'fellow', 'dude'],
            'woman': ['woman', 'women', 'female', 'lady', 'girl'],
            'boy': ['boy', 'boys', 'lad', 'young man', 'male child'],
            'girl': ['girl', 'girls', 'lass', 'young woman', 'female child'],
            'child': ['child', 'children', 'kid', 'kids', 'youngster'],
            'person': ['person', 'people', 'individual', 'figure'],
        }

        # Find which category the old subject belongs to
        variants_to_remove = [old_subject]
        for category, variants in subject_variants.items():
            if old_lower in variants or old_lower == category:
                variants_to_remove.extend(variants)
                break

        # Remove old subject and its variants (but keep the position for replacement)
        for variant in set(variants_to_remove):
            # Use word boundary to avoid partial matches
            narrative = re.sub(rf'\ba\s+{re.escape(variant)}\b', 'SUBJECT_PLACEHOLDER', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\bthe\s+{re.escape(variant)}\b', 'SUBJECT_PLACEHOLDER', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\b{re.escape(variant)}\b', 'SUBJECT_PLACEHOLDER', narrative, flags=re.IGNORECASE)

        # ============================================
        # 2ï¸âƒ£ PURGE ALL HAIR DESCRIPTIONS
        # ============================================
        hair_colors = [
            'black', 'brown', 'blonde', 'blond', 'golden', 'silver', 'gray', 'grey',
            'white', 'red', 'auburn', 'ginger', 'brunette', 'chestnut', 'platinum',
            'dark', 'light', 'dirty blonde', 'strawberry blonde', 'jet black',
            'salt and pepper', 'highlighted', 'dyed', 'colored'
        ]
        hair_styles = [
            'short', 'long', 'medium', 'curly', 'straight', 'wavy', 'frizzy',
            'bald', 'balding', 'shaved', 'buzz cut', 'crew cut', 'mohawk',
            'ponytail', 'bun', 'braided', 'braids', 'dreadlocks', 'dreads',
            'afro', 'pixie', 'bob', 'shoulder-length', 'flowing', 'slicked back',
            'messy', 'neat', 'tousled', 'spiky', 'receding', 'thinning',
            'thick', 'fine', 'wispy', 'layered'
        ]

        # Remove hair color + "hair" combinations
        for color in hair_colors:
            narrative = re.sub(rf'\b{color}\s+hair(ed)?\b', '', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\b{color}-hair(ed)?\b', '', narrative, flags=re.IGNORECASE)

        # Remove hair style + "hair" combinations
        for style in hair_styles:
            narrative = re.sub(rf'\b{style}\s+hair(ed)?\b', '', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\b{style}-hair(ed)?\b', '', narrative, flags=re.IGNORECASE)

        # Remove complex hair descriptions
        narrative = re.sub(r'\bwith\s+[\w\s]+\s+hair\b', '', narrative, flags=re.IGNORECASE)
        narrative = re.sub(r'\b[\w\s]+\s+haired\b', '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 3ï¸âƒ£ PURGE ALL CLOTHING DESCRIPTIONS
        # ============================================
        clothing_patterns = [
            r'\bwearing\s+[\w\s,]+(?:shirt|dress|suit|jacket|coat|pants|jeans|skirt|blouse|sweater|hoodie|t-shirt|tee|top|shorts|trousers|uniform|outfit|attire|clothes|clothing|garment)\b',
            r'\bin\s+(?:a\s+)?[\w\s]+(?:shirt|dress|suit|jacket|coat|pants|jeans|skirt|blouse|sweater|hoodie|t-shirt|tee|top|shorts|trousers|uniform|outfit|attire)\b',
            r'\bdressed\s+in\s+[\w\s,]+\b',
            r'\bclad\s+in\s+[\w\s,]+\b',
            # Specific clothing items with colors
            r'\b(?:red|blue|black|white|green|yellow|pink|purple|orange|brown|gray|grey)\s+(?:shirt|dress|suit|jacket|coat|pants|jeans|skirt|blouse|sweater|hoodie|t-shirt|top)\b',
        ]
        for pattern in clothing_patterns:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 4ï¸âƒ£ PURGE PHYSICAL FEATURES & ACCESSORIES
        # ============================================
        # Facial features - comprehensive patterns for mustache/beard/goatee
        facial_features = [
            # Mustache - ALL variations
            r'\bwith\s+(?:a\s+)?(?:\w+\s+)*mustache\b',
            r'\bwith\s+(?:a\s+)?(?:\w+\s+)*moustache\b',
            r'\bhas\s+(?:a\s+)?(?:\w+\s+)*mustache\b',
            r'\bhas\s+(?:a\s+)?(?:\w+\s+)*moustache\b',
            r'\bhaving\s+(?:a\s+)?(?:\w+\s+)*mustache\b',
            r'\bsporting\s+(?:a\s+)?(?:\w+\s+)*mustache\b',
            r'\b(?:his|the|a)\s+(?:\w+\s+)*mustache\b',
            r'\b(?:his|the|a)\s+(?:\w+\s+)*moustache\b',
            r'\b\w+\s+mustache\b',
            r'\b\w+\s+moustache\b',
            r'\bmustached\b', r'\bmoustached\b',
            r'\bmustache\b', r'\bmoustache\b',
            # Beard - ALL variations
            r'\bwith\s+(?:a\s+)?(?:\w+\s+)*beard\b',
            r'\bhas\s+(?:a\s+)?(?:\w+\s+)*beard\b',
            r'\b(?:his|the|a)\s+(?:\w+\s+)*beard\b',
            r'\b\w+\s+beard\b',
            r'\bbearded\b', r'\bbeard\b',
            # Goatee
            r'\bwith\s+(?:a\s+)?(?:\w+\s+)*goatee\b',
            r'\b(?:his|the|a)\s+(?:\w+\s+)*goatee\b',
            r'\bgoatee\b',
            r'\bwith\s+stubble\b', r'\bstubbled\b',
            r'\bwith\s+freckles\b', r'\bfreckled\b',
            r'\bwith\s+(?:a\s+)?scar\b', r'\bscarred\b',
            r'\bwith\s+dimples\b',
            r'\bwith\s+wrinkles\b', r'\bwrinkled\b',
            r'\bwith\s+(?:a\s+)?tattoo\b', r'\btattooed\b',
        ]
        for pattern in facial_features:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # Eye descriptions
        eye_colors = ['blue', 'green', 'brown', 'hazel', 'gray', 'grey', 'black', 'amber', 'violet']
        for color in eye_colors:
            narrative = re.sub(rf'\b{color}\s+eyes?\b', '', narrative, flags=re.IGNORECASE)
            narrative = re.sub(rf'\b{color}-eyed\b', '', narrative, flags=re.IGNORECASE)
        narrative = re.sub(r'\bwith\s+[\w\s]+\s+eyes\b', '', narrative, flags=re.IGNORECASE)

        # Accessories
        accessories = [
            r'\bwearing\s+(?:a\s+)?(?:glasses|sunglasses|spectacles)\b',
            r'\bwith\s+(?:a\s+)?(?:glasses|sunglasses|spectacles)\b',
            r'\bwearing\s+(?:a\s+)?(?:hat|cap|beanie|helmet)\b',
            r'\bwith\s+(?:a\s+)?(?:hat|cap|beanie|helmet)\b',
            r'\bwearing\s+(?:a\s+)?(?:necklace|earrings|bracelet|watch|ring)\b',
            r'\bwith\s+(?:a\s+)?(?:necklace|earrings|bracelet|watch|ring)\b',
            r'\bwearing\s+(?:a\s+)?(?:scarf|tie|bowtie|bow tie)\b',
            r'\bwith\s+(?:a\s+)?(?:scarf|tie|bowtie|bow tie)\b',
        ]
        for pattern in accessories:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 5ï¸âƒ£ PURGE AGE DESCRIPTORS
        # ============================================
        age_patterns = [
            r'\byoung\b', r'\bold\b', r'\belderly\b', r'\bmiddle-aged\b', r'\bmiddle aged\b',
            r'\bteenage\b', r'\bteen\b', r'\badult\b', r'\bsenior\b', r'\bjuvenile\b',
            r'\bin (?:his|her|their) (?:20s|30s|40s|50s|60s|70s|80s|90s|twenties|thirties|forties|fifties|sixties|seventies|eighties|nineties)\b',
        ]
        for pattern in age_patterns:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 6ï¸âƒ£ PURGE BODY TYPE DESCRIPTORS
        # ============================================
        body_patterns = [
            r'\b(?:tall|short|slim|slender|thin|skinny|fat|heavy|overweight|muscular|athletic|petite|stocky|lanky|burly|chubby|plump)\b',
            r'\bwell-built\b', r'\bwell built\b',
            r'\bbroad[- ]shouldered\b',
        ]
        for pattern in body_patterns:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # 7ï¸âƒ£ PURGE SKIN TONE DESCRIPTORS
        # ============================================
        skin_patterns = [
            r'\bfair[- ]skinned\b', r'\bfair skin\b',
            r'\bdark[- ]skinned\b', r'\bdark skin\b',
            r'\bpale[- ]skinned\b', r'\bpale skin\b',
            r'\btan[- ]skinned\b', r'\btanned skin\b', r'\btanned\b',
            r'\bolive[- ]skinned\b', r'\bolive skin\b',
        ]
        for pattern in skin_patterns:
            narrative = re.sub(pattern, '', narrative, flags=re.IGNORECASE)

        # ============================================
        # CLEANUP
        # ============================================
        # Remove orphaned articles and prepositions
        narrative = re.sub(r'\ba\s+,', ',', narrative)
        narrative = re.sub(r'\bthe\s+,', ',', narrative)
        narrative = re.sub(r'\bwith\s+,', ',', narrative)
        narrative = re.sub(r'\bwearing\s+,', ',', narrative)
        narrative = re.sub(r'\bin\s+,', ',', narrative)

        # Remove multiple spaces and clean up
        narrative = re.sub(r'\s{2,}', ' ', narrative)
        narrative = re.sub(r'\s+,', ',', narrative)
        narrative = re.sub(r',\s*,', ',', narrative)
        narrative = re.sub(r'^\s*,\s*', '', narrative)
        narrative = re.sub(r'\s*,\s*$', '', narrative)
        narrative = narrative.strip()

        # Reconstruct with preserved tags
        if tags:
            tag_lines = [f"[{tag}: {value}]" for tag, value in tags]
            return narrative + "\n" + "\n".join(tag_lines)
        return narrative

    def _get_shot_by_id(self, shot_id: str) -> Optional[Dict]:
        for s in self.workflow.get("shots", []):
            if s.get("shot_id") == shot_id: return s
        return None

    def merge_videos(self) -> str:
        """æ‰§è¡Œæ— æŸåˆå¹¶"""
        ffmpeg_path = get_ffmpeg_path()
        success_shots = [s for s in self.workflow.get("shots", []) if s["status"].get("video_generate") == "SUCCESS"]
        if not success_shots: raise RuntimeError("æ²¡æœ‰å¯åˆå¹¶çš„åˆ†é•œè§†é¢‘ã€‚")
        success_shots.sort(key=lambda x: x["shot_id"])
        concat_list_path = self.job_dir / "concat_list.txt"
        output_video_path = self.job_dir / "final_output.mp4"
        with open(concat_list_path, "w", encoding="utf-8") as f:
            for s in success_shots:
                v_rel_path = s["assets"].get("video")
                if v_rel_path:
                    abs_v_path = (self.job_dir / v_rel_path).absolute()
                    f.write(f"file '{abs_v_path}'\n")
        cmd = [ffmpeg_path, "-y", "-f", "concat", "-safe", "0", "-i", str(concat_list_path), "-c", "copy", str(output_video_path)]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0: raise RuntimeError(f"åˆå¹¶å¤±è´¥: {result.stderr}")
        if "global_stages" in self.workflow:
            self.workflow["global_stages"]["merge"] = "SUCCESS"
        self.save()
        return "final_output.mp4"
</file>

<file path="index.html">
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI å¯¼æ¼”å·¥ä½œå° - æœ€ç»ˆäº¤ä»˜ç‰ˆæœ¬</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/react@18/umd/react.development.js"></script>
    <script src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=Inter:wght@400;600;900&display=swap');
        body { font-family: 'Inter', sans-serif; background-color: #020617; color: #f8fafc; overflow: hidden; }
        .glass { background: rgba(15, 23, 42, 0.85); backdrop-filter: blur(12px); border: 1px solid rgba(255,255,255,0.08); }
        .node-line { stroke: #3b82f6; stroke-width: 2; fill: none; stroke-opacity: 0.2; transition: stroke-opacity 0.3s; }
        .node-line-active { stroke: #3b82f6; stroke-width: 3; stroke-dasharray: 8; animation: flow 1s linear infinite; stroke-opacity: 1; }
        @keyframes flow { from { stroke-dashoffset: 16; } to { stroke-dashoffset: 0; } }
        .custom-scrollbar::-webkit-scrollbar { width: 4px; }
        .custom-scrollbar::-webkit-scrollbar-thumb { background: #1e293b; border-radius: 10px; }
        .canvas-bg { background-image: radial-gradient(circle, #1e293b 1.5px, transparent 1.5px); background-size: 30px 30px; }
        .no-select { user-select: none; }
        .sidebar-transition { transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1); }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect, useRef } = React;

        const App = () => {
            const [workflow, setWorkflow] = useState(null);
            const [currentJobId, setCurrentJobId] = useState(null);
            const [viewMode, setViewMode] = useState('grid'); 
            const [messages, setMessages] = useState([{ role: 'ai', text: "Hello, Director. I am your AI Executive Producer ReTake. Please upload a video clip, and I will immediately perform a shot breakdown for you. You can direct me to perform global style reshaping or main character replacement at any time. Let's get started!" }]);
            const [inputText, setInputText] = useState('');
            const [loading, setLoading] = useState(false);
            const [uploading, setUploading] = useState(false);
            const [isAgentVisible, setIsAgentVisible] = useState(true);
            
            const [isEditing, setIsEditing] = useState(false);

            const [transform, setTransform] = useState({ x: 80, y: 50, scale: 0.35 });
            const isDragging = useRef(false);
            const lastMousePos = useRef({ x: 0, y: 0 });

            // ğŸ¯ è‡ªåŠ¨é€‚é…è§†é‡ï¼šåˆ‡æ¢åˆ° Workflow è§†å›¾æ—¶è‡ªåŠ¨å®šä½åˆ°èƒ½çœ‹åˆ°æ‰€æœ‰èŠ‚ç‚¹çš„ä½ç½®
            const fitToView = () => {
                const totalShots = workflow?.shots?.length || 1;
                // æ ¹æ®åˆ†é•œæ•°é‡è®¡ç®—æœ€ä½³ç¼©æ”¾å’Œä½ç½®
                const optimalScale = Math.max(0.15, Math.min(0.4, 1.5 / totalShots));
                const optimalX = 50;
                const optimalY = 50;
                setTransform({ x: optimalX, y: optimalY, scale: optimalScale });
            };

            const fetchWorkflow = async (jobId) => {
                if (isEditing) return;
                try {
                    const targetId = jobId || currentJobId;
                    const url = targetId ? `/api/workflow?job_id=${targetId}` : '/api/workflow';
                    const res = await fetch(url);
                    const data = await res.json();
                    if (data && data.job_id) {
                        setWorkflow(data);
                        setCurrentJobId(data.job_id);
                    }
                } catch (e) { console.error("Update failed", e); }
            };

            useEffect(() => { fetchWorkflow(); }, []);
            useEffect(() => {
                const timer = setInterval(() => { if (currentJobId && !uploading) fetchWorkflow(currentJobId); }, 3000);
                return () => clearInterval(timer);
            }, [currentJobId, uploading, isEditing]);

            const handleWheel = (e) => {
                if (viewMode !== 'graph') return;
                const delta = e.deltaY > 0 ? -0.05 : 0.05;
                setTransform(prev => ({ ...prev, scale: Math.min(Math.max(prev.scale + delta, 0.05), 1.5) }));
            };
            const handleMouseDown = (e) => { if (viewMode === 'graph') { isDragging.current = true; lastMousePos.current = { x: e.clientX, y: e.clientY }; } };
            const handleMouseMove = (e) => {
                if (!isDragging.current || viewMode !== 'graph') return;
                const dx = e.clientX - lastMousePos.current.x;
                const dy = e.clientY - lastMousePos.current.y;
                setTransform(prev => ({ ...prev, x: prev.x + dx, y: prev.y + dy }));
                lastMousePos.current = { x: e.clientX, y: e.clientY };
            };
            const handleMouseUp = () => { isDragging.current = false; };

            const handleFileUpload = async (e) => {
                const file = e.target.files[0];
                if (!file) return;
                setUploading(true);
                const formData = new FormData();
                formData.append('file', file);
                try {
                    const res = await fetch('/api/upload', { method: 'POST', body: formData });
                    const data = await res.json();
                    if (data.status === 'success') {
                        setCurrentJobId(data.job_id);
                        await fetchWorkflow(data.job_id);
                    }
                } catch (e) { alert("ä¸Šä¼ å¤±è´¥"); }
                setUploading(false);
            };

            const sendAgentMsg = async () => {
                if (!inputText.trim()) return;
                const msg = inputText; setInputText('');
                setMessages(prev => [...prev, { role: 'user', text: msg }]);
                setLoading(true);
                await fetch('/api/agent/chat', { 
                    method: 'POST', 
                    headers: {'Content-Type': 'application/json'}, 
                    body: JSON.stringify({ message: msg, job_id: currentJobId }) 
                });
                setLoading(false);
                fetchWorkflow(currentJobId);
            };

            const handleTextChangeLocal = (shotId, newVal) => {
                setWorkflow(prev => ({
                    ...prev,
                    shots: prev.shots.map(s => s.shot_id === shotId ? { ...s, description: newVal } : s)
                }));
            };

            const saveShotUpdate = async (shotId, desc) => {
                setIsEditing(false); 
                await fetch('/api/shot/update', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ shot_id: shotId, description: desc, job_id: currentJobId })
                });
                fetchWorkflow(currentJobId);
            };

            const runTask = async (type, shotId = null) => {
                await fetch(`/api/run/${type}${shotId ? `?shot_id=${shotId}&job_id=${currentJobId}` : `?job_id=${currentJobId}`}`, { method: 'POST' });
            };

            const getAssetUrl = (path) => path ? `/assets/${currentJobId}/${path}` : "";

            // ğŸ¨ Style Header Polish: Strip filler phrases, output clean "STYLE: [Keywords]" format
            const cleanStylePrompt = (rawStyle) => {
                if (!rawStyle) return "Cinematic";
                // Remove common filler phrases
                const fillerPatterns = [
                    /^total\s+transformation\s+(into|to)\s+/i,
                    /^hyper[- ]?stylized\s+(in|as|with)\s+/i,
                    /^complete\s+(visual\s+)?overhaul\s+(with|into|to)\s+/i,
                    /^aggressive\s+/i,
                    /^bold\s+/i,
                    /\s+style$/i,
                    /\s+aesthetic$/i,
                    /,?\s*16:9\s*(cinematic\s*)?(widescreen\s*)?(format)?/i,
                    /,?\s*maintain\s+aspect\s+ratio/i
                ];
                let cleaned = rawStyle;
                fillerPatterns.forEach(pattern => {
                    cleaned = cleaned.replace(pattern, '');
                });
                return cleaned.trim() || "Cinematic";
            };

            const SHOT_BLOCK_HEIGHT = 650; 
            const totalShots = workflow?.shots?.length || 0;
            const masterSourceY = (totalShots * SHOT_BLOCK_HEIGHT) / 2 - 325;
            const finalMergeX = 4200; 

            if (!workflow && !uploading) {
                return (
                    <div className="h-screen flex items-center justify-center p-4 bg-slate-950 text-white">
                        <div className="max-w-xl w-full glass rounded-[3rem] p-12 text-center space-y-8 border-blue-500/20 border-2 shadow-2xl">
                            <h1 className="text-3xl font-black tracking-tighter uppercase italic text-blue-400">Start ReTake Project</h1>
                            <label className="block bg-slate-900 border-2 border-dashed border-slate-700 hover:border-blue-500 p-12 rounded-[2rem] transition-all cursor-pointer">
                                <span className="text-blue-500 font-black uppercase text-sm tracking-widest">Select Video File</span>
                                <input type="file" onChange={handleFileUpload} className="hidden" accept="video/*" />
                            </label>
                        </div>
                    </div>
                );
            }

            if (uploading) return <div className="h-screen flex items-center justify-center bg-slate-950 text-2xl font-black animate-pulse uppercase tracking-tighter text-blue-400">AI Deconstructing...</div>;

            return (
                <div className="h-screen flex flex-col p-4 gap-4 overflow-hidden text-white relative">
                    {/* Header */}
                    <div className="glass rounded-2xl p-4 flex items-center justify-between border border-white/5 z-40">
                        <div className="flex items-center gap-6">
                            <h1 className="text-xl font-bold tracking-tight bg-gradient-to-r from-blue-400 to-emerald-400 bg-clip-text text-transparent italic uppercase pr-4">ReTake Workstation</h1>
                            <div className="flex bg-slate-900 rounded-xl p-1 border border-white/10 shadow-inner">
                                <button onClick={() => setViewMode('grid')} className={`px-5 py-1.5 rounded-lg text-xs font-bold transition ${viewMode === 'grid' ? 'bg-blue-600 text-white shadow-lg' : 'text-slate-500'}`}>STORYBOARD</button>
                                <button onClick={() => { setViewMode('graph'); setTimeout(fitToView, 100); }} className={`px-5 py-1.5 rounded-lg text-xs font-bold transition ${viewMode === 'graph' ? 'bg-blue-600 text-white shadow-lg' : 'text-slate-500'}`}>WORKFLOW CANVAS</button>
                            </div>
                            {viewMode === 'graph' && (
                                <button onClick={fitToView} className="ml-2 px-3 py-1.5 rounded-lg text-xs font-bold bg-slate-800 text-slate-400 hover:text-white border border-white/10 transition">FIT VIEW</button>
                            )}
                        </div>
                        <div className="flex gap-4">
                            <button onClick={() => { setWorkflow(null); setCurrentJobId(null); }} className="text-[10px] font-black border border-white/10 px-4 py-2 rounded-xl text-slate-500 hover:text-white uppercase tracking-widest">+ New Project</button>
                            <button onClick={() => runTask('video_generate')} className="bg-emerald-600 hover:bg-emerald-500 px-6 py-2 rounded-xl text-sm font-black shadow-lg uppercase transition-all">Execute All</button>
                        </div>
                    </div>

                    <div className="flex flex-1 gap-4 overflow-hidden relative">
                        {/* ç”»å¸ƒåŒºåŸŸ */}
                        <div 
                            className={`flex-1 glass rounded-[3rem] overflow-hidden relative border border-white/10 no-select ${viewMode === 'graph' ? 'canvas-bg' : ''}`}
                            onWheel={handleWheel} onMouseDown={handleMouseDown} onMouseMove={handleMouseMove} onMouseUp={() => isDragging.current = false} onMouseLeave={() => isDragging.current = false}
                        >
                            {viewMode === 'grid' ? (
                                <div className="h-full overflow-y-auto p-10 custom-scrollbar">
                                    <div className="grid grid-cols-1 xl:grid-cols-2 gap-10">
                                        {workflow.shots?.map(shot => {
                                            const isSuccess = shot.status?.video_generate === 'SUCCESS';
                                            return (
                                                <div key={shot.shot_id} className="glass rounded-[2rem] border border-white/5 p-8 bg-slate-950/20 relative shadow-xl">
                                                    <div className="flex justify-between items-center mb-6 px-2">
                                                        <span className="font-mono text-xs text-blue-400 font-bold uppercase tracking-widest">{shot.shot_id}</span>
                                                        {/* ğŸ’¡ ä¿®æ­£ï¼šç§»é™¤ pr-40ï¼Œå°†æ—¶é—´æ ‡æ³¨é å³å¯¹é½ */}
                                                        <span className="text-[10px] text-slate-500 font-mono uppercase tracking-tighter">REF TIME: {shot.start_time}s</span>
                                                    </div>
                                                    <div className="grid grid-cols-3 gap-6">
                                                        <div className="space-y-3">
                                                            <div className="relative aspect-video rounded-2xl overflow-hidden border border-white/10 bg-black">
                                                                <img src={getAssetUrl(shot.assets.first_frame)} className="w-full h-full object-cover" />
                                                                <div className="absolute top-3 left-3 bg-red-500 text-[8px] px-2 py-0.5 font-black rounded uppercase text-white">SOURCE IMAGE</div>
                                                            </div>
                                                            <p className="text-[10px] text-center text-slate-500 uppercase tracking-widest font-bold">SOURCE FRAME</p>
                                                        </div>
                                                        <div className="space-y-3">
                                                            <div className="relative aspect-video rounded-2xl overflow-hidden border border-purple-500/30 bg-slate-900 shadow-[0_0_20px_rgba(168,85,247,0.1)]">
                                                                {shot.assets.stylized_frame ? <img src={getAssetUrl(shot.assets.stylized_frame)} className="w-full h-full object-cover" /> : <div className={`h-full flex items-center justify-center text-[10px] italic uppercase font-black animate-pulse ${shot.status?.stylize === 'RUNNING' ? 'text-purple-400' : 'text-slate-700'}`}>{shot.status?.stylize === 'RUNNING' ? 'RUNNING' : 'NOT STARTED'}</div>}
                                                                <div className="absolute top-3 left-3 bg-purple-600 text-[8px] px-2 py-0.5 font-black rounded uppercase text-white">STYLIZED DNA</div>
                                                                <button onClick={() => runTask('stylize', shot.shot_id)} className="absolute bottom-3 right-3 glass p-2 rounded-lg hover:bg-white/10 transition-colors"><svg className="w-3 h-3 text-purple-400" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="M4 4v5h.582m15.356 2A8.001 8.001 0 004.582 9m0 0H9m11 11v-5h-.581m0 0a8.003 8.003 0 01-15.357-2m15.357 2H15" strokeWidth="3"/></svg></button>
                                                            </div>
                                                            <p className="text-[10px] text-center text-purple-400 uppercase tracking-widest font-bold">Style Frame</p>
                                                        </div>
                                                        <div className="space-y-3">
                                                            <div className={`relative aspect-video rounded-2xl overflow-hidden border ${isSuccess ? 'border-emerald-500/40 shadow-xl' : 'border-blue-500/20 bg-slate-900'}`}>
                                                                {isSuccess ? <video src={`${getAssetUrl(shot.assets.video)}?t=${Date.now()}`} className="w-full h-full object-cover" controls autoPlay muted loop /> : <div className={`h-full flex items-center justify-center text-[10px] italic uppercase font-black animate-pulse ${shot.status?.video_generate === 'RUNNING' ? 'text-blue-400' : 'text-slate-700'}`}>{shot.status?.video_generate === 'RUNNING' ? 'RUNNING' : 'NOT STARTED'}</div>}
                                                                <div className="absolute top-3 left-3 bg-blue-500 text-[8px] px-2 py-0.5 font-black rounded shadow-lg uppercase text-white">AI OUTPUT</div>
                                                            </div>
                                                            <p className="text-[10px] text-center text-blue-400 uppercase tracking-widest font-bold">Final Take</p>
                                                        </div>
                                                    </div>
                                                    <div className="mt-8 p-5 bg-slate-900/50 rounded-2xl border border-white/5 relative">
                                                        <div className="mb-2 text-[9px] text-slate-500 uppercase tracking-tighter font-bold">STYLE: <span className="text-purple-400 font-mono">{cleanStylePrompt(workflow.global?.style_prompt)}</span></div>
                                                        <textarea 
                                                            className="w-full bg-transparent border-none text-[13px] text-slate-300 outline-none h-20 resize-none italic font-medium leading-relaxed" 
                                                            value={shot.description} 
                                                            onFocus={() => setIsEditing(true)}
                                                            onChange={(e) => handleTextChangeLocal(shot.shot_id, e.target.value)}
                                                            onBlur={(e) => saveShotUpdate(shot.shot_id, e.target.value)} 
                                                        />
                                                        <div className="flex justify-end mt-2"><button onClick={() => runTask('video_generate', shot.shot_id)} className="text-[10px] text-emerald-400 font-black uppercase tracking-widest hover:text-emerald-300 transition-all border border-emerald-900/20 px-4 py-1.5 rounded-full shadow-2xl">Execute Re-Draw</button></div>
                                                    </div>
                                                </div>
                                            );
                                        })}
                                    </div>
                                </div>
                            ) : (
                                <div className="absolute inset-0 origin-top-left transition-transform duration-75" style={{ transform: `translate(${transform.x}px, ${transform.y}px) scale(${transform.scale})` }}>
                                    <svg className="absolute inset-0 w-[10000px] h-[10000px] pointer-events-none z-10">
                                        {workflow.shots?.map((shot, i) => (
                                            <React.Fragment key={i}>
                                                <path d={`M 430 ${masterSourceY + 185} C 530 ${masterSourceY + 185}, 530 ${200 + i * SHOT_BLOCK_HEIGHT}, 630 ${200 + i * SHOT_BLOCK_HEIGHT}`} className={`node-line ${shot.status?.video_generate === 'RUNNING' ? 'node-line-active' : ''}`} />
                                                <path d={`M ${finalMergeX - 100} ${200 + i * SHOT_BLOCK_HEIGHT} C ${finalMergeX - 250} ${200 + i * SHOT_BLOCK_HEIGHT}, ${finalMergeX - 250} ${masterSourceY + 185}, ${finalMergeX} ${masterSourceY + 185}`} className="node-line" style={{ strokeOpacity: 0.1 }} />
                                            </React.Fragment>
                                        ))}
                                    </svg>
                                    
                                    <div className="absolute left-20 w-[410px] glass p-10 rounded-[4rem] border-2 border-red-500/40 shadow-2xl" style={{ top: `${masterSourceY}px` }}>
                                        <div className="absolute -right-3 top-1/2 -translate-y-1/2 w-8 h-8 bg-red-500 rounded-full border-4 border-slate-950 shadow-[0_0_20px_#ef4444]"></div>
                                        <h3 className="text-xs font-black text-red-500 mb-6 uppercase tracking-widest underline decoration-red-900">Root: Master Source Video</h3>
                                        <div className="aspect-video bg-black rounded-[2.5rem] overflow-hidden border border-white/10 shadow-3xl text-white">
                                            <video src={getAssetUrl('input.mp4')} className="w-full h-full object-cover" controls muted loop />
                                        </div>
                                    </div>

                                    <div className="absolute left-[630px] top-0 flex flex-col gap-40 py-20">
                                        {workflow.shots?.map((shot, i) => {
                                            const isImgReady = !!shot.assets.stylized_frame;
                                            const isVidReady = shot.status?.video_generate === 'SUCCESS';
                                            return (
                                                <div key={shot.shot_id} className="flex items-center gap-12 text-white">
                                                    
                                                    <div className="w-[600px] glass p-10 rounded-[3.5rem] border border-white/10 relative shadow-2xl">
                                                        <div className="absolute -left-3 top-1/2 -translate-y-1/2 w-6 h-6 bg-blue-500 rounded-full border-4 border-slate-950"></div>
                                                        <div className="text-blue-400 font-black text-[10px] mb-4 uppercase tracking-widest text-white">Input Segment #{i+1}</div>
                                                        <div className="flex gap-6">
                                                            <div className="w-48 h-32 rounded-2xl bg-black overflow-hidden border border-white/5 shadow-inner">
                                                                <img src={getAssetUrl(shot.assets.first_frame)} className="w-full h-full object-cover opacity-80" />
                                                            </div>
                                                            <div className="flex-1">
                                                                <span className="text-[10px] text-slate-500 font-bold uppercase tracking-widest opacity-60 italic">Prompt Content</span>
                                                                <textarea 
                                                                    className="w-full bg-slate-950/50 border border-white/10 rounded-2xl p-4 text-sm text-slate-200 h-32 mt-4 outline-none focus:border-blue-500/40 transition-all font-medium leading-relaxed shadow-inner" 
                                                                    value={shot.description} 
                                                                    onFocus={() => setIsEditing(true)}
                                                                    onChange={(e) => handleTextChangeLocal(shot.shot_id, e.target.value)}
                                                                    onBlur={(e) => saveShotUpdate(shot.shot_id, e.target.value)} 
                                                                />
                                                            </div>
                                                        </div>
                                                    </div>

                                                    <div className="text-4xl text-slate-800 font-black italic select-none">â†’</div>

                                                    <div className="flex flex-col gap-4">
                                                        <div className="w-44 glass p-4 rounded-2xl border border-purple-500/30 flex flex-col items-center">
                                                            <span className="text-[7px] text-slate-500 uppercase font-black mb-1">STYLE</span>
                                                            <p className="text-[9px] text-purple-400 font-mono font-bold truncate w-full text-center">{cleanStylePrompt(workflow.global?.style_prompt)}</p>
                                                        </div>
                                                        <div className="w-44 glass p-4 rounded-2xl border border-white/10 flex flex-col items-center opacity-40">
                                                            <span className="text-[7px] text-slate-500 uppercase font-black mb-1">Image Model</span>
                                                            <p className="text-[9px] text-slate-300 font-bold uppercase tracking-widest">Imagen 4.0</p>
                                                        </div>
                                                    </div>

                                                    <div className="text-4xl text-slate-800 font-black italic select-none">â†’</div>

                                                    <div className={`w-[350px] glass p-4 rounded-[2.5rem] border ${isImgReady ? 'border-purple-500/50' : 'border-white/5 opacity-30'}`}>
                                                        <div className="text-[8px] text-purple-500 font-black uppercase mb-2 tracking-widest text-center">C. Stylized DNA</div>
                                                        <div className="aspect-video bg-black rounded-2xl overflow-hidden border border-white/10 relative">
                                                            {isImgReady ? <img src={getAssetUrl(shot.assets.stylized_frame)} className="w-full h-full object-cover" /> : <div className={`h-full flex items-center justify-center text-[10px] italic uppercase font-black tracking-widest ${shot.status?.stylize === 'RUNNING' ? 'text-purple-400 animate-pulse' : 'text-slate-700'}`}>{shot.status?.stylize === 'RUNNING' ? 'RUNNING' : 'NOT STARTED'}</div>}
                                                        </div>
                                                    </div>

                                                    <div className="text-4xl text-slate-800 font-black italic select-none">â†’</div>

                                                    <div className="w-32 glass p-4 rounded-2xl border border-white/10 flex flex-col items-center opacity-30">
                                                        <span className="text-[7px] text-slate-500 uppercase font-black mb-1">Vid Model</span>
                                                        <p className="text-[9px] text-slate-300 font-bold uppercase font-mono">Veo 3.1</p>
                                                    </div>

                                                    <div className="text-4xl text-slate-800 font-black italic select-none">â†’</div>

                                                    <div className={`w-[700px] glass p-4 rounded-[4rem] border transition-all duration-700 shadow-3xl ${isVidReady ? 'border-emerald-500/50 bg-emerald-500/5' : 'border-white/5 opacity-20'}`}>
                                                        <div className="aspect-video bg-black rounded-[3rem] overflow-hidden relative shadow-inner">
                                                            {isVidReady && shot.assets.video ? <video src={`${getAssetUrl(shot.assets.video)}?t=${Date.now()}`} className="w-full h-full object-cover shadow-2xl" autoPlay muted loop controls /> : <div className={`h-full flex items-center justify-center text-[10px] italic uppercase font-black tracking-widest ${shot.status?.video_generate === 'RUNNING' ? 'text-blue-400 animate-pulse' : 'text-slate-700'}`}>{shot.status?.video_generate === 'RUNNING' ? 'RUNNING' : 'NOT STARTED'}</div>}
                                                        </div>
                                                    </div>
                                                </div>
                                            );
                                        })}
                                    </div>

                                    <div className="absolute w-[800px] glass p-14 rounded-[6rem] border-2 border-emerald-500/40 shadow-[0_0_120px_rgba(16,185,129,0.2)] z-20 text-white" style={{ left: `${finalMergeX}px`, top: `${masterSourceY - 150}px` }}>
                                        <div className="absolute -left-4 top-1/2 -translate-y-1/2 w-10 h-10 bg-emerald-500 rounded-full border-4 border-slate-950 shadow-[0_0_30px_#10b981]"></div>
                                        <div className="flex justify-between items-center mb-8 px-4">
                                            <div className="flex flex-col gap-2">
                                                <h3 className="text-sm font-black text-emerald-400 uppercase tracking-[0.3em]">Final Assembly Export</h3>
                                                <p className="text-[10px] font-mono text-red-400 italic">{workflow.merge_info?.message}</p>
                                            </div>
                                            <button 
                                                disabled={!workflow.merge_info?.can_merge}
                                                onClick={() => runTask('merge')} 
                                                className={`px-10 py-3 rounded-full text-xs font-black uppercase tracking-widest shadow-lg transition-all active:scale-95 ${workflow.merge_info?.can_merge ? 'bg-emerald-600 text-white hover:bg-emerald-500' : 'bg-slate-800 text-slate-500 cursor-not-allowed'}`}
                                            >
                                                {workflow.global_stages?.merge === 'SUCCESS' ? 'RE-ASSEMBLE' : 'Merge All'}
                                            </button>
                                        </div>
                                        <div className="aspect-video bg-black rounded-[4rem] overflow-hidden border border-white/10 shadow-3xl relative">
                                            {workflow.global_stages?.merge === 'SUCCESS' ? <video src={getAssetUrl('final_output.mp4')} className="w-full h-full object-cover" controls autoPlay muted loop /> : <div className="h-full flex flex-col items-center justify-center text-[10px] text-slate-700 font-mono italic p-12 text-center uppercase tracking-[0.6em]">Awaiting segments success...</div>}
                                        </div>
                                    </div>
                                </div>
                            )}
                        </div>

                        {/* Agent Sidebar */}
                        <div className={`sidebar-transition flex flex-col glass rounded-[3rem] overflow-hidden border border-white/10 shadow-2xl relative ${isAgentVisible ? 'w-96 ml-4' : 'w-0 border-none ml-0'}`}>
                            {isAgentVisible && (
                                <div className="flex flex-col h-full text-white">
                                    <div className="p-6 border-b border-white/5 bg-white/5 flex items-center justify-between">
                                        <h2 className="text-xs font-black uppercase text-emerald-400 tracking-widest">Executive Agent</h2>
                                        <button 
                                            onClick={() => setIsAgentVisible(false)} 
                                            className="text-slate-400 hover:text-white transition-colors"
                                        >
                                            <svg className="w-5 h-5" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                                                <path d="M4 14h6v6M10 14l-6 6M20 10h-6V4M14 10l6-6" />
                                            </svg>
                                        </button>
                                    </div>
                                    <div className="flex-1 overflow-y-auto p-6 space-y-6 custom-scrollbar text-xs leading-relaxed text-slate-300">
                                        {messages.map((m, i) => (
                                            <div key={i} className={`flex ${m.role === 'user' ? 'justify-end' : 'justify-start'}`}>
                                                <div className={`max-w-[90%] p-4 rounded-3xl ${m.role === 'user' ? 'bg-blue-600 text-white rounded-tr-none shadow-lg' : 'bg-slate-800 text-slate-200 border border-white/5 shadow-xl'}`}>{m.text}</div>
                                            </div>
                                        ))}
                                    </div>
                                    <div className="p-6 bg-slate-900/80 border-t border-white/10 text-white">
                                        <div className="flex gap-3">
                                            <input value={inputText} onChange={e => setInputText(e.target.value)} onKeyDown={e => e.key === 'Enter' && sendAgentMsg()} placeholder="Command..." className="flex-1 bg-slate-950 border border-white/10 rounded-2xl px-5 py-3 text-xs outline-none focus:border-blue-500 text-white" />
                                            <button onClick={sendAgentMsg} className="bg-blue-600 hover:bg-blue-500 px-5 py-3 rounded-2xl font-black text-xs shadow-lg">Send</button>
                                        </div>
                                    </div>
                                </div>
                            )}
                        </div>
                        
                        {!isAgentVisible && (
                            <button 
                                onClick={() => setIsAgentVisible(true)} 
                                className="fixed bottom-10 right-10 flex items-center gap-3 bg-[#020617] text-white px-5 py-3 rounded-full shadow-[0_0_25px_rgba(59,130,246,0.4)] hover:shadow-[0_0_35px_rgba(16,185,129,0.5)] hover:scale-105 transition-all z-50 group border border-white/20"
                            >
                                <div className="w-8 h-8 rounded-full bg-slate-200 overflow-hidden border border-white/10">
                                    <div className="w-full h-full bg-gradient-to-br from-blue-500 to-emerald-500 flex items-center justify-center text-white text-[10px] font-black italic">RE</div>
                                </div>
                                <div className="flex flex-col items-start leading-tight">
                                    <span className="text-[10px] text-slate-400 font-bold uppercase">Chat</span>
                                    <span className="text-sm font-black text-white">Executive Agent</span>
                                </div>
                            </button>
                        )}
                    </div>
                </div>
            );
        };

        const root = ReactDOM.createRoot(document.getElementById('root'));
        root.render(<App />);
    </script>
</body>
</html>
</file>

<file path="core/runner.py">
# core/runner.py
from pathlib import Path
import shutil
import subprocess
import time
import os
import requests
import io
from PIL import Image

from .workflow_io import save_workflow, load_workflow
from .utils import get_ffmpeg_path


def ensure_videos_dir(job_dir: Path) -> Path:
    videos_dir = job_dir / "videos"
    videos_dir.mkdir(parents=True, exist_ok=True)
    return videos_dir


def ai_stylize_frame(job_dir: Path, wf: dict, shot: dict) -> str:
    """
    ğŸ’¡ ä½¿ç”¨ Imagen 4.0 æˆ– Gemini 2.0 Image Gen ç¡®ä¿å®šå¦†å›¾ç”ŸæˆæˆåŠŸ
    ğŸ¬ Cinematography Fidelity: Hard-coded enforcement of source shot parameters
    """
    from google import genai
    from google.genai import types

    api_key = os.getenv("GEMINI_API_KEY")
    client = genai.Client(api_key=api_key, http_options={'api_version': 'v1beta'})

    src = job_dir / shot["assets"]["first_frame"]
    dst = job_dir / "stylized_frames" / f"{shot['shot_id']}.png"
    dst.parent.mkdir(parents=True, exist_ok=True)

    if dst.exists(): os.remove(dst)

    global_style = wf.get("global", {}).get("style_prompt", "Cinematic")
    description = shot.get("description", "")

    # ğŸ¬ Extract cinematography parameters for fidelity enforcement
    cinema = shot.get("cinematography", {})
    shot_scale = cinema.get("shot_scale", "")
    subject_position = cinema.get("subject_frame_position", "")
    subject_orientation = cinema.get("subject_orientation", "")
    gaze_direction = cinema.get("gaze_direction", "")
    motion_vector = cinema.get("motion_vector", "")

    # ğŸ¯ Build cinematography constraint block
    cinema_constraints = []

    # 1ï¸âƒ£ Shot Scale Mapping
    scale_instructions = {
        "EXTREME_WIDE": "EXTREME WIDE SHOT - Subject very small in frame, vast environment dominates",
        "WIDE": "WIDE SHOT - Full body visible, significant environment context",
        "MEDIUM_WIDE": "MEDIUM WIDE SHOT - Subject from knees up, environmental context",
        "MEDIUM": "MEDIUM SHOT - Subject from waist up, balanced framing",
        "MEDIUM_CLOSE": "MEDIUM CLOSE-UP - Subject from chest up, intimate but contextual",
        "CLOSE_UP": "CLOSE-UP - Face fills most of frame, minimal background",
        "EXTREME_CLOSE_UP": "EXTREME CLOSE-UP - Single feature (eyes, lips) fills frame"
    }
    if shot_scale and shot_scale in scale_instructions:
        cinema_constraints.append(f"ğŸ“ SHOT SCALE: {scale_instructions[shot_scale]}")

    # 2ï¸âƒ£ Subject Position in Frame
    if subject_position:
        cinema_constraints.append(f"ğŸ“ FRAME POSITION: Subject MUST be positioned at {subject_position} of the 16:9 frame")

    # 3ï¸âƒ£ Orientation & Facing
    if subject_orientation:
        orientation_map = {
            "facing-camera": "Subject facing directly toward camera (frontal view)",
            "back-to-camera": "Subject's back facing camera (rear view)",
            "profile-left": "Subject in left profile (nose pointing to frame left)",
            "profile-right": "Subject in right profile (nose pointing to frame right)",
            "three-quarter-left": "Subject in 3/4 view facing left (showing right side of face)",
            "three-quarter-right": "Subject in 3/4 view facing right (showing left side of face)"
        }
        orient_desc = orientation_map.get(subject_orientation, subject_orientation)
        cinema_constraints.append(f"ğŸ§­ BODY ORIENTATION: {orient_desc}")

    # 4ï¸âƒ£ Gaze Direction
    if gaze_direction:
        gaze_map = {
            "looking-at-camera": "Eyes looking directly into camera lens",
            "looking-left": "Eyes directed toward the left side of frame",
            "looking-right": "Eyes directed toward the right side of frame",
            "looking-up": "Eyes directed upward",
            "looking-down": "Eyes directed downward",
            "looking-off-screen-left": "Eyes looking past the left edge of frame",
            "looking-off-screen-right": "Eyes looking past the right edge of frame"
        }
        gaze_desc = gaze_map.get(gaze_direction, gaze_direction)
        cinema_constraints.append(f"ğŸ‘ï¸ GAZE DIRECTION: {gaze_desc}")

    # 5ï¸âƒ£ Motion Vector
    if motion_vector and motion_vector != "static":
        cinema_constraints.append(f"ğŸƒ MOTION VECTOR: Capture mid-action of '{motion_vector}' - body pose and motion blur should indicate this movement")

    # Build final constraint string
    cinematography_block = ""
    if cinema_constraints:
        cinematography_block = "\n\nğŸ¬ CINEMATOGRAPHY FIDELITY - MANDATORY CONSTRAINTS (from source shot):\n" + "\n".join(cinema_constraints) + "\nâš ï¸ These parameters are LOCKED and must be preserved exactly as specified."

    # ğŸ¨ Conditional Design Elements: Only trigger graphic layouts if explicitly requested
    design_keywords = ['poster', 'layout', 'magazine', 'border', 'collage', 'graphic design', 'storyboard paper']
    style_lower = global_style.lower()
    is_design_style = any(kw in style_lower for kw in design_keywords)

    if is_design_style:
        # User explicitly requested a design/layout style
        prompt = f"""STYLIZED GRAPHIC DESIGN COMPOSITION.
Create a {global_style} layout with intentional design elements.
Subject: {description}.
Style: {global_style} - Apply graphic design aesthetics as requested.
Format: 16:9 aspect ratio with artistic layout elements.{cinematography_block}"""
    else:
        # ğŸ¬ DEFAULT: Full-bleed cinematic film still using structured prompt format
        # Format: [Subject], [Action/Pose], [Environment], [Style & Atmosphere], [Lighting & Color], [Camera & Tech Specs]

        # Extract action/pose from motion vector
        action_pose = motion_vector if motion_vector and motion_vector != "static" else "in a natural pose"

        # Build structured prompt components
        subject_block = f"[SUBJECT]: {description}"
        action_block = f"[ACTION/POSE]: {action_pose}, captured mid-motion with dynamic energy"
        environment_block = "[ENVIRONMENT]: Immersive scene environment extending to all edges of the 16:9 frame, rich background details"
        style_block = f"[STYLE & ATMOSPHERE]: {global_style} aesthetic, visually striking, enhanced visual impact with refined details and textures"
        lighting_block = "[LIGHTING & COLOR]: Dramatic cinematic lighting, rich color grading, depth through light and shadow layers, volumetric atmosphere"
        tech_block = "[CAMERA & TECH]: 35mm cinematic lens, 8K ultra high resolution, shallow depth of field, natural bokeh, film grain texture"

        prompt = f"""PROFESSIONAL CINEMATIC FILM STILL - TEXT-TO-IMAGE GENERATION

{subject_block}
{action_block}
{environment_block}
{style_block}
{lighting_block}
{tech_block}
{cinematography_block}

COMPOSITION RULES:
- Full-bleed edge-to-edge rendering filling 100% of the 16:9 canvas
- ZERO borders, margins, or white space - render as if captured from cinema camera sensor
- Subject photographed as cinematic scene, NOT shrunk into centered box
- Professional cinematography with rule of thirds and depth of field
- ALL cinematography constraints above MUST be strictly followed

QUALITY ENHANCEMENT:
- More visually impactful than standard output
- Rich detail textures and refined material quality
- Dramatic light/shadow interplay for depth
- Cinematic color palette with professional grading

FORBIDDEN:
- Any white/black borders or margins
- Changing shot scale, subject position, orientation, or gaze from source
- Poster layouts, magazine compositions, or storyboard aesthetics
- Any graphic design elements unless explicitly in style prompt

--ar 16:9"""

    print(f"ï¸  AI æ­£åœ¨å°è¯•ç”Ÿæˆå®šå¦†å›¾: {shot['shot_id']}")

    try:
        print(f"ğŸ“¡ å°è¯•è°ƒç”¨ Imagen 4.0 (models/imagen-4.0-generate-001)...")
        response = client.models.generate_images(
            model="models/imagen-4.0-generate-001",
            prompt=prompt,
            config=types.GenerateImagesConfig(
                number_of_images=1,
                aspect_ratio="16:9"
            )
        )
        if response.generated_images:
            gen_img = response.generated_images[0]
            if hasattr(gen_img.image, 'save'):
                gen_img.image.save(dst)
            else:
                with open(dst, 'wb') as f: f.write(gen_img.image.image_bytes)
            print(f"âœ… ä½¿ç”¨ Imagen 4.0 ç”ŸæˆæˆåŠŸï¼")
            return f"stylized_frames/{dst.name}"
    except Exception as e:
        print(f"âš ï¸ Imagen 4.0 è°ƒç”¨å¤±è´¥: {str(e)[:100]}...")

    try:
        print(f"ğŸ“¡ å°è¯•è°ƒç”¨é›†æˆç”Ÿå›¾æ¨¡å‹ (models/gemini-2.0-flash-exp-image-generation)...")
        response = client.models.generate_content(
            model="models/gemini-2.0-flash-exp-image-generation",
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=["IMAGE"]
            )
        )
        for part in response.parts:
            if part.inline_data is not None:
                img = Image.open(io.BytesIO(part.inline_data.data))
                img.save(dst)
                print(f"âœ… ä½¿ç”¨ Gemini 2.0 é›†æˆæ¨¡å‹ç”ŸæˆæˆåŠŸï¼")
                return f"stylized_frames/{dst.name}"
    except Exception as e:
        print(f"âŒ æ‰€æœ‰ç”Ÿå›¾æ¨¡å‹å‡å¤±è´¥: {str(e)[:100]}...")

    print("âš ï¸ æ‰§è¡ŒåŸå›¾å ä½ã€‚")
    shutil.copyfile(src, dst)
    return f"stylized_frames/{dst.name}"


def mock_generate_video(job_dir: Path, shot: dict) -> str:
    videos_dir = ensure_videos_dir(job_dir)
    out_path = videos_dir / f"{shot['shot_id']}.mp4"
    if out_path.exists(): os.remove(out_path)
    src_video = job_dir / "input.mp4"
    ffmpeg = get_ffmpeg_path()
    cmd = [ffmpeg, "-y", "-i", str(src_video), "-t", "1.0", "-c", "copy", str(out_path)]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return f"videos/{out_path.name}"


def veo_generate_video(job_dir: Path, wf: dict, shot: dict) -> str:
    from google import genai
    from google.genai import types

    api_key = os.getenv("GEMINI_API_KEY")
    # ä½¿ç”¨ä¸ video_generator.py ç›¸åŒçš„å®¢æˆ·ç«¯åˆå§‹åŒ–æ–¹å¼
    client = genai.Client(api_key=api_key)

    videos_dir = ensure_videos_dir(job_dir)
    out_path = videos_dir / f"{shot['shot_id']}.mp4"
    if out_path.exists(): os.remove(out_path)

    img_rel = shot.get("assets", {}).get("stylized_frame") or f"stylized_frames/{shot['shot_id']}.png"
    img_path = job_dir / img_rel

    if not img_path.exists():
        ai_stylize_frame(job_dir, wf, shot)

    print(f"ğŸš€ [Veo 3.1] æ­£åœ¨æ¸²æŸ“åˆ†é•œè§†é¢‘: {shot['shot_id']}")

    image_bytes = img_path.read_bytes()
    description = shot.get('description', '')
    style = wf.get('global', {}).get('style_prompt', '')

    # ğŸ¬ Extract cinematography parameters for video fidelity
    cinema = shot.get("cinematography", {})
    shot_scale = cinema.get("shot_scale", "")
    subject_position = cinema.get("subject_frame_position", "")
    subject_orientation = cinema.get("subject_orientation", "")
    gaze_direction = cinema.get("gaze_direction", "")
    motion_vector = cinema.get("motion_vector", "static")

    # Build video-specific cinematography constraints
    video_constraints = []
    if shot_scale:
        video_constraints.append(f"Maintain {shot_scale} framing throughout")
    if subject_position:
        video_constraints.append(f"Subject stays at {subject_position} of frame")
    if subject_orientation:
        video_constraints.append(f"Subject maintains {subject_orientation} body angle")
    if gaze_direction:
        video_constraints.append(f"Gaze direction: {gaze_direction}")

    constraints_str = ". ".join(video_constraints) if video_constraints else ""

    # ğŸ¬ Structured Image-to-Video Prompt Format
    # Format: [Camera Movement], [Specific Action], [Physics Details], [Atmosphere Change]

    # Determine camera movement based on motion vector
    if motion_vector and motion_vector != "static":
        if "walking" in motion_vector or "running" in motion_vector:
            camera_movement = "subtle tracking shot following subject movement"
        elif "toward" in motion_vector:
            camera_movement = "gentle dolly back as subject approaches"
        elif "away" in motion_vector:
            camera_movement = "slow push in as subject recedes"
        else:
            camera_movement = "steady shot with minimal camera drift"
        specific_action = f"Subject performs: {motion_vector}"
    else:
        camera_movement = "locked static shot with subtle breathing movement"
        specific_action = "Subject maintains pose with natural micro-movements (breathing, blinking, subtle weight shifts)"

    # Physics details for realism
    physics_details = "natural physics: hair/fabric responds to movement, ambient particles float in light beams, subtle environmental motion (leaves, dust, reflections)"

    # Atmosphere continuity
    atmosphere_change = f"maintain {style} atmosphere throughout, consistent lighting evolution, seamless style continuity"

    prompt = f"""PROFESSIONAL IMAGE-TO-VIDEO GENERATION - 3-5 SECOND CINEMATIC CLIP

[CAMERA MOVEMENT]: {camera_movement}
[SPECIFIC ACTION]: {specific_action}
[PHYSICS DETAILS]: {physics_details}
[ATMOSPHERE]: {atmosphere_change}

SCENE CONTEXT: {description}
ART STYLE: {style} - Maintain CONSISTENT style across ALL frames

ğŸ¬ CINEMATOGRAPHY LOCK (from source shot - DO NOT CHANGE):
{constraints_str}

MOTION QUALITY REQUIREMENTS:
- High motion quality, cinematic fluidity
- Smooth interpolation between frames
- Subject position and composition MUST remain STABLE
- No sudden flips, mirror effects, or jarring camera changes
- Preserve exact shot scale and framing from reference image

PHYSICS ENHANCEMENT:
- Realistic material physics (cloth flow, hair dynamics)
- Environmental interaction (wind effects, light particles)
- Natural motion blur on moving elements
- Atmospheric depth continuity

CRITICAL: Cinematography parameters are LOCKED - preserve exactly as specified.
high motion quality, cinematic, professional cinematography"""

    # ğŸ”„ è‡ªæ„ˆå¼é‡è¯•é€»è¾‘ï¼šé‡åˆ° 429 é”™è¯¯æ—¶è‡ªåŠ¨ç­‰å¾…å¹¶é‡è¯•
    max_retries = 3
    retry_wait_seconds = 60

    for attempt in range(max_retries):
        try:
            # image ä½œä¸ºç‹¬ç«‹å‚æ•°ä¼ é€’ï¼Œä¸åœ¨ config å†…
            operation = client.models.generate_videos(
                model="veo-3.1-generate-preview",
                prompt=prompt,
                image=types.Image(
                    image_bytes=image_bytes,
                    mime_type="image/png"
                ),
                config=types.GenerateVideosConfig(
                    aspect_ratio="16:9"
                )
            )

            print(f"â³ è§†é¢‘æ­£åœ¨äº‘ç«¯æ¸²æŸ“ (Operation ID: {operation.name})")

            poll_count = 0
            max_polls = 60  # 20 minutes max
            while not operation.done:
                poll_count += 1
                if poll_count > max_polls:
                    raise RuntimeError(f"Veo è½®è¯¢è¶…æ—¶: å·²ç­‰å¾…è¶…è¿‡ 20 åˆ†é’Ÿ")
                print(f"â³ è§†é¢‘æ¸²æŸ“ä¸­... (è½®è¯¢ {poll_count})")
                time.sleep(20)
                operation = client.operations.get(operation)

            # æ£€æŸ¥é”™è¯¯
            if operation.error:
                raise RuntimeError(f"Veo åç«¯æŠ¥é”™: {operation.error}")

            # æ£€æŸ¥ç»“æœ
            if not operation.result or not operation.result.generated_videos:
                raise RuntimeError("Veo ä»»åŠ¡å®Œæˆä½†æœªè¿”å›è§†é¢‘æ•°æ®ã€‚åŸå› ï¼šå¯èƒ½è§¦å‘äº†å†…å®¹å®‰å…¨å®¡æ ¸æ‹¦æˆªã€‚")

            generated_video = operation.result.generated_videos[0]

            # ä¼˜å…ˆä½¿ç”¨ SDK åŸç”Ÿ save æ–¹æ³•
            try:
                generated_video.video.save(str(out_path))
                print(f"ğŸ’¾ è§†é¢‘ç”ŸæˆæˆåŠŸ (SDK save): {out_path}")
                return f"videos/{out_path.name}"
            except Exception as save_err:
                print(f"âš ï¸ SDK save å¤±è´¥ ({save_err})ï¼Œå°è¯•æ‰‹åŠ¨ä¸‹è½½...")

            # å¤‡ç”¨ï¼šæ‰‹åŠ¨ä¸‹è½½
            file_id = None
            video_obj = generated_video.video if hasattr(generated_video, 'video') else generated_video

            if hasattr(video_obj, 'name') and video_obj.name:
                file_id = video_obj.name if "/" in video_obj.name else f"files/{video_obj.name}"
            elif hasattr(video_obj, 'uri') and video_obj.uri:
                file_id = f"files/{video_obj.uri.split('/')[-1]}"

            if not file_id:
                raise RuntimeError(f"æ— æ³•ä»å“åº”ä¸­è§£ææœ‰æ•ˆçš„ File ID: {type(video_obj).__name__}")

            # é˜²å¾¡æ€§ä¿®å¤ï¼šfile_id å¯èƒ½è‡ªå¸¦ ?alt=media æˆ– ?key=...
            clean_file_id = file_id.split("?", 1)[0]

            print(f"âœ… ç”ŸæˆæˆåŠŸï¼Œæ­£åœ¨ä¸‹è½½æ–‡ä»¶: {clean_file_id}")

            download_url = f"https://generativelanguage.googleapis.com/v1beta/{clean_file_id}"
            query_params = {
                "alt": "media",
                "key": api_key,
            }

            response = requests.get(
                download_url,
                params=query_params,
                stream=True,
            )

            if response.status_code == 200:
                with open(out_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=1024*1024): f.write(chunk)
                print(f"ğŸ’¾ è§†é¢‘ç”ŸæˆæˆåŠŸ (æ‰‹åŠ¨ä¸‹è½½): {out_path}")
                return f"videos/{out_path.name}"
            else:
                raise RuntimeError(f"ä¸‹è½½å¤±è´¥: çŠ¶æ€ç  {response.status_code}")

        except Exception as e:
            error_str = str(e).lower()
            is_rate_limit = "429" in error_str or "rate" in error_str or "quota" in error_str or "resource_exhausted" in error_str

            if is_rate_limit and attempt < max_retries - 1:
                wait_time = retry_wait_seconds * (attempt + 1)  # é€’å¢ç­‰å¾…æ—¶é—´
                print(f"âš ï¸ è§¦å‘ RPM é™åˆ¶ (429)ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ({attempt + 1}/{max_retries})...")
                time.sleep(wait_time)
                continue
            else:
                print(f"âŒ Veo å¤±è´¥: {str(e)}")
                raise e

    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    raise RuntimeError(f"Veo ç”Ÿæˆå¤±è´¥ï¼šå·²é‡è¯• {max_retries} æ¬¡")


def run_stylize(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    shots_to_process = []
    for shot in wf.get("shots", []):
        sid = shot.get("shot_id")
        if target_shot and sid != target_shot: continue
        status = shot.get("status", {}).get("stylize", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"): continue
        shots_to_process.append(shot)

    for idx, shot in enumerate(shots_to_process):
        sid = shot.get("shot_id")

        # ğŸš¦ RPM é™æµï¼šæ‰¹é‡æ‰§è¡Œæ—¶ï¼Œæ¯ä¸ªåˆ†é•œä¹‹é—´ä¼‘çœ  35 ç§’
        if idx > 0 and target_shot is None:
            print(f"â³ RPM é™æµï¼šç­‰å¾… 35 ç§’åå¤„ç†ä¸‹ä¸€ä¸ªåˆ†é•œ...")
            time.sleep(35)

        shot.setdefault("status", {})["stylize"] = "RUNNING"
        save_workflow(job_dir, wf)
        try:
            rel_path = ai_stylize_frame(job_dir, wf, shot)
            shot.setdefault("assets", {})["stylized_frame"] = rel_path
            shot["status"]["stylize"] = "SUCCESS"
            print(f"âœ… Stylize SUCCESS: {sid}")
        except Exception as e:
            shot["status"]["stylize"] = "FAILED"
            shot.setdefault("errors", {})["stylize"] = str(e)
        save_workflow(job_dir, wf)


def run_video_generate(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    shots_to_process = []
    for shot in wf.get("shots", []):
        sid = shot.get("shot_id")
        if target_shot and sid != target_shot: continue
        status = shot.get("status", {}).get("video_generate", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"): continue
        shots_to_process.append(shot)

    for idx, shot in enumerate(shots_to_process):
        sid = shot.get("shot_id")

        # ğŸš¦ RPM é™æµï¼šæ‰¹é‡æ‰§è¡Œæ—¶ï¼Œæ¯ä¸ªåˆ†é•œä¹‹é—´ä¼‘çœ  35 ç§’
        if idx > 0 and target_shot is None:
            print(f"â³ RPM é™æµï¼šç­‰å¾… 35 ç§’åå¤„ç†ä¸‹ä¸€ä¸ªåˆ†é•œ...")
            time.sleep(35)

        shot.setdefault("status", {})["video_generate"] = "RUNNING"
        save_workflow(job_dir, wf)
        try:
            video_model = wf.get("global", {}).get("video_model", "mock")
            if video_model == "veo":
                rel_video_path = veo_generate_video(job_dir, wf, shot)
            else:
                rel_video_path = mock_generate_video(job_dir, shot)
            shot.setdefault("assets", {})["video"] = rel_video_path
            shot["status"]["video_generate"] = "SUCCESS"
            print(f"âœ… Video SUCCESS: {sid}")
        except Exception as e:
            shot["status"]["video_generate"] = "FAILED"
            shot.setdefault("errors", {})["video_generate"] = str(e)
            print(f"âŒ Video FAILED: {sid} -> {e}")
        save_workflow(job_dir, wf)


def run_pipeline(job_dir: Path, target_shot: str | None = None) -> None:
    wf = load_workflow(job_dir)
    run_stylize(job_dir, wf, target_shot=target_shot)
    wf = load_workflow(job_dir)
    run_video_generate(job_dir, wf, target_shot=target_shot)
</file>

</files>
