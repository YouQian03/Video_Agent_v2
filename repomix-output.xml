This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/*.mp4, **/*.png, **/*.jpg, **/venv/**, **/.venv/**, **/__pycache__/**, **/.git/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
core/
  __init__.py
  changes.py
  runner.py
  workflow_io.py
jobs/
  demo_job_001/
    storyboard.json
    workflow.json
outputs/
  storyboard.json
analyze_video.py
apply_changes.py
build_workflow.py
extract_frames.py
run_workflow.py
smoke_test_core.py
stylize_frames.py
test_gemini.py
video_generator.py
workflow_cli.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="core/__init__.py">

</file>

<file path="core/changes.py">
from typing import Optional

def apply_global_style(wf: dict, new_style_prompt: str, cascade: bool = True) -> int:
    wf.setdefault("global", {})["style_prompt"] = new_style_prompt

    affected = 0
    if cascade:
        for shot in wf.get("shots", []):
            shot.setdefault("status", {})["stylize"] = "NOT_STARTED"
            shot.setdefault("status", {})["video_generate"] = "NOT_STARTED"
            affected += 1
    return affected

def replace_entity_reference(wf: dict, entity_id: str, new_ref_image: str) -> int:
    entities = wf.setdefault("entities", {})
    if entity_id not in entities:
        raise KeyError(f"entity ä¸å­˜åœ¨ï¼š{entity_id}")

    entities[entity_id]["reference_image"] = new_ref_image

    affected = 0
    for shot in wf.get("shots", []):
        shot_entities = shot.get("entities", [])
        if entity_id in shot_entities:
            shot.setdefault("status", {})["stylize"] = "NOT_STARTED"
            shot.setdefault("status", {})["video_generate"] = "NOT_STARTED"
            affected += 1
    return affected
</file>

<file path="core/workflow_io.py">
import json
from pathlib import Path

def load_workflow(job_dir: Path) -> dict:
    wf_path = job_dir / "workflow.json"
    return json.loads(wf_path.read_text(encoding="utf-8"))

def save_workflow(job_dir: Path, wf: dict) -> None:
    wf_path = job_dir / "workflow.json"
    wf_path.write_text(
        json.dumps(wf, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
</file>

<file path="jobs/demo_job_001/storyboard.json">
[
  {
    "shot_number": 1,
    "frame_description": "A medium shot of a small, fluffy, light-brown dog lying on a wooden floor. Part of a blue water bottle is visible on the left edge of the frame.",
    "content_analysis": "The focus is on the dog. The environment is domestic.",
    "start_time": 0.0,
    "end_time": 1.67,
    "duration_seconds": 1.67,
    "shot_type": "Medium Shot",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Shallow Depth of Field, focus on the dog",
    "lighting": "Natural indoor lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 2,
    "frame_description": "A wide shot of a park with children playing near a small body of water. There are buildings and trees in the background, shot during the daytime.",
    "content_analysis": "The scene depicts a peaceful park setting with recreational activities.",
    "start_time": 1.67,
    "end_time": 4.34,
    "duration_seconds": 2.67,
    "shot_type": "Wide Shot",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, everything in the shot is reasonably sharp",
    "lighting": "Daylight",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 3,
    "frame_description": "A wide shot of a dark hill at night with some lights visible in the distance.",
    "content_analysis": "The shot conveys a sense of darkness and a distant cityscape.",
    "start_time": 4.34,
    "end_time": 5.45,
    "duration_seconds": 1.11,
    "shot_type": "Wide Shot",
    "camera_angle": "Low Angle",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, the hill and distant city are both clear",
    "lighting": "Night time with limited artificial lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 4,
    "frame_description": "A wide shot of a beach at sunset with silhouettes of people walking along the shore. Tall buildings are visible in the background.",
    "content_analysis": "The scene showcases a coastal environment during twilight.",
    "start_time": 5.45,
    "end_time": 6.97,
    "duration_seconds": 1.52,
    "shot_type": "Wide Shot",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, buildings and people are visible",
    "lighting": "Sunset",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 5,
    "frame_description": "A low angle shot of a grassy hill under a clear blue sky.",
    "content_analysis": "The shot focuses on natural landscape elements.",
    "start_time": 6.97,
    "end_time": 8.18,
    "duration_seconds": 1.21,
    "shot_type": "Low Angle Shot",
    "camera_angle": "Low Angle",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, grass and sky",
    "lighting": "Daylight",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 6,
    "frame_description": "A view looking upwards towards buildings, a street lamp and sky. Cloudy sky partially obscuring buildings.",
    "content_analysis": "The scene shows buildings and a sky view in an urban area.",
    "start_time": 8.18,
    "end_time": 9.41,
    "duration_seconds": 1.23,
    "shot_type": "Low Angle Shot",
    "camera_angle": "Low Angle",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, building and sky are clear",
    "lighting": "Cloudy daylight",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 7,
    "frame_description": "A top-down shot looking down a staircase with a shadow of a person going down.",
    "content_analysis": "The view shows the perspective of going down the stairs.",
    "start_time": 9.41,
    "end_time": 10.67,
    "duration_seconds": 1.26,
    "shot_type": "High Angle Shot",
    "camera_angle": "High Angle",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, stairs and shadow are clear",
    "lighting": "Artificial lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 8,
    "frame_description": "Close up shot of a lizard on a white wall.",
    "content_analysis": "The focus is on a lizard.",
    "start_time": 10.67,
    "end_time": 12.05,
    "duration_seconds": 1.38,
    "shot_type": "Close-up",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Shallow depth of field, focus on the lizard",
    "lighting": "Artificial lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 9,
    "frame_description": "A medium shot of a small, fluffy, light-brown dog lying on a wooden floor. Part of a blue water bottle is visible on the left edge of the frame.",
    "content_analysis": "The focus is on the dog. The environment is domestic.",
    "start_time": 12.05,
    "end_time": 13.43,
    "duration_seconds": 1.38,
    "shot_type": "Medium Shot",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Shallow Depth of Field, focus on the dog",
    "lighting": "Natural indoor lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  }
]
</file>

<file path="outputs/storyboard.json">
[
  {
    "shot_number": 1,
    "frame_description": "A medium shot of a small, fluffy, light-brown dog lying on a wooden floor. Part of a blue water bottle is visible on the left edge of the frame.",
    "content_analysis": "The focus is on the dog. The environment is domestic.",
    "start_time": 0.0,
    "end_time": 1.67,
    "duration_seconds": 1.67,
    "shot_type": "Medium Shot",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Shallow Depth of Field, focus on the dog",
    "lighting": "Natural indoor lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 2,
    "frame_description": "A wide shot of a park with children playing near a small body of water. There are buildings and trees in the background, shot during the daytime.",
    "content_analysis": "The scene depicts a peaceful park setting with recreational activities.",
    "start_time": 1.67,
    "end_time": 4.34,
    "duration_seconds": 2.67,
    "shot_type": "Wide Shot",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, everything in the shot is reasonably sharp",
    "lighting": "Daylight",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 3,
    "frame_description": "A wide shot of a dark hill at night with some lights visible in the distance.",
    "content_analysis": "The shot conveys a sense of darkness and a distant cityscape.",
    "start_time": 4.34,
    "end_time": 5.45,
    "duration_seconds": 1.11,
    "shot_type": "Wide Shot",
    "camera_angle": "Low Angle",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, the hill and distant city are both clear",
    "lighting": "Night time with limited artificial lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 4,
    "frame_description": "A wide shot of a beach at sunset with silhouettes of people walking along the shore. Tall buildings are visible in the background.",
    "content_analysis": "The scene showcases a coastal environment during twilight.",
    "start_time": 5.45,
    "end_time": 6.97,
    "duration_seconds": 1.52,
    "shot_type": "Wide Shot",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, buildings and people are visible",
    "lighting": "Sunset",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 5,
    "frame_description": "A low angle shot of a grassy hill under a clear blue sky.",
    "content_analysis": "The shot focuses on natural landscape elements.",
    "start_time": 6.97,
    "end_time": 8.18,
    "duration_seconds": 1.21,
    "shot_type": "Low Angle Shot",
    "camera_angle": "Low Angle",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, grass and sky",
    "lighting": "Daylight",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 6,
    "frame_description": "A view looking upwards towards buildings, a street lamp and sky. Cloudy sky partially obscuring buildings.",
    "content_analysis": "The scene shows buildings and a sky view in an urban area.",
    "start_time": 8.18,
    "end_time": 9.41,
    "duration_seconds": 1.23,
    "shot_type": "Low Angle Shot",
    "camera_angle": "Low Angle",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, building and sky are clear",
    "lighting": "Cloudy daylight",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 7,
    "frame_description": "A top-down shot looking down a staircase with a shadow of a person going down.",
    "content_analysis": "The view shows the perspective of going down the stairs.",
    "start_time": 9.41,
    "end_time": 10.67,
    "duration_seconds": 1.26,
    "shot_type": "High Angle Shot",
    "camera_angle": "High Angle",
    "camera_movement": "Static",
    "focus_and_depth": "Deep focus, stairs and shadow are clear",
    "lighting": "Artificial lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 8,
    "frame_description": "Close up shot of a lizard on a white wall.",
    "content_analysis": "The focus is on a lizard.",
    "start_time": 10.67,
    "end_time": 12.05,
    "duration_seconds": 1.38,
    "shot_type": "Close-up",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Shallow depth of field, focus on the lizard",
    "lighting": "Artificial lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  },
  {
    "shot_number": 9,
    "frame_description": "A medium shot of a small, fluffy, light-brown dog lying on a wooden floor. Part of a blue water bottle is visible on the left edge of the frame.",
    "content_analysis": "The focus is on the dog. The environment is domestic.",
    "start_time": 12.05,
    "end_time": 13.43,
    "duration_seconds": 1.38,
    "shot_type": "Medium Shot",
    "camera_angle": "Eye-Level",
    "camera_movement": "Static",
    "focus_and_depth": "Shallow Depth of Field, focus on the dog",
    "lighting": "Natural indoor lighting",
    "music_and_sound": "Song playing in background",
    "voiceover": null
  }
]
</file>

<file path="analyze_video.py">
import os
import json
import time
from pathlib import Path
from google import genai

PROJECT_DIR = Path(__file__).parent
VIDEO_PATH = PROJECT_DIR / "downloads" / "input.mp4"
OUT_PATH = PROJECT_DIR / "outputs" / "storyboard.json"

DIRECTOR_METAPROMPT = r"""
è¯·ä½ æ‰®æ¼”ä¸€ä½ä¸“ä¸šçš„å½±è§†åˆ†é•œåˆ†æå¸ˆï¼Œä¸“æ³¨ä»¥ã€Œç”»é¢å˜åŒ–ã€ä¸ºæ ¸å¿ƒï¼Œ
å°†åˆšæ‰çš„è§†é¢‘æ‹†è§£ä¸ºè¯¦ç»†çš„åˆ†é•œè¡¨ã€‚
ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚
æ ¹å…ƒç´ ä¸ºåŒ…å«å¤šä¸ªåˆ†é•œå¯¹è±¡çš„JSONæ•°ç»„ã€‚
æ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
shot_number, frame_description, content_analysis,
start_time, end_time, duration_seconds,
shot_type, camera_angle, camera_movement,
focus_and_depth, lighting, music_and_sound, voiceoverã€‚
æ— ä¿¡æ¯è¯·å¡« nullã€‚
ä»…è¾“å‡ºçº¯ JSONã€‚
""".strip()


def ensure_api_key() -> str:
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError(
            "æ²¡æœ‰æ£€æµ‹åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚\n"
            "è¯·åœ¨å½“å‰ç»ˆç«¯æ‰§è¡Œï¼š\n"
            '  export GEMINI_API_KEY="ä½ çš„key"\n'
            "ç„¶åå†è¿è¡Œæœ¬è„šæœ¬ã€‚"
        )
    return api_key


def extract_json_array(text: str):
    if not text:
        raise ValueError("æ¨¡å‹æ²¡æœ‰è¿”å›æ–‡æœ¬ï¼ˆresponse.text ä¸ºç©ºï¼‰ã€‚")

    s = text.strip()
    if s.startswith("[") and s.endswith("]"):
        return json.loads(s)

    l = s.find("[")
    r = s.rfind("]")
    if l != -1 and r != -1 and r > l:
        return json.loads(s[l : r + 1])

    raise ValueError(
        "æœªèƒ½ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSON æ•°ç»„ã€‚\n"
        "è¯·æŠŠæ¨¡å‹åŸå§‹è¾“å‡ºå¤åˆ¶å‡ºæ¥æ£€æŸ¥ï¼ˆå®ƒå¯èƒ½æ²¡æœ‰æŒ‰è¦æ±‚è¾“å‡º JSONï¼‰ã€‚"
    )


def wait_until_file_active(client: genai.Client, file_obj, timeout_s: int = 120, poll_s: int = 2):
    """
    Files API ä¸Šä¼ åï¼Œæ–‡ä»¶å¯èƒ½å¤„äº PROCESSING çŠ¶æ€ï¼Œå¿…é¡»ç­‰åˆ° ACTIVE æ‰èƒ½ä½¿ç”¨ã€‚
    """
    file_name = getattr(file_obj, "name", None)
    if not file_name:
        # æå°‘æ•°æƒ…å†µä¸‹å¯¹è±¡ç»“æ„ä¸åŒï¼Œç›´æ¥è¿”å›è¯•è¯•
        return file_obj

    start = time.time()
    last_state = None

    while True:
        f = client.files.get(name=file_name)
        state = getattr(f, "state", None)

        if state != last_state:
            print(f"æ–‡ä»¶çŠ¶æ€ï¼š{state}")
            last_state = state

        if state == "ACTIVE":
            return f

        if time.time() - start > timeout_s:
            raise TimeoutError(
                f"ç­‰å¾…æ–‡ä»¶å˜ä¸º ACTIVE è¶…æ—¶ï¼ˆ>{timeout_s}sï¼‰ã€‚"
                "ä½ å¯ä»¥é‡è¯•ä¸€æ¬¡ï¼Œæˆ–è€…æ¢æ›´å°/æ›´å¸¸è§ç¼–ç çš„ mp4ã€‚"
            )

        time.sleep(poll_s)


def main():
    api_key = ensure_api_key()

    if not VIDEO_PATH.exists():
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶ï¼š{VIDEO_PATH}\n"
            "è¯·ç¡®è®¤ä½ å·²æŠŠè§†é¢‘æ”¾åˆ° downloads/ é‡Œï¼Œå¹¶å‘½åä¸º input.mp4"
        )

    size_mb = VIDEO_PATH.stat().st_size / (1024 * 1024)
    print(f"å‡†å¤‡å¤„ç†è§†é¢‘ï¼š{VIDEO_PATH.name} ({size_mb:.1f} MB)")

    client = genai.Client(api_key=api_key)

    print("å¼€å§‹ä¸Šä¼ è§†é¢‘åˆ° Files APIâ€¦")
    uploaded = client.files.upload(file=str(VIDEO_PATH))
    print(f"âœ… ä¸Šä¼ å®Œæˆï¼š{uploaded.name}")

    print("ç­‰å¾…æ–‡ä»¶å˜ä¸º ACTIVEâ€¦")
    video_file = wait_until_file_active(client, uploaded, timeout_s=180, poll_s=2)
    print("âœ… æ–‡ä»¶å·² ACTIVEï¼Œå¯ä»¥å¼€å§‹åˆ†æ")

    print("å¼€å§‹åˆ†æè§†é¢‘ï¼ˆç”Ÿæˆåˆ†é•œ JSONï¼‰â€¦")
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=[DIRECTOR_METAPROMPT, video_file],
    )

    raw_text = getattr(response, "text", None) or ""
    storyboard = extract_json_array(raw_text)

    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    OUT_PATH.write_text(
        json.dumps(storyboard, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    print(f"\nâœ… å·²ç”Ÿæˆåˆ†é•œ JSONï¼š{OUT_PATH}")
    print(f"åˆ†é•œæ•°é‡ï¼š{len(storyboard)}")


if __name__ == "__main__":
    main()
</file>

<file path="apply_changes.py">
import json
import argparse
from pathlib import Path

PROJECT_DIR = Path(__file__).parent
DEFAULT_JOB_ID = "demo_job_001"

def load_workflow(job_dir: Path) -> dict:
    return json.loads((job_dir / "workflow.json").read_text(encoding="utf-8"))

def save_workflow(job_dir: Path, wf: dict) -> None:
    (job_dir / "workflow.json").write_text(json.dumps(wf, ensure_ascii=False, indent=2), encoding="utf-8")

def apply_global_style(wf: dict, new_style_prompt: str, cascade: bool = True) -> int:
    """
    ä¿®æ”¹å…¨å±€é£æ ¼ï¼Œå¹¶çº§è”ä½¿ç›¸å…³èŠ‚ç‚¹éœ€è¦é‡è·‘ã€‚
    cascade=Trueï¼šæŠŠæ‰€æœ‰ shots çš„ stylize & video_generate é‡ç½®ä¸º NOT_STARTED
    è¿”å›ï¼šå—å½±å“ shots æ•°é‡
    """
    wf.setdefault("global", {})["style_prompt"] = new_style_prompt

    affected = 0
    if cascade:
        for shot in wf.get("shots", []):
            # é£æ ¼æ”¹äº†ï¼Œé£æ ¼åŒ–å›¾å°±åº”è¯¥é‡æ–°ç”Ÿæˆï¼ˆçœŸå®äº§å“é‡Œå¯èƒ½æœ‰ç¼“å­˜ç­–ç•¥ï¼Œdemo å…ˆå…¨é‡è·‘ï¼‰
            shot.setdefault("status", {})["stylize"] = "NOT_STARTED"
            shot.setdefault("status", {})["video_generate"] = "NOT_STARTED"
            affected += 1
    return affected

def replace_entity_reference(wf: dict, entity_id: str, new_ref_image: str) -> int:
    """
    æ›¿æ¢æŸä¸ª entity çš„ reference_imageï¼Œå¹¶åªå½±å“å¼•ç”¨å®ƒçš„ shotsï¼š
    - æ ‡è®° stylize / video_generate ä¸º NOT_STARTED
    è¿”å›ï¼šå—å½±å“ shots æ•°é‡
    """
    entities = wf.setdefault("entities", {})
    if entity_id not in entities:
        raise KeyError(f"entity ä¸å­˜åœ¨ï¼š{entity_id}")

    entities[entity_id]["reference_image"] = new_ref_image

    affected = 0
    for shot in wf.get("shots", []):
        shot_entities = shot.get("entities", [])
        if entity_id in shot_entities:
            shot.setdefault("status", {})["stylize"] = "NOT_STARTED"
            shot.setdefault("status", {})["video_generate"] = "NOT_STARTED"
            affected += 1
    return affected


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--job_id", default=DEFAULT_JOB_ID)
    parser.add_argument("--set_global_style", default=None, help="è®¾ç½®æ–°çš„ global.style_prompt")
    parser.add_argument("--no_cascade", action="store_true", help="ä¸è§¦å‘çº§è”é‡è·‘ï¼ˆé»˜è®¤ä¼šè§¦å‘ï¼‰")
    parser.add_argument("--replace_entity", default=None, help="è¦æ›¿æ¢çš„ entity_idï¼Œä¾‹å¦‚ entity_1")
    parser.add_argument("--new_ref", default=None, help="æ–°çš„ reference_image è·¯å¾„ï¼Œä¾‹å¦‚ stylized_frames/shot_02.png")

    args = parser.parse_args()

    job_dir = PROJECT_DIR / "jobs" / args.job_id
    wf = load_workflow(job_dir)

    if args.set_global_style is not None:
        affected = apply_global_style(wf, args.set_global_style, cascade=(not args.no_cascade))
        save_workflow(job_dir, wf)
        print(f"âœ… å·²æ›´æ–° global.style_prompt")
        print(f"âœ… å—å½±å“ shotsï¼š{affected}ï¼ˆstylize/video_generate å·²æ ‡è®°ä¸º NOT_STARTEDï¼‰")
    else:
        print("æ²¡æœ‰æŒ‡å®šä»»ä½•ä¿®æ”¹å‚æ•°ã€‚ç¤ºä¾‹ï¼š")
        print('  python apply_changes.py --set_global_style "cinematic noir, high contrast"')
    
    if args.replace_entity and args.new_ref:
        affected = replace_entity_reference(wf, args.replace_entity, args.new_ref)
        save_workflow(job_dir, wf)
        print(f"âœ… å·²æ›¿æ¢ {args.replace_entity} çš„ reference_image -> {args.new_ref}")
        print(f"âœ… å—å½±å“ shotsï¼š{affected}ï¼ˆstylize/video_generate å·²æ ‡è®°ä¸º NOT_STARTEDï¼‰")
        return

if __name__ == "__main__":
    main()
</file>

<file path="build_workflow.py">
import json
from pathlib import Path

PROJECT_DIR = Path(__file__).parent
JOB_DIR = PROJECT_DIR / "jobs" / "demo_job_001"

STORYBOARD_PATH = JOB_DIR / "storyboard.json"
FRAMES_DIR = JOB_DIR / "frames"
STYLIZED_DIR = JOB_DIR / "stylized_frames"
WORKFLOW_PATH = JOB_DIR / "workflow.json"

def to_seconds(t):
    if t is None:
        return None
    if isinstance(t, (int, float)):
        return float(t)
    s = str(t).strip()
    if not s:
        return None
    try:
        return float(s)
    except ValueError:
        pass
    parts = s.split(":")
    try:
        parts = [float(p) for p in parts]
    except ValueError:
        return None
    if len(parts) == 3:
        hh, mm, ss = parts
        return hh * 3600 + mm * 60 + ss
    if len(parts) == 2:
        mm, ss = parts
        return mm * 60 + ss
    if len(parts) == 1:
        return parts[0]
    return None

def main():
    storyboard = json.loads(STORYBOARD_PATH.read_text(encoding="utf-8"))

    shots = []
    for s in storyboard:
        shot_number = s.get("shot_number")
        if shot_number is None:
            continue
        sid = f"shot_{int(shot_number):02d}"

        start = to_seconds(s.get("start_time"))
        end = to_seconds(s.get("end_time"))
        desc = s.get("frame_description") or s.get("content_analysis") or ""

        frame_path = f"frames/{sid}.png"
        stylized_path = f"stylized_frames/{sid}.png"

        shots.append({
            "shot_id": sid,
            "start_time": start,
            "end_time": end,
            "description": desc,
            "voiceover": s.get("voiceover"),
            "assets": {
                "first_frame": frame_path if (JOB_DIR / frame_path).exists() else None,
                "stylized_frame": stylized_path if (JOB_DIR / stylized_path).exists() else None,
                "video": None
            },
            "status": {
                "analyze": "SUCCESS",
                "extract_frames": "SUCCESS",
                "stylize": "SUCCESS",
                "video_generate": "NOT_STARTED"
            }
        })

    workflow = {
        "job_id": "demo_job_001",
        "source_video": "input.mp4",
        "global": {
            "aspect_ratio": "16:9",
            "style_prompt": "de-replication stylization"
        },
        "entities": {},  # å…ˆç•™ç©ºï¼Œåç»­æˆ‘ä»¬åŠ â€œäººç‰©/èµ„äº§å…¨å±€æ›¿æ¢â€
        "shots": shots
    }

    WORKFLOW_PATH.write_text(json.dumps(workflow, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"âœ… workflow.json å·²ç”Ÿæˆï¼š{WORKFLOW_PATH}")
    print(f"shots æ•°é‡ï¼š{len(shots)}")

if __name__ == "__main__":
    main()
</file>

<file path="extract_frames.py">
import json
import subprocess
from pathlib import Path

PROJECT_DIR = Path(__file__).parent
VIDEO_PATH = PROJECT_DIR / "downloads" / "input.mp4"
STORYBOARD_PATH = PROJECT_DIR / "outputs" / "storyboard.json"
FRAMES_DIR = PROJECT_DIR / "frames"

def to_seconds(t):
    if t is None:
        return None
    if isinstance(t, (int, float)):
        return float(t)

    s = str(t).strip()
    if not s:
        return None

    try:
        return float(s)
    except ValueError:
        pass

    parts = s.split(":")
    try:
        parts = [float(p) for p in parts]
    except ValueError:
        return None

    if len(parts) == 3:
        hh, mm, ss = parts
        return hh * 3600 + mm * 60 + ss
    if len(parts) == 2:
        mm, ss = parts
        return mm * 60 + ss
    if len(parts) == 1:
        return parts[0]
    return None

def main():
    if not VIDEO_PATH.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è§†é¢‘ï¼š{VIDEO_PATH}")
    if not STORYBOARD_PATH.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° storyboardï¼š{STORYBOARD_PATH}")

    FRAMES_DIR.mkdir(parents=True, exist_ok=True)

    storyboard = json.loads(STORYBOARD_PATH.read_text(encoding="utf-8"))
    saved = 0

    for shot in storyboard:
        shot_num = shot.get("shot_number", saved + 1)
        start_time = shot.get("start_time", None)
        ts = to_seconds(start_time)

        if ts is None:
            continue

        out_path = FRAMES_DIR / f"shot_{int(shot_num):02d}.png"

        cmd = [
            "/opt/homebrew/bin/ffmpeg",
            "-y",
            "-ss", str(ts),
            "-i", str(VIDEO_PATH),
            "-frames:v", "1",
            "-q:v", "2",
            str(out_path)
        ]

        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        if out_path.exists():
            saved += 1

    print(f"âœ… æˆªå¸§å®Œæˆï¼š{saved} å¼ ï¼Œä¿å­˜åœ¨ {FRAMES_DIR}")

if __name__ == "__main__":
    main()
</file>

<file path="run_workflow.py">
import json
import argparse
from pathlib import Path
import shutil

PROJECT_DIR = Path(__file__).parent
DEFAULT_JOB_ID = "demo_job_001"

def load_workflow(job_dir: Path) -> dict:
    wf_path = job_dir / "workflow.json"
    return json.loads(wf_path.read_text(encoding="utf-8"))

def save_workflow(job_dir: Path, wf: dict) -> None:
    wf_path = job_dir / "workflow.json"
    wf_path.write_text(json.dumps(wf, ensure_ascii=False, indent=2), encoding="utf-8")

def find_shot(wf: dict, shot_id: str) -> dict | None:
    for s in wf.get("shots", []):
        if s.get("shot_id") == shot_id:
            return s
    return None

def ensure_videos_dir(job_dir: Path) -> Path:
    videos_dir = job_dir / "videos"
    videos_dir.mkdir(parents=True, exist_ok=True)
    return videos_dir

def mock_generate_video(job_dir: Path, shot: dict) -> str:
    """
    Demo ç‰ˆæœ¬ï¼šç”Ÿæˆä¸€ä¸ªâ€œå ä½è§†é¢‘æ–‡ä»¶â€ï¼Œç”¨æ¥éªŒè¯ runner çš„å·¥ä½œæ–¹å¼ã€‚
    åç»­æ¥ Seedance/Veo æ—¶ï¼Œåªéœ€æ›¿æ¢è¿™ä¸ªå‡½æ•°ã€‚
    """
    videos_dir = ensure_videos_dir(job_dir)
    out_path = videos_dir / f"{shot['shot_id']}.mp4"

    # ç”¨ input.mp4 çš„å‰ 1 ç§’å¤åˆ¶æˆä¸€ä¸ªå°æ–‡ä»¶ï¼ˆç¡®ä¿æ˜¯å¯æ’­æ”¾ mp4ï¼‰
    src_video = job_dir / "input.mp4"
    if not src_video.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æºè§†é¢‘ï¼š{src_video}")

    # ç›´æ¥å¤åˆ¶ä¼šå¾ˆå¤§ï¼›ä¸ºäº†å¿«ï¼Œæˆ‘ä»¬å¤åˆ¶ä¸€ä¸ªå°ç‰‡æ®µï¼ˆç”¨ ffmpegï¼‰
    # ä½ å·²ç»å®‰è£…äº† ffmpegï¼Œä½† PATH å¯èƒ½ä¸ç¨³å®šï¼Œç”¨ç»å¯¹è·¯å¾„æœ€ç¨³
    ffmpeg = "/opt/homebrew/bin/ffmpeg"

    import subprocess
    cmd = [
        ffmpeg, "-y",
        "-i", str(src_video),
        "-t", "1.0",
        "-c", "copy",
        str(out_path)
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    return f"videos/{out_path.name}"

def run_video_generate(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    """
    æ‰§è¡Œ video_generate èŠ‚ç‚¹ï¼š
    - target_shot=Noneï¼šè·‘æ‰€æœ‰ NOT_STARTED æˆ– FAILED çš„ shot
    - target_shot=shot_03ï¼šåªè·‘æŒ‡å®š shotï¼ˆå•èŠ‚ç‚¹é‡è·‘ï¼‰
    """
    shots = wf.get("shots", [])
    for shot in shots:
        sid = shot.get("shot_id")

        if target_shot and sid != target_shot:
            continue

        status = shot.get("status", {}).get("video_generate", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"):
            continue

        # æ ‡è®°è¿è¡Œä¸­
        shot.setdefault("status", {})["video_generate"] = "RUNNING"
        shot.setdefault("errors", {})["video_generate"] = None
        save_workflow(job_dir, wf)

        try:
            rel_video_path = mock_generate_video(job_dir, shot)
            shot.setdefault("assets", {})["video"] = rel_video_path
            shot["status"]["video_generate"] = "SUCCESS"
            print(f"âœ… video_generate SUCCESS: {sid} -> {rel_video_path}")
        except Exception as e:
            shot["status"]["video_generate"] = "FAILED"
            shot.setdefault("errors", {})["video_generate"] = str(e)
            print(f"âŒ video_generate FAILED: {sid} -> {e}")

        save_workflow(job_dir, wf)

def mock_stylize_frame(job_dir: Path, shot: dict) -> str:
    """
    Demo ç‰ˆé£æ ¼åŒ–ï¼šæŠŠ first_frame å¤åˆ¶æˆæ–°çš„ stylized_frameï¼ˆè¦†ç›–å†™ï¼‰ã€‚
    åç»­æ¥ Nano Banana æ—¶ï¼Œåªæ›¿æ¢è¿™é‡Œã€‚
    """
    src = job_dir / shot["assets"]["first_frame"]
    if not src.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° first_frameï¼š{src}")

    dst = job_dir / "stylized_frames" / f"{shot['shot_id']}.png"
    dst.parent.mkdir(parents=True, exist_ok=True)
    shutil.copyfile(src, dst)
    return f"stylized_frames/{dst.name}"

def run_stylize(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    shots = wf.get("shots", [])
    for shot in shots:
        sid = shot.get("shot_id")

        if target_shot and sid != target_shot:
            continue

        status = shot.get("status", {}).get("stylize", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"):
            continue

        shot.setdefault("status", {})["stylize"] = "RUNNING"
        shot.setdefault("errors", {})["stylize"] = None
        save_workflow(job_dir, wf)

        try:
            rel_path = mock_stylize_frame(job_dir, shot)
            shot.setdefault("assets", {})["stylized_frame"] = rel_path
            shot["status"]["stylize"] = "SUCCESS"
            print(f"âœ… stylize SUCCESS: {sid} -> {rel_path}")
        except Exception as e:
            shot["status"]["stylize"] = "FAILED"
            shot.setdefault("errors", {})["stylize"] = str(e)
            print(f"âŒ stylize FAILED: {sid} -> {e}")

        save_workflow(job_dir, wf)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--job_id", default=DEFAULT_JOB_ID)
    parser.add_argument("--node", choices=["video_generate"], default="video_generate")
    parser.add_argument("--shot", default=None, help="åªè¿è¡ŒæŸä¸ª shotï¼Œä¾‹å¦‚ shot_03")
    args = parser.parse_args()

    job_dir = PROJECT_DIR / "jobs" / args.job_id
    if not job_dir.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° job ç›®å½•ï¼š{job_dir}")

    wf = load_workflow(job_dir)

    if args.node == "video_generate":
        # â‘  å…ˆè·‘ stylize èŠ‚ç‚¹
        run_stylize(job_dir, wf, target_shot=args.shot)

        # â‘¡ é‡æ–°åŠ è½½ workflowï¼ˆç¡®ä¿çŠ¶æ€æœ€æ–°ï¼‰
        wf = load_workflow(job_dir)

        # â‘¢ å†è·‘ video_generate èŠ‚ç‚¹
        run_video_generate(job_dir, wf, target_shot=args.shot)


    print("âœ… runner æ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    main()
</file>

<file path="smoke_test_core.py">
from pathlib import Path
from core.workflow_io import load_workflow, save_workflow
from core.changes import replace_entity_reference
from core.runner import run_pipeline

JOB_DIR = Path("jobs/demo_job_001")

def main():
    wf = load_workflow(JOB_DIR)
    print("âœ… load_workflow ok, shots =", len(wf.get("shots", [])))

    # åšä¸€æ¬¡æ— å®³çš„ entity reference æ›¿æ¢ï¼ˆæ¢æˆè‡ªå·±å·²æœ‰çš„æ–‡ä»¶ï¼‰
    if "entity_1" in wf.get("entities", {}):
        replace_entity_reference(wf, "entity_1", "stylized_frames/shot_03.png")
        save_workflow(JOB_DIR, wf)
        print("âœ… replace_entity_reference ok")

    # è·‘ pipelineï¼ˆä¼šæŒ‰ NOT_STARTED æ‰§è¡Œï¼‰
    run_pipeline(JOB_DIR)
    print("âœ… run_pipeline ok")

if __name__ == "__main__":
    main()
</file>

<file path="stylize_frames.py">
import os
from pathlib import Path

from google import genai
from google.genai import types

PROJECT_DIR = Path(__file__).parent
FRAMES_DIR = PROJECT_DIR / "frames"
OUT_DIR = PROJECT_DIR / "stylized_frames"
OUT_DIR.mkdir(exist_ok=True)

MODEL = "gemini-2.5-flash-image"  # Nano Bananaï¼ˆæ›´å¿«ã€æ›´é€‚åˆéªŒè¯æ‰¹é‡ç¨³å®šæ€§ï¼‰
# å¦‚æœä½ ä¹‹åæƒ³ç”¨ Proï¼ˆæ›´å¼ºã€æ›´è´µï¼‰ï¼šMODEL = "gemini-3-pro-image-preview"

PROMPT = """
You are given a storyboard reference frame from a viral short video.

Goal: "De-replication stylization" (same structure, new details).
- Preserve: composition, camera angle, subject placement, overall color palette, lighting mood, and emotional tone.
- Must change: all fine details must be newly designed (faces, clothing details, textures, materials, patterns, background objects, any text/logos).
- Avoid pixel-level similarity. Do NOT copy any identifiable characters, logos, or exact text.
- Keep it cinematic and coherent.

Output:
- 16:9 image
- high clarity, rich details
"""

def save_first_image_from_response(response, out_path: Path) -> bool:
    """
    Nano Banana responses can include text parts and image parts.
    We scan parts and save the first image we find.
    """
    for part in response.parts:
        if part.inline_data is not None:
            img = part.as_image()   # requires pillow
            img.save(out_path)
            return True
    return False

def main():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("æ²¡æœ‰æ£€æµ‹åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡ï¼ˆè¯·å…ˆ export GEMINI_API_KEY=ä½ çš„keyï¼‰")

    if not FRAMES_DIR.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° frames æ–‡ä»¶å¤¹ï¼š{FRAMES_DIR}")

    frame_paths = sorted(FRAMES_DIR.glob("shot_*.png"))
    if not frame_paths:
        raise FileNotFoundError(f"frames é‡Œæ²¡æœ‰ shot_*.pngï¼š{FRAMES_DIR}")

    client = genai.Client(api_key=api_key)

    print(f"å°†å¤„ç† {len(frame_paths)} å¼ åˆ†é•œå›¾ï¼Œè¾“å‡ºåˆ°ï¼š{OUT_DIR}")

    for img_path in frame_paths:
        out_path = OUT_DIR / img_path.name

        # ä¼ å…¥å›¾ç‰‡ï¼ˆå®˜æ–¹æ¨èï¼štypes.Part.from_bytesï¼‰
        image_part = types.Part.from_bytes(
            data=img_path.read_bytes(),
            mime_type="image/png",
        )

        print(f"ğŸ–¼ï¸  Stylize {img_path.name} ...")

        response = client.models.generate_content(
            model=MODEL,
            contents=[
                image_part,
                PROMPT
            ],
            # å¯é€‰ï¼šå¦‚æœä½ ç”¨ gemini-3-pro-image-preview æƒ³æŒ‡å®šè¾“å‡ºå‚æ•°ï¼Œå¯ä»¥æ‰“å¼€ä¸‹é¢ config
            # config=types.GenerateContentConfig(
            #     response_modalities=["TEXT", "IMAGE"],
            #     image_config=types.ImageConfig(aspect_ratio="16:9", image_size="2K"),
            # )
        )

        ok = save_first_image_from_response(response, out_path)
        if ok:
            print(f"âœ… saved -> {out_path}")
        else:
            # æœ‰æ—¶æ¨¡å‹åªå›æ–‡å­—ï¼ˆè¡¨ç¤ºæ²¡å‡ºå›¾æˆ–è¢«æ‹’ç»/é™çº§ï¼‰ï¼ŒæŠŠæ–‡å­—æ‰“å°å‡ºæ¥ä¾¿äºä½ åšå¯è¡Œæ€§åˆ¤æ–­
            print("âš ï¸ æ²¡æ‹¿åˆ°å›¾ç‰‡è¾“å‡ºï¼Œæ¨¡å‹è¿”å›æ–‡æœ¬å¦‚ä¸‹ï¼š")
            print(response.text)

    print("âœ… å…¨éƒ¨å®Œæˆ")

if __name__ == "__main__":
    main()
</file>

<file path="test_gemini.py">
import os
from google import genai

def main():
    # ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– API Key
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("æ²¡æœ‰æ£€æµ‹åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡")

    client = genai.Client(api_key=api_key)

    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents="ç”¨ä¸€å¥è¯å‘Šè¯‰æˆ‘ï¼Œä½ æ˜¯è°ï¼Ÿ"
    )

    print("Gemini å›å¤ï¼š")
    print(response.text)

if __name__ == "__main__":
    main()
</file>

<file path="video_generator.py">
import os
import time
from google import genai
from google.genai import types

# ç¡®ä¿ç¯å¢ƒå˜é‡å·²åŠ è½½
api_key = os.environ.get("GEMINI_API_KEY")
client = genai.Client(api_key=api_key)

def run_veo_generation(shot_id, prompt, image_path, output_dir="output_videos"):
    """
    é’ˆå¯¹ 2026 å¹´ Gemini 3 / Veo ç”Ÿæ€ä¼˜åŒ–çš„è§†é¢‘ç”Ÿæˆå‡½æ•°
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 1. ä»¥äºŒè¿›åˆ¶è¯»å–é£æ ¼åŒ–åçš„å‚è€ƒå›¾
    try:
        with open(image_path, 'rb') as f:
            image_bytes = f.read()
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶: {image_path}")
        return None

    print(f"ğŸš€ å¯åŠ¨ Veo 3.1 ä»»åŠ¡ | åˆ†é•œ: {shot_id}")
    
    try:
        # 2. è°ƒç”¨ä¸“é—¨çš„ generate_videos æ¥å£
        # ä¿®å¤ç‚¹ï¼šå¿…é¡»ä½¿ç”¨ generate_videos è€Œé generate_content
        operation = client.models.generate_videos(
            model="veo-3.1-generate-preview",
            prompt=prompt,
            config=types.GenerateVideosConfig(
                # ä¿®å¤ç‚¹ï¼šå‚è€ƒå›¾å¿…é¡»æ”¾åœ¨è¿™ä¸ª image å­—æ®µé‡Œ
                image=types.Part.from_bytes(
                    data=image_bytes,
                    mime_type="image/png"
                ),
                aspect_ratio="16:9"
            )
        )

        # 3. å¼‚æ­¥è½®è¯¢ (Veo è§†é¢‘ç”Ÿæˆä¸æ˜¯å³æ—¶çš„)
        print(f"â³ è§†é¢‘æ­£åœ¨äº‘ç«¯æ¸²æŸ“ (Operation ID: {operation.name})")
        while not operation.done:
            print(".", end="", flush=True)
            time.sleep(10)  # æ¯ 10 ç§’æŸ¥è¯¢ä¸€æ¬¡è¿›åº¦
            operation = client.operations.get(operation.name)

        # 4. æ£€æŸ¥ç»“æœå¹¶ä¿å­˜
        if operation.result and operation.result.generated_videos:
            generated_video = operation.result.generated_videos[0]
            output_path = os.path.join(output_dir, f"{shot_id}.mp4")
            
            # ä½¿ç”¨ SDK åŸç”Ÿ save æ–¹æ³•
            generated_video.video.save(output_path)
            print(f"\nâœ… è§†é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
            return output_path
        else:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥ï¼ŒåŸå› : {operation.error}")
            return None

    except Exception as e:
        print(f"\nâŒ è°ƒç”¨ Veo API å‡ºç°ä¸¥é‡å¼‚å¸¸: {str(e)}")
        return None

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç  (ä½ å¯ä»¥ç›´æ¥è¿è¡Œ python video_generator.py éªŒè¯)
    test_prompt = "A cinematic drone shot of a neon cyberpunk city in the rain."
    test_image = "stylized_frames/shot_01.png"
    run_veo_generation("shot_01", test_prompt, test_image)
</file>

<file path="workflow_cli.py">
import argparse
from pathlib import Path

from core.workflow_io import load_workflow, save_workflow
from core.changes import apply_global_style, replace_entity_reference
from core.runner import run_pipeline

DEFAULT_JOB_ID = "demo_job_001"
PROJECT_DIR = Path(__file__).parent

def job_dir_from_id(job_id: str) -> Path:
    return PROJECT_DIR / "jobs" / job_id

def cmd_list(job_dir: Path) -> None:
    wf = load_workflow(job_dir)
    print(f"job_id: {wf.get('job_id')}")
    print(f"global.style_prompt: {wf.get('global', {}).get('style_prompt')}")
    print("-" * 60)
    for s in wf.get("shots", []):
        sid = s.get("shot_id")
        st = s.get("status", {})
        print(f"{sid:7}  stylize={st.get('stylize')}  video={st.get('video_generate')}")

def cmd_set_style(job_dir: Path, style: str, cascade: bool) -> None:
    wf = load_workflow(job_dir)
    affected = apply_global_style(wf, style, cascade=cascade)
    save_workflow(job_dir, wf)
    print(f"âœ… style å·²æ›´æ–°ï¼š{style}")
    print(f"âœ… å—å½±å“ shotsï¼š{affected}ï¼ˆcascade={cascade}ï¼‰")

def cmd_replace_entity(job_dir: Path, entity_id: str, new_ref: str) -> None:
    wf = load_workflow(job_dir)
    affected = replace_entity_reference(wf, entity_id, new_ref)
    save_workflow(job_dir, wf)
    print(f"âœ… å·²æ›¿æ¢ {entity_id}.reference_image -> {new_ref}")
    print(f"âœ… å—å½±å“ shotsï¼š{affected}")

def cmd_run(job_dir: Path, shot: str | None) -> None:
    run_pipeline(job_dir, target_shot=shot)
    print("âœ… runner æ‰§è¡Œå®Œæˆ")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--job_id", default=DEFAULT_JOB_ID)

    sub = parser.add_subparsers(dest="cmd", required=True)

    p_list = sub.add_parser("list", help="åˆ—å‡º shots çŠ¶æ€")
    p_list.set_defaults(func="list")

    p_style = sub.add_parser("set-style", help="è®¾ç½®å…¨å±€ style_prompt")
    p_style.add_argument("style")
    p_style.add_argument("--no-cascade", action="store_true", help="ä¸çº§è”é‡è·‘ï¼ˆé»˜è®¤ä¼šçº§è”ï¼‰")
    p_style.set_defaults(func="set-style")

    p_ent = sub.add_parser("replace-entity", help="æ›¿æ¢ entity çš„ reference_imageï¼Œå¹¶æ ‡è®°å—å½±å“ shots")
    p_ent.add_argument("entity_id")
    p_ent.add_argument("new_ref")
    p_ent.set_defaults(func="replace-entity")

    p_run = sub.add_parser("run", help="è¿è¡Œ runnerï¼ˆæŒ‰ NOT_STARTED æ‰§è¡Œï¼‰")
    p_run.add_argument("--shot", default=None, help="åªè·‘æŸä¸ª shotï¼Œä¾‹å¦‚ shot_03")
    p_run.set_defaults(func="run")

    args = parser.parse_args()
    job_dir = job_dir_from_id(args.job_id)
    if not job_dir.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° job ç›®å½•ï¼š{job_dir}")

    if args.func == "list":
        cmd_list(job_dir)
    elif args.func == "set-style":
        cmd_set_style(job_dir, args.style, cascade=(not args.no_cascade))
    elif args.func == "replace-entity":
        cmd_replace_entity(job_dir, args.entity_id, args.new_ref)
    elif args.func == "run":
        cmd_run(job_dir, args.shot)

if __name__ == "__main__":
    main()
</file>

<file path="core/runner.py">
# core/runner.py
from pathlib import Path
import shutil
import subprocess
import time
import os
import requests 

from .workflow_io import save_workflow, load_workflow


def ensure_videos_dir(job_dir: Path) -> Path:
    videos_dir = job_dir / "videos"
    videos_dir.mkdir(parents=True, exist_ok=True)
    return videos_dir


def mock_stylize_frame(job_dir: Path, shot: dict) -> str:
    src = job_dir / shot["assets"]["first_frame"]
    if not src.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° first_frameï¼š{src}")

    dst = job_dir / "stylized_frames" / f"{shot['shot_id']}.png"
    dst.parent.mkdir(parents=True, exist_ok=True)
    shutil.copyfile(src, dst)
    return f"stylized_frames/{dst.name}"


def mock_generate_video(job_dir: Path, shot: dict) -> str:
    videos_dir = ensure_videos_dir(job_dir)
    out_path = videos_dir / f"{shot['shot_id']}.mp4"
    src_video = job_dir / "input.mp4"
    if not src_video.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æºè§†é¢‘ï¼š{src_video}")
    ffmpeg = "/opt/homebrew/bin/ffmpeg"
    cmd = [
        ffmpeg, "-y",
        "-i", str(src_video),
        "-t", "1.0",
        "-c", "copy",
        str(out_path)
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return f"videos/{out_path.name}"


def veo_generate_video(job_dir: Path, wf: dict, shot: dict) -> str:
    """
    Veo 3.1 å›¾ç”Ÿè§†é¢‘ - æœ€ç»ˆä¿®å¤ç‰ˆ
    1. ä½¿ç”¨ v1alpha ç”Ÿæˆï¼ˆå¿…é¡»ï¼‰
    2. ä½¿ç”¨ v1beta ä¸‹è½½ï¼ˆæ›´ç¨³å®šï¼‰
    3. ä½¿ç”¨ requests params å­—å…¸é¿å… URL æ‹¼æ¥é”™è¯¯
    """
    from google import genai
    from google.genai import types

    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("æ²¡æœ‰æ£€æµ‹åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡")

    videos_dir = ensure_videos_dir(job_dir)
    out_path = videos_dir / f"{shot['shot_id']}.mp4"

    img_rel = shot.get("assets", {}).get("stylized_frame")
    if not img_rel:
        raise RuntimeError("shot ç¼ºå°‘ assets.stylized_frame")
    img_path = job_dir / img_rel

    # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯ (ç”Ÿæˆé˜¶æ®µç”¨ v1alpha)
    client = genai.Client(api_key=api_key, http_options={'api_version': 'v1alpha'})

    # 2. å‘èµ· Veo è¯·æ±‚
    print(f"ğŸš€ å‘èµ· Veo è¯·æ±‚ (Shot: {shot['shot_id']})...")
    operation = client.models.generate_videos(
        model="veo-3.1-generate-preview", 
        prompt=f"Cinematic video, {shot.get('description', '')}. Style: {wf.get('global', {}).get('style_prompt', '')}.",
        image=types.Image(
            image_bytes=img_path.read_bytes(),
            mime_type="image/png"
        ),
        config=types.GenerateVideosConfig(
            number_of_videos=1,
            duration_seconds=6.0
        ),
    )

    # 3. è½®è¯¢çŠ¶æ€
    print(f"â³ ä»»åŠ¡å·²æäº¤ï¼ŒVeo æ­£åœ¨ç”Ÿæˆè§†é¢‘ (çº¦ 1-3 åˆ†é’Ÿ)...")
    while not operation.done:
        time.sleep(20)
        operation = client.operations.get(operation)
        print(f"â³ ä»åœ¨ç”Ÿæˆä¸­...")

    if operation.error:
        raise RuntimeError(f"Veo åç«¯æŠ¥é”™: {operation.error}")

    # 4. å‡†å¤‡ä¸‹è½½
    resp = operation.response
    video_obj = resp.generated_videos[0].video
    
    # file_id é€šå¸¸æ˜¯ "files/xxxx"
    file_id = getattr(video_obj, 'name', None)
    if not file_id and hasattr(video_obj, 'uri'):
        file_id = f"files/{video_obj.uri.split('/')[-1]}"

    if not file_id:
        raise RuntimeError(f"æ— æ³•å®šä½ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶: {video_obj}")

    # 5. æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨ v1beta ç«¯ç‚¹å’Œ requests è‡ªåŠ¨å‚æ•°å¤„ç†
    print(f"âœ… ç”ŸæˆæˆåŠŸï¼Œæ­£åœ¨é€šè¿‡ v1beta ç¨³å®šç«¯ç‚¹ä¸‹è½½è§†é¢‘...")
    
    # ä½¿ç”¨ v1beta å¾€å¾€èƒ½è§£å†³ alpha ç«¯ç‚¹çš„ alt=media è§£æ Bug
    download_url = f"https://generativelanguage.googleapis.com/v1beta/{file_id}"
    
    # ä½¿ç”¨ params å­—å…¸ï¼Œrequests ä¼šè‡ªåŠ¨å¤„ç†æˆ ?alt=media&key=...
    # è¿™ç§æ–¹å¼æ¯”å­—ç¬¦ä¸²æ ¼å¼åŒ–æ›´å®‰å…¨ï¼Œä¸ä¼šå‡ºç° ? å’Œ & æ··æ·†
    query_params = {
        'alt': 'media',
        'key': api_key
    }

    try:
        response = requests.get(download_url, params=query_params, stream=True)
        
        # å¦‚æœ v1beta ä¸é€šï¼Œå†æœ€åå°è¯•ä¸€æ¬¡ v1alpha
        if response.status_code != 200:
            print(f"âš ï¸ v1beta ä¸‹è½½å¤±è´¥ (Code: {response.status_code})ï¼Œå°è¯• v1alpha...")
            alpha_url = f"https://generativelanguage.googleapis.com/v1alpha/{file_id}"
            response = requests.get(alpha_url, params=query_params, stream=True)

        if response.status_code == 200:
            with open(out_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=1024*1024): # 1MB chunks
                    if chunk:
                        f.write(chunk)
            print(f"ğŸ’¾ è§†é¢‘ç”Ÿæˆå¹¶ä¸‹è½½æˆåŠŸï¼æœ¬åœ°è·¯å¾„: {out_path}")
        else:
            raise RuntimeError(f"ä¸‹è½½ä¾ç„¶å¤±è´¥ã€‚çŠ¶æ€ç : {response.status_code}, è¯¦æƒ…: {response.text}")
            
    except Exception as e:
        print(f"âŒ ä¸‹è½½è¿‡ç¨‹å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
        raise e

    return f"videos/{out_path.name}"


def run_stylize(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    for shot in wf.get("shots", []):
        sid = shot.get("shot_id")
        if target_shot and sid != target_shot: continue
        status = shot.get("status", {}).get("stylize", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"): continue
        shot.setdefault("status", {})["stylize"] = "RUNNING"
        save_workflow(job_dir, wf)
        try:
            rel_path = mock_stylize_frame(job_dir, shot)
            shot.setdefault("assets", {})["stylized_frame"] = rel_path
            shot["status"]["stylize"] = "SUCCESS"
            print(f"âœ… stylize SUCCESS: {sid} -> {rel_path}")
        except Exception as e:
            shot["status"]["stylize"] = "FAILED"
            shot.setdefault("errors", {})["stylize"] = str(e)
            print(f"âŒ stylize FAILED: {sid} -> {e}")
        save_workflow(job_dir, wf)


def run_video_generate(job_dir: Path, wf: dict, target_shot: str | None = None) -> None:
    for shot in wf.get("shots", []):
        sid = shot.get("shot_id")
        if target_shot and sid != target_shot: continue
        status = shot.get("status", {}).get("video_generate", "NOT_STARTED")
        if not target_shot and status not in ("NOT_STARTED", "FAILED"): continue
        shot.setdefault("status", {})["video_generate"] = "RUNNING"
        save_workflow(job_dir, wf)
        try:
            video_model = wf.get("global", {}).get("video_model", "mock")
            if video_model == "veo":
                print("ğŸ”¥ USING VEO PATH")
                rel_video_path = veo_generate_video(job_dir, wf, shot)
            else:
                rel_video_path = mock_generate_video(job_dir, shot)
            shot.setdefault("assets", {})["video"] = rel_video_path
            shot["status"]["video_generate"] = "SUCCESS"
            print(f"âœ… video_generate SUCCESS: {sid} -> {rel_video_path}")
        except Exception as e:
            import traceback
            shot["status"]["video_generate"] = "FAILED"
            shot.setdefault("errors", {})["video_generate"] = repr(e)
            print("âŒ video_generate FAILED:")
            traceback.print_exc()
        save_workflow(job_dir, wf)


def run_pipeline(job_dir: Path, target_shot: str | None = None) -> None:
    wf = load_workflow(job_dir)
    run_stylize(job_dir, wf, target_shot=target_shot)
    wf = load_workflow(job_dir)
    run_video_generate(job_dir, wf, target_shot=target_shot)
</file>

<file path="jobs/demo_job_001/workflow.json">
{
  "job_id": "demo_job_001",
  "source_video": "input.mp4",
  "global": {
    "aspect_ratio": "16:9",
    "style_prompt": "cinematic noir, high contrast, moody lighting",
    "video_model": "veo"
  },
  "entities": {
    "entity_1": {
      "type": "main_subject",
      "name": "dog",
      "reference_image": "stylized_frames/shot_02.png",
      "notes": "demo entity for global replace"
    }
  },
  "shots": [
    {
      "shot_id": "shot_01",
      "start_time": 0.0,
      "end_time": 1.67,
      "description": "A medium shot of a small, fluffy, light-brown dog lying on a wooden floor. Part of a blue water bottle is visible on the left edge of the frame.",
      "entities": [
        "entity_1"
      ],
      "voiceover": null,
      "assets": {
        "first_frame": "frames/shot_01.png",
        "stylized_frame": "stylized_frames/shot_01.png",
        "video": "videos/shot_01.mp4"
      },
      "status": {
        "analyze": "SUCCESS",
        "extract_frames": "SUCCESS",
        "stylize": "SUCCESS",
        "video_generate": "SUCCESS"
      },
      "errors": {
        "video_generate": "RuntimeError('ä¸‹è½½å¤±è´¥ã€‚çŠ¶æ€ç : 400, è¯¦æƒ…: {\\n  \"error\": {\\n    \"code\": 400,\\n    \"message\": \"Invalid value \\\\\"media?key=AIzaSyBgDuk23TmU4r1Btgs8sjhhEiywhOn9NKg\\\\\" for query parameter \\'alt\\'\",\\n    \"status\": \"INVALID_ARGUMENT\"\\n  }\\n}\\n')",
        "stylize": null
      }
    },
    {
      "shot_id": "shot_02",
      "start_time": 1.67,
      "end_time": 4.34,
      "description": "A wide shot of a park with children playing near a small body of water. There are buildings and trees in the background, shot during the daytime.",
      "voiceover": null,
      "assets": {
        "first_frame": "frames/shot_02.png",
        "stylized_frame": "stylized_frames/shot_02.png",
        "video": "videos/shot_02.mp4"
      },
      "status": {
        "analyze": "SUCCESS",
        "extract_frames": "SUCCESS",
        "stylize": "SUCCESS",
        "video_generate": "SUCCESS"
      },
      "errors": {
        "video_generate": null,
        "stylize": null
      }
    },
    {
      "shot_id": "shot_03",
      "start_time": 4.34,
      "end_time": 5.45,
      "description": "A wide shot of a dark hill at night with some lights visible in the distance.",
      "voiceover": null,
      "assets": {
        "first_frame": "frames/shot_03.png",
        "stylized_frame": "stylized_frames/shot_03.png",
        "video": "videos/shot_03.mp4"
      },
      "status": {
        "analyze": "SUCCESS",
        "extract_frames": "SUCCESS",
        "stylize": "SUCCESS",
        "video_generate": "SUCCESS"
      },
      "errors": {
        "video_generate": null,
        "stylize": null
      }
    },
    {
      "shot_id": "shot_04",
      "start_time": 5.45,
      "end_time": 6.97,
      "description": "A wide shot of a beach at sunset with silhouettes of people walking along the shore. Tall buildings are visible in the background.",
      "voiceover": null,
      "assets": {
        "first_frame": "frames/shot_04.png",
        "stylized_frame": "stylized_frames/shot_04.png",
        "video": "videos/shot_04.mp4"
      },
      "status": {
        "analyze": "SUCCESS",
        "extract_frames": "SUCCESS",
        "stylize": "SUCCESS",
        "video_generate": "SUCCESS"
      },
      "errors": {
        "video_generate": null,
        "stylize": null
      }
    },
    {
      "shot_id": "shot_05",
      "start_time": 6.97,
      "end_time": 8.18,
      "description": "A low angle shot of a grassy hill under a clear blue sky.",
      "voiceover": null,
      "assets": {
        "first_frame": "frames/shot_05.png",
        "stylized_frame": "stylized_frames/shot_05.png",
        "video": "videos/shot_05.mp4"
      },
      "status": {
        "analyze": "SUCCESS",
        "extract_frames": "SUCCESS",
        "stylize": "SUCCESS",
        "video_generate": "SUCCESS"
      },
      "errors": {
        "video_generate": null,
        "stylize": null
      }
    },
    {
      "shot_id": "shot_06",
      "start_time": 8.18,
      "end_time": 9.41,
      "description": "A view looking upwards towards buildings, a street lamp and sky. Cloudy sky partially obscuring buildings.",
      "voiceover": null,
      "assets": {
        "first_frame": "frames/shot_06.png",
        "stylized_frame": "stylized_frames/shot_06.png",
        "video": "videos/shot_06.mp4"
      },
      "status": {
        "analyze": "SUCCESS",
        "extract_frames": "SUCCESS",
        "stylize": "SUCCESS",
        "video_generate": "SUCCESS"
      },
      "errors": {
        "video_generate": null,
        "stylize": null
      }
    },
    {
      "shot_id": "shot_07",
      "start_time": 9.41,
      "end_time": 10.67,
      "description": "A top-down shot looking down a staircase with a shadow of a person going down.",
      "voiceover": null,
      "assets": {
        "first_frame": "frames/shot_07.png",
        "stylized_frame": "stylized_frames/shot_07.png",
        "video": "videos/shot_07.mp4"
      },
      "status": {
        "analyze": "SUCCESS",
        "extract_frames": "SUCCESS",
        "stylize": "SUCCESS",
        "video_generate": "SUCCESS"
      },
      "errors": {
        "video_generate": null,
        "stylize": null
      }
    },
    {
      "shot_id": "shot_08",
      "start_time": 10.67,
      "end_time": 12.05,
      "description": "Close up shot of a lizard on a white wall.",
      "voiceover": null,
      "assets": {
        "first_frame": "frames/shot_08.png",
        "stylized_frame": "stylized_frames/shot_08.png",
        "video": "videos/shot_08.mp4"
      },
      "status": {
        "analyze": "SUCCESS",
        "extract_frames": "SUCCESS",
        "stylize": "SUCCESS",
        "video_generate": "SUCCESS"
      },
      "errors": {
        "video_generate": null,
        "stylize": null
      }
    },
    {
      "shot_id": "shot_09",
      "start_time": 12.05,
      "end_time": 13.43,
      "description": "A medium shot of a small, fluffy, light-brown dog lying on a wooden floor. Part of a blue water bottle is visible on the left edge of the frame.",
      "voiceover": null,
      "assets": {
        "first_frame": "frames/shot_09.png",
        "stylized_frame": "stylized_frames/shot_09.png",
        "video": "videos/shot_09.mp4"
      },
      "status": {
        "analyze": "SUCCESS",
        "extract_frames": "SUCCESS",
        "stylize": "SUCCESS",
        "video_generate": "SUCCESS"
      },
      "errors": {
        "video_generate": null,
        "stylize": null
      }
    }
  ]
}
</file>

</files>
